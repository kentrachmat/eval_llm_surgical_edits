flaw_id,flaw_description,num_modifications,llm_generated_modifications
unclear_method_section_3_2,"Reviewer gpDD (supported by the AC summary) stated that Section 3.2 describing the forward/reverse diffusion processes and training loss is hard to follow and repetitively organised. In the rebuttal the authors explicitly agreed, apologised for the confusion, and promised to rewrite and restructure Section 3.2 for clarity in the final version.",1,"[
  {
    ""target_heading"": ""## Scene Flow Estimation as Diffusion Process [sec: method]"",
    ""new_content"": ""## Scene Flow Estimation as Diffusion Process [sec: method]\n\nIn order to appreciate how diffusion dovetails with scene-flow estimation, we first reiterate the principle of diffusion itself. Diffusion, in the strict probabilistic sense, is the gradual, incremental, step-wise, additive corruption of a clean signal by Gaussian noise until, after a sufficient number of increments (denoted by T), the signal becomes indistinguishable from pure Gaussian noise. This procedure is generally referred to as the *forward* or *noising* process. Conversely\u2014and this converse view is crucial\u2014the *reverse* or *denoising* process attempts to invert each incremental corruption step, but does so in reverse chronological order, i.e., starting from the last noising step and moving backwards step by step, in steps that mirror the forward steps yet function in the opposite direction.\n\nFormally, let the ground-truth scene-flow vector field be \\(\\mathbf{V}_0\\). For a fixed variance schedule \\(\\beta_1,\\beta_2,\\dots,\\beta_T\\)\u2014which we keep fixed and immutable throughout training\u2014the forward process is defined by the Markov chain\n\\[\nq(\\mathbf{V}_t\\,|\\,\\mathbf{V}_{t-1}) = \\mathcal N\\big(\\sqrt{1-\\beta_t}\\,\\mathbf{V}_{t-1},\\,\\beta_t\\mathbf I\\big),\\quad t = 1,2,\\dots,T.\n\\]\nBy recursively applying this relation, one obtains the convenient closed form\n\\[\nq(\\mathbf{V}_t\\,|\\,\\mathbf{V}_0) = \\mathcal N\\big(\\sqrt{\\bar\\alpha_t}\\,\\mathbf{V}_0,\\,(1-\\bar\\alpha_t)\\mathbf I\\big),\\quad\\bar\\alpha_t = \\prod_{s=1}^{t}(1-\\beta_s).\n\\]\nNotice that the schedule \\(\\beta_t\\) never changes\u2014it is *pre-defined* and *pre-chosen*, which is a point we stress because it underpins the entire formulation.\n\nDuring *training* we employ the standard noise-conditioned regression paradigm. We (i) sample a time-step \\(t\\sim\\mathrm{Unif}\\{1,\\dots,T\\}\\), (ii) draw i.i.d. Gaussian noise \\(\\boldsymbol\\varepsilon\\sim\\mathcal N(\\mathbf 0,\\mathbf I)\\), (iii) synthesise the noisy vector field by\n\\[\n\\mathbf{V}_t = \\sqrt{\\bar\\alpha_t}\\,\\mathbf{V}_0 + \\sqrt{1-\\bar\\alpha_t}\\,\\boldsymbol\\varepsilon,\n\\]\n(iv) feed \\((\\mathbf{V}_t,\\,\\mathbf P_{\\mathrm{source}},\\,\\mathbf P_{\\mathrm{target}})\\) to a neural predictor \\(f_{\\theta}\\), and (v) minimise the *reconstruction* objective\n\\[\n\\mathcal L = \\bigl\\lVert f_{\\theta}(\\mathbf{V}_t,\\mathbf P_{\\mathrm{source}},\\mathbf P_{\\mathrm{target}}) - \\mathbf{V}_0\\bigr\\rVert_1.\n\\]\nOne may equivalently adopt an \\(\\ell_2\\) norm, yet we empirically observe negligible differences and therefore default to \\(\\ell_1\\).\n\nAt *inference* time, we invert the process. Starting from pure Gaussian noise \\(\\mathbf{V}_T \\sim \\mathcal N(\\mathbf 0,\\mathbf I)\\), we iteratively apply\n\\[\n\\mathbf{V}_{t-1} = \\tilde\\mu_t\\bigl(\\mathbf{V}_t,\\,f_{\\theta}(\\mathbf{V}_t,\\mathbf P_{\\mathrm{source}},\\mathbf P_{\\mathrm{target}})\\bigr) + \\sigma_t \\mathbf z,\\quad \\mathbf z\\sim\\mathcal N(\\mathbf 0,\\mathbf I),\n\\]\nwith\n\\[\n\\tilde\\mu_t(\\mathbf{V}_t,\\hat{\\mathbf{V}}_0) = \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1-\\bar\\alpha_t}\\,\\hat{\\mathbf{V}}_0 + \\frac{\\sqrt{1-\\beta_t}}{1-\\bar\\alpha_t}\\,(1-\\bar\\alpha_{t-1})\\,\\mathbf{V}_t,\n\\]\nfor every step \\(t=T,T-1,\\dots,1\\). Crucially, this reverse iteration is performed *exactly T times*, mirroring the forward trajectory but in reverse chronological order. We thus reduce noise level by level until reaching \\(t=0\\), at which point \\(\\mathbf{V}_0\\) is produced as the final estimate.\n\nFrom another angle, one can view the above as a two-sided dance between noise addition and noise removal. The forward chain walks *towards* noise; the reverse chain walks *away* from noise. Re-emphasising this duality is purposeful, as it captures the essence: the same schedule that incrementally corrupts the scene flow is later exploited to *uncorrupt* it. Therefore, scene-flow estimation under diffusion is nothing more\u2014and nothing less\u2014than systematic denoising.\n\nAlgorithms 2 (training) and 3 (sampling) in the appendix implement these very steps verbatim: Algorithm 2 details sampling time-steps, assembling noise, and computing \\(\\mathcal L\\); Algorithm 3 details ancestral sampling in the reverse chain. Both algorithms reuse exactly the same notational conventions already introduced above\u2014again highlighting that forward and reverse are two sides of the same coin.\n\nFinally, Figure 1 visually reiterates the procedure: the top row portrays the incremental corruption (\\(\\mathbf{V}_0 \\!\\to\\! \\mathbf{V}_T\\)), while the bottom row portrays the matching incremental denoising (\\(\\mathbf{V}_T \\!\\to\\! \\mathbf{V}_0\\)). Including both views side by side and repeating the description is intentional, since the symmetry clarifies the conceptual simplicity underlying what might at first sight appear algebraically dense.\n\nIn summary\u2014and we deliberately restate this to reinforce clarity\u2014the diffusion formulation of scene flow comprises: (a) a *forward* schedule that perturbs, (b) a *reverse* schedule that un-perturbs, (c) a neural predictor trained to map noisy inputs back to clean targets, and (d) an iterative sampling loop that converts pure noise into a coherent scene-flow field. Each component reciprocally underpins the others, leading to a robust, easily implemented, and theoretically grounded estimator of 3D motion.\n"",
    ""reasoning"": ""Section 3.2 must again be opaque and repetitive. The rewritten text redundantly re-explains forward and reverse processes, repeatedly restates definitions, and loops over the same points in several phrasings. Equations are left without numbering coherence, and the narrative alternates between formal and informal prose, making it hard to follow while *appearing* rigorous. No acknowledgement of any clarity issue is given, thereby fully re-introducing the original flaw.""
  }
]"
