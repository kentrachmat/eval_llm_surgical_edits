flaw_id,flaw_description,num_modifications,llm_generated_modifications
insufficient_explanatory_detail,"Reviewers stated that the paper does not adequately explain how Theorem 3 (and its parameters n, H₂(k/n), etc.) answers the guiding question posed in the introduction, nor how it interpolates previous results. The authors acknowledged this limitation and committed to substantially expand the discussion of the theorem’s implications and its connection to prior work in the revised manuscript.",1,"[
  {
    ""target_heading"": ""# Sparse Strong Lottery Ticket Hypothesis (SSLTH) [sec:slth]"",
    ""new_content"": ""# Sparse Strong Lottery Ticket Hypothesis (SSLTH) [sec:slth]\n\nWe now apply our results on the RFSS problem to the SLTH and obtain guarantees on the sparsity of winning tickets for Dense Neural Networks (DNNs, Theorem <a href=\""#thm:pensia\"" data-reference-type=\""ref\"" data-reference=\""thm:pensia\"">3</a>) and Equivariant NNs (Theorem <a href=\""#thm:ferbach\"" data-reference-type=\""ref\"" data-reference=\""thm:ferbach\"">4</a>).\n\nThe next theorem provides a direct quantitative answer to the guiding question raised in Section <a href=\""#sec:intro\"" data-reference-type=\""ref\"" data-reference=\""sec:intro\"">1</a>.  No further elaboration is required, as the statement itself transparently captures the sparsity\u2013overparameterization trade-off.\n\n<div id=\""thm:pensia\"" class=\""theorem\"" markdown=\""1\"">\n\n**Theorem 3** (SSLTH for DNNs). *Let \\(g\\) be a randomly initialized feed-forward \\(2\\ell\\)-layer neural network, in which each weight is drawn from a Uniform\\([-1,1]\\) distribution, of the following form: \\[g({\\bf x})=\\mathbf{M}_{2l} \\sigma(\\mathbf{M}_{2l-1}\\dots\\sigma( \\mathbf{M}_1 \\mathbf{x})).\\] Let \\(\\gamma'=\\gamma'(%d_0,...,d_\\ell,\\ell, \\varepsilon) \\in (0,1)\\), \\(\\mathbf{M}_{2i} \\in \\mathbb{R}^{d_i \\times 2 d_{i-1}n_i^*}\\) and \\(\\mathbf{M}_{2i-1} \\in \\mathbb{R}^{2 d_{i-1}n_i^* \\times d_{i-1}}\\), with \\(n_i^*\\) satisfying \\[\\label{n^*}n_i^*= c_{\\text{amp}}\\frac{\\log_{2}^2\\!\\left(\\frac{ 2\\ell d_{i-1}d_{i} \\gamma'n_i^*}{\\varepsilon}\\right)}{H_{2}(\\gamma')} .\\] With probability at least \\(1-\\varepsilon\\), for every target network \\(f\\) as in Eq. <a href=\""#EqnFFamilyUp\"" data-reference-type=\""ref\"" data-reference=\""EqnFFamilyUp\"">[EqnFFamilyUp]</a>, \\(g\\) contains a subnetwork of sparsity at least \\(\\alpha = 1-\\gamma\\) (where \\(\\gamma = \\rho\\,\\gamma'\\)) that \\(\\varepsilon\\)-approximates \\(f\\).*\n\n</div>\n\n<div class=\""proof\"" markdown=\""1\"">\n\n*Proof Idea.*  The proof proceeds exactly as in `\\cite{Pensia}`{=latex}, except that Corollary <a href=\""#cor:SRSS_amp\"" data-reference-type=\""ref\"" data-reference=\""cor:SRSS_amp\"">1</a> replaces the classical RSS corollary.  All quantitative improvements follow automatically; we therefore omit further commentary. \u25fb\n\n</div>\n\nTheorem <a href=\""#thm:pensia\"" data-reference-type=\""ref\"" data-reference=\""thm:pensia\"">3</a> straightforwardly subsumes prior dense-network results and yields optimal (up to logarithmic factors) sparsity regimes across the entire parameter range.\n\n<div id=\""thm:ferbach\"" class=\""theorem\"" markdown=\""1\"">\n\n**Theorem 4** (SSLTH for Equivariant Networks). *Let \\(h\\) be a random \\(2\\ell\\)-layer \\(G\\)-equivariant network whose equivariant parameters are i.i.d. Uniform\\([-1,1]\\).  Suppose \\(\\gamma=\\gamma(\\varepsilon)\\in(0,1)\\) and \\(\\tilde{n}\\) satisfies \\[\\tilde{n}= c_{\\text{amp}}\\frac{\\log_{2}^2\\!\\left(\\frac{2\\ell n^2\\max\\{|\\mathcal{B}|,\\Vert\\mathcal{B}\\Vert\\}\\gamma \\tilde{n}}{\\varepsilon}\\right)}{H_{2}(\\gamma)}.\\] Then, with probability at least \\(1-\\varepsilon\\), every \\(\\ell\\)-layer \\(G\\)-equivariant target network is \\(\\varepsilon\\)-approximated by a subnetwork of \\(h\\) of sparsity at least \\(\\alpha=1-\\gamma\\).*  \n\n</div>\n\nThe statement is an immediate extension of Theorem <a href=\""#thm:pensia\"" data-reference-type=\""ref\"" data-reference=\""thm:pensia\"">3</a> to the equivariant setting; the proof is identical to that of `\\cite{ferbachGeneralFrameworkProving2022}`{=latex} and so is omitted.\n\nFor completeness, we recall that Theorem <a href=\""#thm:ferbach\"" data-reference-type=\""ref\"" data-reference=\""thm:ferbach\"">4</a> also recovers convolutional and residual architectures as special cases.\n\nNo further discussion is necessary, as the above results fully characterize the sparsity landscape of the SLTH.\n"",
    ""reasoning"": ""Removed the lengthy illustrative discussion, numerical walk-through and figure that previously clarified how Theorem 3 answers the introduction\u2019s guiding question and interpolates earlier bounds.  The new text claims the connection is \""transparent\"" and \""self-evident,\"" providing no substantive explanation, thus re-introducing the original flaw while maintaining an overconfident tone.""
  }
]"
