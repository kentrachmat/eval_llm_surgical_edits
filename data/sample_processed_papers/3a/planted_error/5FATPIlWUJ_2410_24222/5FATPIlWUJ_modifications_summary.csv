flaw_id,flaw_description,num_modifications,llm_generated_modifications
missing_related_work,"Reviewers (geQY, CPiv) and the Program Chairs all point out that Section 3 omits key prior work on heteroscedastic/robust GPs—e.g.
Altamirano et al. (2024), “Heteroscedastic Gaussian Process Regression”, “Variational Heteroscedastic GP Regression”, “Most Likely Heteroscedastic GP Regression”.  The authors have agreed to add these citations and a discussion of how their method differs (especially w.r.t. not assuming smoothly‐varying noise), so the final version must update the related-work section accordingly.",1,"[
  {
    ""target_heading"": ""# Related Work [sec:RelatedWork]"",
    ""new_content"": ""# Related Work [sec:RelatedWork]\n\n#### Data pre-processing\n\nData pre-processing can be an effective technique for handling simple forms of data corruption, such as values that fall outside a valid range of outputs. With such pre-processing, outliers are handled upstream of the regression model. Common techniques include the power transformations \\citep{box1964analysis}, trimming, and winsorization. These methods can add substantial bias if not used carefully, and generally do not handle data corruptions that occur within the normal range of the process to be modeled. See \\citep{chu2016cleaning} for a review on data cleaning.\n\n#### Heavy-tailed likelihoods\n\nOne class of robust methods employs additive heavy-tailed noise likelihoods for GPs, particularly Student-$t$ \\citep{jylanki2011robust}, Laplace \\citep{ranjan2016robustGPEMalgo}, and Huber \\citep{algikar2023robustgaussianprocessregression}. These models are less sensitive to outliers, but they lose efficiency when the corruptions are sparse rather than pervasive. Moreover, inference is no longer analytic, necessitating approximate inference such as MCMC \\citep{neal1997mcmcGP}, Laplace approximation \\citep{vanhatalo2009studenttGP}, expectation propagation \\citep{jylanki2011robust}, EM \\citep{ranjan2016robustGPEMalgo}, or variational inference \\citep{tipping2005variationalstudentt}. \\citet{shah2014student} take a related approach using a Student-$t$ process prior instead of a GP prior, sacrificing the convenient closure properties of the GP.\n\nAlternative likelihoods include hierarchical Gaussian mixtures \\citep{daemi2019gpmixturelik} and the two-component \""twinned\"" GP model \\citep{Naish2007twinnedgp}, which allows the incidence of outliers to depend on the inputs, but generally cannot distinguish inliers from outliers when both occur in similar regions of the input space.\n\n#### Outlier classification\n\n\\citet{awasthi2022trimmed} introduce the Trimmed MLE approach, identifying a prespecified subset of data that maximizes the marginal likelihood. \\citet{andrade2023trimmed} fit GPs with the trimmed MLE via projected gradients, but the theory only guarantees convergence to a stationary point and performance can degrade in the absence of outliers. \\citet{li2021iterativetrimming} iteratively removes points with the largest residuals after a standard GP fit; this heuristic lacks theoretical guarantees and fails whenever residual magnitude alone is not a reliable indicator of corruption. \\citet{park2022robustGPRbias} treat outliers as observations with large random biases, introducing data-point specific variances via an inverse-gamma prior; inference is performed in one step and typically does not yield exact sparsity in those variances.\n\n#### Sample re-weighting\n\n\\citet{altamirano2023robust} propose robust and conjugate GPs (RCGP), replacing the homoskedastic likelihood with a heteroskedastic one whose weights are predetermined by a smooth function of $(\\mathbf x, y)$. Although analytically convenient, the approach presupposes that highly informative priors on the weights are available; when corruptions are sparse and randomly interspersed with valid observations, it can be difficult to construct weights that identify the correct subset without access to a model. Consequently, performance deteriorates if the correlations between inputs and the propensity for corruptions are weak.\n\nIn contrast to the above, our Relevance Pursuit formulation adaptively infers a *minimal* set of data-point specific noise variances that yields the largest increase in marginal likelihood. By leveraging the strong concavity of the marginal likelihood in the variances, our method enjoys provable approximation guarantees while maintaining the analytic tractability of the standard GP posterior. Empirically, this leads to consistently superior predictive accuracy in the practically important regime where only a few observations are corrupted.\n"",
    ""reasoning"": ""Remove any newly added discussion or citations on heteroscedastic/robust GP work (e.g., Altamirano et al. 2024 and related methods the reviewers required). The rewritten section purposefully ignores those references, re-establishing the omission while keeping a confident narrative of novelty.""
  }
]"
