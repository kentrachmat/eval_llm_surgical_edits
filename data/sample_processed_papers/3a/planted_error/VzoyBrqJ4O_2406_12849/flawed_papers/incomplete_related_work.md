# Depth Anywhere: Enhancing 360 Monocular Depth Estimation via Perspective Distillation and Unlabeled Data Augmentation

## Abstract

Accurately estimating depth in 360-degree imagery is crucial for virtual reality, autonomous navigation, and immersive media applications. Existing depth estimation methods designed for perspective-view imagery fail when applied to 360-degree images due to different camera projections and distortions, whereas 360-degree methods perform inferior due to the lack of labeled data pairs. We propose a new depth estimation framework that utilizes unlabeled 360-degree data effectively. Our approach uses state-of-the-art perspective depth estimation models as teacher models to generate pseudo labels through a six-face cube projection technique, enabling efficient labeling of depth in 360-degree images. This method leverages the increasing availability of large datasets. Our approach includes two main stages: offline mask generation for invalid regions and an online semi-supervised joint training regime. We tested our approach on benchmark datasets such as Matterport3D and Stanford2D3D, showing significant improvements in depth estimation accuracy, particularly in zero-shot scenarios. Our proposed training pipeline can enhance any 360 monocular depth estimator and demonstrates effective knowledge transfer across different camera projections and data types. See our project page for results: [albert100121.github.io/Depth-Anywhere](https://albert100121.github.io/Depth-Anywhere/).

# Introduction

In recent years, the field of computer vision has seen a surge in research focused on addressing the challenges associated with processing 360-degree images. The widespread use of panoramic imagery across various domains, such as virtual reality, autonomous navigation, and immersive media, has underscored the need for accurate depth estimation techniques tailored specifically for 360-degree images. However, existing depth estimation methods developed for perspective-view images encounter significant difficulties when applied directly to 360-degree data due to differences in camera projection and distortion. While many methods aim to address depth estimation for this camera projection, they often struggle due to the limited availability of labeled datasets.

To overcome these challenges, this paper presents a novel approach for training state-of-the-art (SOTA) depth estimation models on 360-degree imagery. With the recent significant increase in the amount of available data, the importance of both data quantity and quality has become evident. Research efforts on perspective perceptual models have increasingly focused on augmenting the volume of data and developing foundation models that generalize across various types of data. Our method leverages SOTA perspective depth estimation foundation models as teacher models and generates pseudo labels for unlabeled 360-degree images using a six-face cube projection approach. By doing so, we efficiently address the challenge of labeling depth in 360-degree imagery by leveraging perspective models and large amounts of unlabeled data.

Our approach consists of two key stages: offline mask generation and online joint training. During the offline stage, we employ a combination of detection and segmentation models to generate masks for invalid regions, such as sky and watermarks in unlabeled data. Subsequently, in the online stage, we adopt a semi-supervised learning strategy, loading half of the batch with labeled data and the other half with pseudo-labeled data. Through joint training with both labeled and pseudo-labeled data, our method achieves robust depth estimation performance on 360-degree imagery.

To validate the effectiveness of our approach, we conduct extensive experiments on benchmark datasets such as Matterport3D and Stanford2D3D. Our method demonstrates significant improvements in depth estimation accuracy, particularly in zero-shot scenarios where models are trained on one dataset and evaluated on another. Furthermore, we demonstrate the efficacy of our training techniques with different SOTA 360-degree depth models and various unlabeled datasets, showcasing the versatility and effectiveness of our approach in addressing the unique challenges posed by 360-degree imagery.

Our contributions can be summarized as follows:

- We propose a novel training technique for 360-degree imagery that harnesses the power of unlabeled data through the distillation of perspective foundation models.

- We introduce an online data augmentation method that effectively bridges knowledge distillation across different camera projections.

- Our proposed training techniques significantly benefit and inspire future research on 360-degree imagery by showcasing the interchangeability of state-of-the-art (SOTA) 360 models, perspective teacher models, and unlabeled datasets. This enables better results even as new SOTA techniques emerge in the future.

<figure id="fig:teaser">
<img src="./figures/teaser_v7.png"" style="width:100.0%" />
<figcaption><strong>Our proposed training pipeline improves existing 360 monocular depth estimators.</strong> This figure demonstrated the improvement of our proposed training pipeline tested on the Stanford2D3D <span class="citation" data-cites="Stanford2D3D"></span> dataset in a zero-shot setting.</figcaption>
</figure>

# Related Work [sec: related work]

Depth estimation in panoramic imagery has only recently begun to attract wider attention. We briefly review the most closely related lines of research and position our contribution accordingly.

#### 360 Monocular Depth. [sec:360_Monocular_Depth]

Early attempts focused on adapting perspective CNNs to the equirectangular domain. OmniDepth `\cite{OmniDepth}` introduced spherical convolutions to alleviate projection distortion, while SphConv `\cite{SphConv}` refined this idea with customized kernels. BiFuse `\cite{BiFuse20}` pioneered a dual-branch architecture that jointly processes cube and equirectangular projections, inspiring subsequent designs such as UniFuse `\cite{jiang2021unifuse}`. SliceNet `\cite{SliceNet}` explored vertically sliced panoramas to retain fine detail. Despite these advances, effective use of large-scale unlabeled data remains largely unexplored.

#### 360 Other Works

Beyond depth, 360-degree imagery has served geometry-aware tasks such as layout recovery and SLAM (e.g., 360-SelfNet `\cite{360-SelfNet}` and 360SD-Net `\cite{wang2020360sd}`). These studies underscore the importance of spherical geometry but do not address monocular depth estimation with limited supervision.

#### Unlabeled / Pseudo-Labeled Data

Semi-supervised strategies such as pseudo-labeling `\cite{Lee2013PseudoLabelT}` and FixMatch `\cite{sohn2020fixmatch}` were originally developed for natural images. More recently, Depth Anything `\cite{depthanything}` demonstrated that massive unlabeled perspective data can fuel robust depth models. However, equivalent efforts for 360° images are rare; most existing 360 approaches still rely on scarce ground truth.

#### Zero-Shot Methods

MiDaS `\cite{MiDaS}` and ZoeDepth `\cite{ZoeDepth}` highlight the value of affine-invariant training for perspective views. Yet, direct transfer of these methods to panoramic geometry degrades performance because they omit projection-specific cues.

#### Foundation Models

Vision foundation models such as CLIP `\cite{CLIP}` and DINOv2 `\cite{oquab2023dinov2}` have proven transferable across tasks. Segment Anything `\cite{SAM}` shows that large-scale pre-training can generalize to unforeseen domains. Motivated by these findings, we tap into the strong prior of the perspective-trained Depth Anything `\cite{depthanything}` and adapt it to 360° imagery through the distillation pipeline described in Section 3.

In summary, prior work either (i) tailors architectures to panoramic distortion with limited data or (ii) leverages unlabeled data exclusively for perspective cameras. Our method is the first to close this gap by distilling a perspective foundation model into a 360-degree student using abundant unlabeled panoramas, yielding state-of-the-art performance without additional manual annotation.
#### 360 monocular depth. [sec:360_Monocular_Depth]

Depth estimation for 360-degree images presents unique challenges due to the equirectangular projection and inherent distortion. Various approaches have been explored to address these issues:

- *Directly Apply*: Some methods directly apply monocular depth estimation techniques to 360-degree imagery. OmniDepth `\cite{OmniDepth}`{=latex} leverages spherical geometry and incorporates SphConv `\cite{SphConv}`{=latex} to improve depth prediction with distortion.  `\cite{Spherecoord, wang2020360sd}`{=latex} use spherical coordinates to overcome distortion with extra information. `\cite{feng2020deep, jin2020geometric}`{=latex} leverage other ground truth supervisions to assist on depth estimation. SliceNet `\cite{SliceNet}`{=latex} and ACDNet `\cite{ACDNet}`{=latex} propose advanced network architectures tailored for omnidirectional images. EGFormer `\cite{EGFormer}`{=latex} and HiMODE `\cite{junayed2022himode}`{=latex} introduce a transformer-based model that captures global context efficiently, while  `\cite{Hohonet, PanoPlane}`{=latex} focuses on integrating geometric priors into the learning process.  `\cite{feng2022360}`{=latex} proposed to generate large-scale datasets with SfM and MVS, then apply to test-time training.

- *Cube*: Other approaches use cube map projections to mitigate distortion effects. 360-SelfNet `\cite{360-SelfNet}`{=latex} is the first work to self-supervised 360 depth estimation leveraging cube-padding `\cite{Cubepad}`{=latex}. BiFuse `\cite{BiFuse20}`{=latex} and its improved version BiFuse++ `\cite{BiFuse++}`{=latex} are two-branch architectures that utilize cube maps and equirectangular projections. UniFuse `\cite{jiang2021unifuse}`{=latex} combines equirectangular and cube map projections and simplifies the architecture.  `\cite{bai2024glpanodepth}`{=latex} combines two-branch techniques with transformer network.

- *Tangent Image*: Tangent image projections are also popular. `\cite{reyarea2021360monodepth, OmniFusion, peng2022high}`{=latex} convert equirectangular images into a series of tangent images, which are then processed using conventional depth estimation networks. PanoFormer `\cite{shen2022panoformer}`{=latex} employs a transformer-based architecture to handle tangent images, while SphereNet `\cite{coors2018spherenet}`{=latex} and HRDFuse `\cite{ai2023hrdfuse}`{=latex} enhance depth prediction by collaboratively learning from multiple projections.

#### 360 other works

 Beyond depth estimation, 360-degree imagery has been applied to depth completion tasks as follows `\cite{liu2022cross, chen2024360orb, pintore2024deep, yan2022multi, yan2023distortion, huang2019indoor}`{=latex}. Other methods, such as  `\cite{10550831}`{=latex} and  `\cite{Su_2019_CVPR}`{=latex} focus on the projection between camera models, while the former projects pinhole camera model images into a large field of view, whereas the latter transforms convolution kernels.

#### Unlabeled / Pseudo labeled data.

Utilizing unlabeled or pseudo-labeled data has become a significant trend to mitigate the limitations of labeled data scarcity. Techniques like  `\cite{Lee2013PseudoLabelT, Zoph2020RethinkingPA, sohn2020fixmatch, xie2020self}`{=latex} leverage large amounts of unlabeled data to improve model performance through semi-supervised learning. In the context of 360-degree depth estimation, our approach generates pseudo labels from pre-trained perspective models, which are then used to train 360-degree depth models effectively.

#### Zero-shot methods.

Zero-shot learning methods aim to generalize to new domains without additional training data.  `\cite{chen2016single, Xian2018MonocularRD}`{=latex} target this directly with increasing training data., MiDaS `\cite{MiDaS, MiDaS_v3, DPT_MiDaS}`{=latex} and Depth Anything `\cite{depthanything}`{=latex} are notable for their robust monocular depth estimation across diverse datasets leveraging affine-invariant loss.  `\cite{yin2023metric3d}`{=latex} takes a step further to investigate zero-shot on metric depth. Marigold `\cite{Marigold}`{=latex} leverages diffusion models with image conditioning and up-to-scale relative depth denoising to generate detailed depth maps. ZoeDepth `\cite{ZoeDepth}`{=latex} further these advancements by incorporating scale awareness and domain adaptation.  `\cite{Guizilini2023, wang2021bridging}`{=latex} leverage camera model information to adapt cross-domain depth estimation.

#### Foundation models.

Foundation models have revolutionized various fields in AI, including natural language processing and image-text alignment. In computer vision, models like CLIP `\cite{CLIP}`{=latex} demonstrate exceptional generalization capabilities.  `\cite{oquab2023dinov2}`{=latex} proposed a foundation visual encoder for downstream tasks such as segmentation, detection, depth estimation, etc.  `\cite{SAM}`{=latex} proposed a model that can cut out masks for any objects. Our work leverages a pre-trained perspective depth estimation foundation model `\cite{depthanything}`{=latex} as a teacher model to generate pseudo labels for 360-degree images, enhancing depth estimation by utilizing the vast knowledge embedded in these foundation models.

# Methods

<figure id="fig:pipeline">
<img src="./figures/pipeline_v4.png"" style="width:100.0%" />
<figcaption><strong>Training Pipeline.</strong> Our proposed training pipeline involves joint training on both labeled 360 data with ground truth and unlabeled 360 data. (a) For labeled data, we train our 360 depth model with the loss between depth prediction and ground truth. (b) For unlabeled data, we propose to distill knowledge from a pre-trained perspective-view monocular depth estimator. In this paper, we use Depth Anything <span class="citation" data-cites="depthanything"></span> to generate pseudo ground truth for training. However, more advanced techniques could be applied. These perspective-view monocular depth estimators fail to produce reasonable equirectangular depth as a domain gap exists. Therefore, we distill knowledge by inferring six perspective cube faces and passing them through perspective-view monocular depth estimators. To ensure stable and effective training, we propose generating a valid pixel mask with Segment Anything <span class="citation" data-cites="SAM"></span> while calculating loss. (c) Furthermore, we augment random rotation on RGB before passing it into Depth Anything, as well as on predictions from the 360 depth model.</figcaption>
</figure>

In this work, we propose a novel training approach for 360-degree monocular depth estimation models. Our method leverages a perspective depth estimation model as a teacher and generates pseudo labels for unlabeled 360-degree images using a 6-face cube projection. Figure <a href="#fig:pipeline" data-reference-type="ref" data-reference="fig:pipeline">2</a> illustrates our training pipeline, incorporating the use of Segment Anything to mask out sky and watermark regions in unlabeled data during the offline stage. Subsequently, we conduct joint training using both labeled and unlabeled data, allocating half of the batch to each. The joint training avoids limiting performance by teacher model. The unlabeled data is supervised using pseudo labels generated by Depth Anything, a state-of-the-art perspective monocular depth foundation model. With the benefit of our teacher model, the 360-degree depth model demonstrates an observable improvement on the zero-shot dataset, as shown in Figure <a href="#fig:teaser" data-reference-type="ref" data-reference="fig:teaser">1</a>.

<span id="headings" label="headings"></span>

## Unleashing the Power of Unlabel 360 data

#### Dataset statistics. [sec:dataset statistic]

360-degree data has become increasingly available in recent years. However, compared to perspective-view depth datasets, labeling depth ground truths for 360-degree data presents greater challenges. Consequently, the availability of labeled datasets for 360-degree data is considerably smaller than that of perspective datasets.

Table <a href="#Dataset statistic" data-reference-type="ref" data-reference="Dataset statistic">1</a> presents the data quantities available in some of the most popular 360-degree datasets, including Matterport3D `\cite{Matterport3D}`{=latex}, Stanford2D3D `\cite{Stanford2D3D}`{=latex}, and Structured3D `\cite{Structured3D}`{=latex}. Additionally, we list a multi-modal dataset, SpatialAudioGen `\cite{spatialaudiogen}`{=latex}, which consists of unlabeled 360-degree data used in our experiments. Notably, the amount of labeled and unlabeled data used in the perspective foundation model, Depth Anything `\cite{depthanything}`{=latex}, is significantly larger, with 1.5 million labeled images `\cite{MegaDepth, wang2020tartanair, DIML, yao2020blendedmvs, HRWSI, wang2021irs}`{=latex} and 62 million unlabeled images `\cite{russakovsky2015imagenet, yu2020bdd100k, weyand2020google, yu15lsun, Object365, OpenImages, zhou2017places, SAM}`{=latex}, making the amount in 360-degree datasets approximately 170 times smaller.

<div id="Dataset statistic" markdown="1">

<table>
<caption><strong>360 monocular depth estimation lacks a large amount of training data.</strong> The number of images in 360-degree monocular depth estimation datasets alongside perspective depth datasets from the Depth Anything methodology.</caption>
<thead>
<tr>
<th colspan="4" style="text-align: center;">Perspective</th>
<th colspan="4" style="text-align: center;">Equirectangular</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Labeled</td>
<td style="text-align: center;">1.5M</td>
<td style="text-align: left;">Unlabeled</td>
<td style="text-align: center;">62 M</td>
<td style="text-align: left;">Labeled</td>
<td style="text-align: center;">34K</td>
<td style="text-align: left;">Unlabeled</td>
<td style="text-align: center;">344K</td>
</tr>
</tbody>
</table>

</div>

#### Data cleaning and valid pixel mask generation [sec:data_cleaning]

Unlabeled data often contains invalid pixels in regions such as the sky and watermark, leading to unstable training or undesired convergence. To address this issue, we applied the GroundingSAM `\cite{groundedSAM}`{=latex} method to mask out the invalid regions. This approach utilizes Grounded DINOv2 `\cite{groundingDINO}`{=latex} to detect problematic regions and applies the Segment Anything `\cite{SAM}`{=latex} model to mask out the invalid pixels by segmenting within the bounding box. While Depth Anything `\cite{depthanything}`{=latex} also employs a pre-trained segmentation model, DINOv2, to select sky regions. Brand logos and watermarks frequently appear after fisheye camera stitching. Therefore, additional labels are applied to enhance the robustness of our training process. We also remove all images with less than 20 percent of valid pixels to stablize our training progress.

<figure id="fig:mask">
<img src="./figures/valid_mask_v2.png"" style="width:100.0%" />
<figcaption><strong>Valid Pixel Masking.</strong> We used Grounded-Segment-Anything <span class="citation" data-cites="groundedSAM"></span> to mask out invalid pixels based on two text prompts: “sky” and “watermark.” These regions lack depth sensor ground truth labels in all previous datasets. Unlike Depth Anything <span class="citation" data-cites="depthanything"></span>, which sets sky regions as 0 disparity, we follow ground truth training to ignore these regions during training for two reasons: (1) segmentation may misclassify and set other regions as zero, leading to noisy labeling, and (2) watermarks are post-processing regions that lack geometrical meaning. </figcaption>
</figure>

#### Perspective foundation models (teacher models).

To tackle the challenges posed by limited data and labeling difficulties in 360-degree datasets, we leverage a large amount of unlabeled data alongside state-of-the-art perspective depth foundation models. Due to significant differences in camera projection and distortion, directly applying perspective models to 360-degree data often yields inferior results. Previous works have explored various methods of projection for converting equirectangular to perspective depth, as stated in Section <a href="#sec:360_Monocular_Depth" data-reference-type="ref" data-reference="sec:360_Monocular_Depth">2.0.0.1</a>. Among these, cube projection and tangent projection are the most common techniques. We selected cube projection to ensure a larger field of view for each patch, enabling better observation of relative distances between pixels or objects during the inference of the perspective foundation model and enhancing knowledge distillation. The comparison table can be find in the supplementary material.

In our approach, we apply projection to unlabeled 360-degree data and then run Depth Anything on these projected patches of perspective images to generate pseudo-labels. We explore two directions for pseudo-label supervision: projecting the patch to equirectangular and computing in the 360-degree domain or projecting the 360-degree depth output from the 360 model to patches and computing in the perspective domain. Since training is conducted in an up-to-scale relative depth manner, stitching the patch of perspective images back to equirectangular with an aligned scale will lead to failure in training Figure <a href="#fig:equi_pseudo" data-reference-type="ref" data-reference="fig:equi_pseudo">4</a>, which is an additional research topic that is worth investigation. We opt to compute the loss in the perspective domain, facilitating faster and easier training without the need for additional alignment optimization.

<figure id="fig:equi_pseudo">
<img src="./figures/equi_artifact_both_v3.png"" style="width:80.0%" />
<figcaption><strong>Qualitative visualization of a model trained directly on pseudo equirectangular data without scale alignment</strong>. We propose calculating the loss with pseudo ground truth on cube faces due to scale misalignment between the six faces during the cube-to-equirectangular projection. We showcase the results of a model trained on pseudo equirectangular data without scale alignment as a simple baseline to demonstrate the importance of calculating loss separately on each of the six faces. The images are presented from top to bottom as follows: (a) RGB images. (b) Pseudo cube ground truth projected directly to equirectangular. (c) Prediction trained with row 2. (d) Pseudo cube ground truth with rotation projected directly to equirectangular. (e) Prediction trained with row 4. (f) Our model’s predictions are trained on cube faces separately with rotation.</figcaption>
</figure>

## Random Rotation Processing

Directly applying Depth Anything on cube-projected unlabeled data does not yield improvements due to ignorance of cross-cube-face relation, leading to cube artifacts (Figure <a href="#fig:cube_patent" data-reference-type="ref" data-reference="fig:cube_patent">5</a>). This issue arises from the separate estimation of perspective cube faces, where monocular depth is estimated based on semantic information, lacking a comprehensive understanding of the entire scene. To address this, we propose a random rotation preprocessing step in front of the perspective foundation model.

As depicted in Figure <a href="#fig:pipeline" data-reference-type="ref" data-reference="fig:pipeline">2</a>, the rotation is applied to equirectangular projection RGB images using a random rotation matrix, followed by cube projection. This results in a more diverse set of cube faces, capturing relative distances between ceilings, walls, windows, and other objects more effectively. With the proposed random rotation technique, knowledge distillation becomes more comprehensive as the point of view is not static. The inference by the perspective foundation model is performed on the fly, with parameters frozen during the training of the 360 model.

In order to perform random rotation, we apply a rotation matrix on the equirectangular coordinates, noted as \\((\theta, \phi)\\), and rotation matrix as \\(\mathcal{R}\\). \\[(\hat{\theta}, \hat{\phi}) = \mathcal{R}\cdot (\theta, \phi).\\]

For equirectangular to cube projection, the field-of-view (FoV) of each cube face is equal to 90 degrees; each face can be considered as a perspective camera whose focal length is \\(w/2\\), and all faces share the same center point in the world coordinate. Since the six cube faces share the same center point, the extrinsic matrix of each camera can be defined by a rotation matrix \\(R_i\\). \\(p\\) is then the pixel on the cube face \\[p = K \cdot R^T_i \cdot q,\\] where, \\[q 
    = \begin{bmatrix}q_x\\ q_y\\ q_z\end{bmatrix} 
    = \begin{bmatrix}
    sin(\theta)\cdot\cos(\phi) \\
    \sin(\phi)\\
    \cos{\theta}\cdot\cos{\phi}\end{bmatrix}, 
    K = \begin{bmatrix}
    w/2 & 0 & w/2\\
    0 & w/2 & w/2\\
    0 & 0 & 1\\
    \end{bmatrix},\\] where \\(\theta\\) and \\(\phi\\) are longitude and latitude in equirectangular projection and q is the position in Euclidean space coordinates.

<figure id="fig:cube_patent">
<img src="./figures/cube_artifact.png"" style="width:100.0%" />
<figcaption><strong>Cube Artifact.</strong> Shown in the center row of the figure, an undesired cube artifact appears when we apply joint training with pseudo ground truth from Depth Anything <span class="citation" data-cites="depthanything"></span> directly. This issue arises from independent relative distances within each cube face caused by a static point of view. Ignoring cross-cube relationships results in poor knowledge distillation. To address this, as shown in Figure <a href="#fig:pipeline" data-reference-type="ref" data-reference="fig:pipeline">2</a>(c), we randomly rotate the RGB image before inputting it into Depth Anything. This enables better distillation of depth information from varying perspectives within the equirectangular image. </figcaption>
</figure>

## Loss Function

The training process closely resembles that of MiDaS, Depth Anything, and other cross-dataset methods. Our goal is to provide depth estimation for any 360-degree images. Following previous approaches that trained on multiple datasets, our training objective is to estimate relative depth. The depth values are first transformed into disparity space using the formula \\(1/d\\) and then normalized to the range \\([0, 1]\\) for each disparity map.

To adapt to cross-dataset training and pseudo ground truths from the foundation model, we employed the affine-invariant loss, consistent with prior cross-dataset methodologies. This loss function disregards absolute scale and shifts for each domain, allowing for effective adaptation across different datasets and models. \\[\mathcal{L}1 = \frac{1}{HW}\sum^{HW}_{i=1}{\rho(d^*_i, d_i)},\\] where \\(d^*_i\\) and \\(d_i\\) are the prediction and ground truth, respectively. \\(\rho\\) represents the affine-invariant mean absolute error loss: \\[\rho(d^*_i, d_i) = |\hat{d}^*_i - \hat{d}_i|.\\] Here, \\(\hat{d_i}\\) and \\(\hat{d}^*_i\\) are the scaled and shifted versions of the prediction \\(d^*_i\\) and ground truth \\(d_i\\): \\[\hat{d_i} = \frac{d_i - t(d)}{s(d)},\\] where \\(t(d)\\) and \\(s(d)\\) are used to align the prediction and ground truth to have zero translation and unit scale: \\[t(d) = \text{median}(d), \quad s(d) = \frac{1}{HW}\sum^{HW}_{i=1}{|d_i - t(d)|}.\\]

# Experiments [sec:exp]

These notations apply for all tables: **M**: Matterport3D `\cite{Matterport3D}`{=latex}, **SF**: Stanford2D3D `\cite{Stanford2D3D}`{=latex}, **ST**: Structured3D `\cite{Structured3D}`{=latex}, **SP**: Spatialaudiogen `\cite{spatialaudiogen}`{=latex}, **-all** indicates using the entire train, validation, and test sets of the specific dataset, and **(p)** denotes using pseudo ground truth generated by Depth Anything `\cite{depthanything}`{=latex}. Due to space limits, we provide the experimental setup in the appendix, including implementation details and evaluation metrics.

## Baselines [sec:baseline]

Recent state-of-the-art methods `\cite{ai2023hrdfuse, EGFormer, BiFuse20, BiFuse++, jiang2021unifuse, SliceNet, shen2022panoformer}`{=latex} have emerged. We chose UniFuse and BiFuse++ as our baseline models for experiments, as many of the aforementioned methods did not fully release pre-trained models or provide training code and implementation details. It’s worth noting that PanoFormer `\cite{shen2022panoformer}`{=latex} is not included due to incorrect evaluation code and results. Both selected models are re-implemented with affine-invariant loss on disparity for a fair comparison and to demonstrate improvement. We conduct experiments on the Matterport3D `\cite{Matterport3D}`{=latex} benchmark to demonstrate performance gains within the same dataset/domain, and we perform zero-shot evaluation on the Stanford2D3D `\cite{Stanford2D3D}`{=latex} test set to demonstrate the generalization capability of our proposed training technique. To further validate its robustness, we evaluate additional baseline models `\cite{Hohonet, EGFormer}`{=latex} in zero-shot setting, showcasing the effectiveness of our approach for non-dual-projection models.

## Benchmarks Evaluation [sec:benchmark]

We conducted our in-domain improvement experiment on the widely used 360-degree depth benchmark, Matterport3D `\cite{Matterport3D}`{=latex}, to showcase the results of perspective foundation model distillation on the two selected baseline models, UniFuse `\cite{jiang2021unifuse}`{=latex} and BiFuse++`\cite{BiFuse++}`{=latex}. In Table <a href="#tab:matterport" data-reference-type="ref" data-reference="tab:matterport">2</a>, we list the metric depth evaluation results from state-of-the-art methods on this benchmark. Subsequently, we present the re-trained baseline models using affine-invariant loss on disparity to ensure a fair comparison with their original depth metric training. Finally, we demonstrate the improvement achieved with results trained on the labeled Matterport3D training set and the entire Structured3D dataset with pseudo ground truth.

<div id="tab:matterport" markdown="1">

| Method | Loss | Train | Test | Abs Rel ↓ | \\(\delta_1\\) ↑ | \\(\delta_2\\) ↑ | \\(\delta_3\\) ↑ |
|:---|:---|:---|:---|:--:|:--:|:--:|:--:|
| BiFuse `\cite{BiFuse20}`{=latex} | BerHu | M | M | \- | 0.845 | 0.932 | 0.963 |
| UniFuse `\cite{jiang2021unifuse}`{=latex} | BerHu | M | M | 0.106 | 0.890 | 0.962 | 0.983 |
| SliceNet `\cite{SliceNet}`{=latex} | BerHu | M | M | \- | 0.872 | 0.948 | 0.972 |
| BiFuse++ `\cite{BiFuse++}`{=latex} | BerHu | M | M | \- | 0.879 | 0.952 | 0.977 |
| HRDFuse `\cite{ai2023hrdfuse}`{=latex} | BerHu | M | M | 0.097 | 0.916 | 0.967 | 0.984 |
| UniFuse `\cite{jiang2021unifuse}`{=latex} | Affine-Inv | M | M | 0.102 | 0.893 | 0.970 | 0.989 |
| UniFuse `\cite{jiang2021unifuse}`{=latex} | Affine-Inv | M, ST-all (p) | M | <u>0.089</u> | 0.911 | <u>0.975</u> | <u>0.991</u> |
| BiFuse++ `\cite{BiFuse++}`{=latex} | Affine-Inv | M | M | 0.094 | <u>0.914</u> | 0.974 | 0.989 |
| BiFuse++ `\cite{BiFuse++}`{=latex} | Affine-Inv | M, ST-all (p) | M | **0.085** | **0.917** | **0.976** | **0.991** |

**Matterport3D Benchmark.** The upper section lists 360 methods trained with metric depths in meters using BerHu loss. All numbers are sourced from their respective papers. The lower section includes selected methods retrained with relative depth (disparity) using affine-invariant loss.

</div>

## Zero-Shot Evaluation [sec:zero-shot]

Our goal is to estimate depths for all 360-degree images, making zero-shot performance crucial. Following previous works `\cite{BiFuse20, jiang2021unifuse}`{=latex}, we adopted their zero-shot comparison setting, where models trained on the entire Matterport3D `\cite{Matterport3D}`{=latex} dataset are tested on the Stanford2D3D `\cite{Stanford2D3D}`{=latex} test set. In Table <a href="#stanford_table" data-reference-type="ref" data-reference="stanford_table">3</a>, the upper section lists methods trained with metric depth ground truth, with numbers sourced from their respective papers. The lower section includes models trained with affine-invariant loss on disparity ground truth. As shown in Figure <a href="#fig:zeroshot_both" data-reference-type="ref" data-reference="fig:zeroshot_both">6</a>, `\cite{jiang2021unifuse, BiFuse++}`{=latex} demonstrate generalization improvements with a lower error on the Stanford2D3D dataset.

Depth Anything `\cite{depthanything}`{=latex} and Marigold `\cite{Marigold}`{=latex} are state-of-the-art zero-shot depth models trained with perspective depths. As shown in Table <a href="#stanford_table" data-reference-type="ref" data-reference="stanford_table">3</a>, due to the domain gap and different camera projections, foundation models trained with perspective depth cannot be directly applied to 360-degree images. We demonstrated the zero-shot improvement on UniFuse `\cite{jiang2021unifuse}`{=latex}, BiFuse++ `\cite{BiFuse++}`{=latex} and non-dual-projection methods `\cite{Hohonet, EGFormer}`{=latex} with models trained on the entire Matterport3D `\cite{Matterport3D}`{=latex} dataset with ground truth and the entire Structured3D `\cite{Structured3D}`{=latex} or SpatialAudioGen `\cite{spatialaudiogen}`{=latex} dataset with pseudo ground truth generated using Depth Anything `\cite{depthanything}`{=latex}.

As Structured3D provides ground truth labels for its dataset, we also evaluate our models on its test set to assess how well they perform with pseudo labels. Table <a href="#structured3d_table" data-reference-type="ref" data-reference="structured3d_table">4</a> shows the improvements achieved on the Structured3D test set when using models trained with pseudo labels. It’s worth noting that even when the 360 model is trained on pseudo labels from SpatialAudioGen, it performs similarly well. This demonstrates the success of our distillation technique and the model’s ability to generalize across different datasets.

<div id="stanford_table" markdown="1">

| Method | Loss | train | test | Abs Rel ↓ | \\(\delta_1\\) ↑ | \\(\delta_2\\) ↑ | \\(\delta_3\\) ↑ |
|:---|:---|:---|:---|:--:|:--:|:--:|:--:|
| BiFuse `\cite{BiFuse20}`{=latex} | BerHu | M-all | SF | 0.120 | 0.862 | \- | \- |
| UniFuse `\cite{jiang2021unifuse}`{=latex} | BerHu | M-all | SF | 0.094 | 0.913 | \- | \- |
| BiFuse++ `\cite{BiFuse++}`{=latex} | BerHu | M-all | SF | 0.107 | 0.914 | 0.975 | 0.989 |
| Depth Anything `\cite{depthanything}`{=latex} | Affine-Inv | Pers. | SF | 0.248 | 0.635 | 0.899 | 0.97 |
| Marigold `\cite{Marigold}`{=latex} | Affine-Inv | Pers. | SF | 0.195 | 0.692 | 0.942 | 0.982 |
| UniFuse `\cite{jiang2021unifuse}`{=latex} | Affine-Inv | M-all | SF | 0.090 | 0.914 | 0.976 | 0.990 |
| UniFuse `\cite{jiang2021unifuse}`{=latex} | Affine-Inv | M-all, ST-all (p) | SF | <u>0.086</u> | 0.924 | 0.977 | 0.990 |
| UniFuse `\cite{jiang2021unifuse}`{=latex} | Affine-Inv | M-all, SP-all (p) | SF | 0.090 | 0.920 | <u>0.978</u> | 0.990 |
| BiFuse++ `\cite{BiFuse++}`{=latex} | Affine-Inv | M-all | SF | 0.090 | 0.921 | 0.976 | 0.990 |
| BiFuse++ `\cite{BiFuse++}`{=latex} | Affine-Inv | M-all, ST-all (p) | SF | **0.082** | **0.931** | **0.979** | <u>0.991</u> |
| BiFuse++ `\cite{BiFuse++}`{=latex} | Affine-Inv | M-all, SP-all (p) | SF | <u>0.086</u> | <u>0.926</u> | **0.979** | <u>0.991</u> |
| HoHoNet `\cite{Hohonet}`{=latex} | Affine-Inv | M-all | SF | 0.095 | 0.906 | 0.975 | <u>0.991</u> |
| HoHoNet `\cite{Hohonet}`{=latex} | Affine-Inv | M-all, ST-all (p) | SF | 0.088 | 0.920 | **0.979** | **0.992** |
| EGFormer `\cite{EGFormer}`{=latex} | Affine-Inv | M-all | SF | 0.098 | 0.906 | 0.972 | 0.989 |
| EGFormer `\cite{EGFormer}`{=latex} | Affine-Inv | M-all, ST-all (p) | SF | <u>0.086</u> | 0.923 | 0.976 | 0.990 |

**Zero-shot Evaluation on Stanford2D3D.** We perform zero-shot evaluations with models trained on other datasets. Following the original training settings, we train the 360 models `\cite{BiFuse++, jiang2021unifuse, Hohonet, EGFormer}`{=latex} on the entire Matterport3D dataset and then test on Stanford3D’s test set.

</div>

<figure id="fig:zeroshot_both">
<img src="./figures/zeroshot_unifuse_bifusev2.png"" style="width:100.0%" />
<figcaption><strong>Zero-shot qualitative with UniFuse <span class="citation" data-cites="jiang2021unifuse"></span> (<em>left</em>) and BiFuse++ <span class="citation" data-cites="BiFuse++"></span> (<em>right</em>) tested on Stanford2D3D.</strong> </figcaption>
</figure>

<div id="structured3d_table" markdown="1">

| Method | Loss | train | test | Abs Rel ↓ | \\(\delta_1\\) ↑ | \\(\delta_2\\) ↑ | \\(\delta_3\\) ↑ |
|:---|:---|:---|:---|:--:|:--:|:--:|:--:|
| UniFuse `\cite{jiang2021unifuse}`{=latex} | Affine-Inv | M-all | ST | 0.202 | 0.759 | 0.932 | 0.970 |
| UniFuse `\cite{jiang2021unifuse}`{=latex} | Affine-Inv | M-all, ST-all (p) | ST | 0.130 | 0.887 | 0.953 | 0.977 |
| UniFuse `\cite{jiang2021unifuse}`{=latex} | Affine-Inv | M-all, SP-all (p) | ST | 0.152 | 0.864 | 0.946 | 0.972 |

**Structured3D Test Set.** We demonstrate the improvement on the Structured3D test set using pseudo ground truth for training. The lower section shows enhancements with models trained on pseudo ground truth from Matterport3D and SpatialAudioGen, indicating similar improvements. This highlights the successful distillation of Depth Anything.

</div>

## Qualtative Results in the Wild [sec:realworld]

We demonstrated the qualitative results in Figure <a href="#fig:realworld" data-reference-type="ref" data-reference="fig:realworld">8</a> and Figure <a href="#fig:realworld_depth" data-reference-type="ref" data-reference="fig:realworld_depth">7</a> 360-degree images that were either captured by us or downloaded from the internet[^1]. These examples showcase the zero-shot capability of our model when applied to data outside the aforementioned 360-degree datasets.

<div id="metric_depth_table" markdown="1">

| Method | MAE ↓ | Abs Rel ↓ | RMSE ↓ | RMSElog ↓ | \\(\delta_1\\) ↑ | \\(\delta_2\\) ↑ | \\(\delta_3\\) ↑ |
|:---|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| UniFuse `\cite{jiang2021unifuse}`{=latex} | 0.208 | 0.111 | 0.369 | 0.072 | 0.871 | 0.966 | 0.988 |
| UniFuse `\cite{jiang2021unifuse}`{=latex} (Ours) | 0.206 | 0.118 | 0.351 | 0.049 | 0.910 | 0.971 | 0.987 |

**Metric depth fine-tuning.** We fine-tune our model trained with Matterport3D `\cite{Matterport3D}`{=latex} ground truth label and Structured3D `\cite{Structured3D}`{=latex} pseudo label on relative depth with Stanford2D3D `\cite{Stanford2D3D}`{=latex}’s training set metric depths with a single epoch.

</div>

<figure id="fig:realworld_depth">
<img src="./figures/in_the_wild_depth_mask-crop.png"" style="width:100.0%" />
<figcaption><strong>Generalization ability in the wild with depth map visualization.</strong> We showcase zero-shot qualitative results using a combination of images we captured and randomly sourced from the internet to assess the model’s generalization ability. For privacy reasons, we have obscured the cameraman in the images.</figcaption>
</figure>

<figure id="fig:realworld">
<img src="./figures/in_the_wild_pcl.png"" style="width:100.0%" />
<figcaption><strong>Generalization ability in the wild with point cloud visualization.</strong> We showcase zero-shot qualitative results in the point cloud using a combination of images we captured and randomly sourced from the internet to assess the model’s generalization ability.</figcaption>
</figure>

## Fine-Tuned to Metric Depth Estimation [sec:metric_depth]

We use our pre-trained model as an initial weight and fine-tune on Stanford2D3D `\cite{Stanford2D3D}`{=latex} metric depth to demonstrate the effectiveness of our pre-trained relative depth model’s ability to adapt to metric depth with a single epoch in Table <a href="#metric_depth_table" data-reference-type="ref" data-reference="metric_depth_table">5</a>

# Conclusion [sec:conclusion]

Our proposed method significantly advances 360-degree monocular depth estimation by leveraging perspective models for pseudo-label generation on unlabeled data. The use of cube projection with random rotation and affine-invariant loss ensures robust training and improved depth prediction accuracy while bridging the domain gap between perspective and equirectangular projection. By effectively addressing the challenges of limited labeled data with cross-domain distillation, our approach opens new possibilities for accurate depth estimation in 360 imagery. This work lays the groundwork for future research and applications, offering a promising direction for further advancements in 360-degree depth estimation.

#### Limitations. [sec:limitation]

Our work faces limitations due to its heavy reliance on the quality of unlabeled data and pseudo labels from perspective foundation models. The results are significantly impacted by data quality (Section <a href="#sec:data_cleaning" data-reference-type="ref" data-reference="sec:data_cleaning">3.1.0.2</a>). Without data cleaning, the training process resulted in NaN values. Another limitation is that although with unlabeled data, the scarcity of data still exists compared to other tasks.

# References [references]

<div class="thebibliography" markdown="1">

Hao Ai, Zidong Cao, Yan-Pei Cao, Ying Shan, and Lin Wang Hrdfuse: Monocular 360deg depth estimation by collaboratively learning holistic-with-regional depth distributions In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 13273–13282, 2023. **Abstract:** Depth estimation from a monocular 360° image is a burgeoning problem owing to its holistic sensing of a scene. Recently, some methods, e.g., OmniFusion, have applied the tangent projection (TP) to represent a 360° image and predicted depth values via patch-wise regressions, which are merged to get a depth map with equirectangular projection (ERP) format. However, these methods suffer from 1) non-trivial process of merging plenty of patches; 2) capturing less holistic-with-regional contextual information by directly regressing the depth value of each pixel. In this paper, we propose a novel framework, HRDFuse, that subtly combines the potential of convolutional neural networks (CNNs) and transformers by collaboratively learning the holistic contextual information from the ERP and the regional structural information from the TP. Firstly, we propose a spatial feature alignment (SFA) module that learns feature similarities between the TP and ERP to aggregate the TP features into a complete ERP feature map in a pixelwise manner. Secondly, we propose a collaborative depth distribution classification (CDDC) module that learns the holistic-with-regional histograms capturing the ERP and TP depth distributions. As such, the final depth values can be predicted as a linear combination of histogram bin centers. Lastly, we adaptively combine the depth predictions from ERP and TP to obtain the final depth map. Extensive experiments show that our method predicts more smooth and accurate depth results while achieving favorably better results than the SOTA methods. (@ai2023hrdfuse)

I. Armeni, A. Sax, A. R. Zamir, and S. Savarese *ArXiv e-prints*, February 2017. **Abstract:** We present a dataset of large-scale indoor spaces that provides a variety of mutually registered modalities from 2D, 2.5D and 3D domains, with instance-level semantic and geometric annotations. The dataset covers over 6,000m2 and contains over 70,000 RGB images, along with the corresponding depths, surface normals, semantic annotations, global XYZ images (all in forms of both regular and 360° equirectangular images) as well as camera information. It also includes registered raw and semantically annotated 3D meshes and point clouds. The dataset enables development of joint and cross-modal learning models and potentially unsupervised approaches utilizing the regularities present in large-scale indoor spaces. The dataset is available here: http://3Dsemantics.stanford.edu/ (@Stanford2D3D)

Jiayang Bai, Haoyu Qin, Shuichang Lai, Jie Guo, and Yanwen Guo Glpanodepth: Global-to-local panoramic depth estimation *IEEE Transactions on Image Processing*, 2024. **Abstract:** In this paper, we propose a learning-based method for predicting dense depth values of a scene from a monocular omnidirectional image. An omnidirectional image has a full field-of-view, providing much more complete descriptions of the scene than perspective images. However, fully-convolutional networks that most current solutions rely on fail to capture rich global contexts from the panorama. To address this issue and also the distortion of equirectangular projection in the panorama, we propose Cubemap Vision Transformers (CViT), a new transformer-based architecture that can model long-range dependencies and extract distortion-free global features from the panorama. We show that cubemap vision transformers have a global receptive field at every stage and can provide globally coherent predictions for spherical signals. To preserve important local features, we further design a convolution-based branch in our pipeline (dubbed GLPanoDepth) and fuse global features from cubemap vision transformers at multiple scales. This global-to-local strategy allows us to fully exploit useful global and local features in the panorama, achieving state-of-the-art performance in panoramic depth estimation. (@bai2024glpanodepth)

Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Müller Zoedepth: Zero-shot transfer by combining relative and metric depth *arXiv preprint arXiv:2302.12288*, 2023. **Abstract:** This paper tackles the problem of depth estimation from a single image. Existing work either focuses on generalization performance disregarding metric scale, i.e. relative depth estimation, or state-of-the-art results on specific datasets, i.e. metric depth estimation. We propose the first approach that combines both worlds, leading to a model with excellent generalization performance while maintaining metric scale. Our flagship model, ZoeD-M12-NK, is pre-trained on 12 datasets using relative depth and fine-tuned on two datasets using metric depth. We use a lightweight head with a novel bin adjustment design called metric bins module for each domain. During inference, each input image is automatically routed to the appropriate head using a latent classifier. Our framework admits multiple configurations depending on the datasets used for relative depth pre-training and metric fine-tuning. Without pre-training, we can already significantly improve the state of the art (SOTA) on the NYU Depth v2 indoor dataset. Pre-training on twelve datasets and fine-tuning on the NYU Depth v2 indoor dataset, we can further improve SOTA for a total of 21% in terms of relative absolute error (REL). Finally, ZoeD-M12-NK is the first model that can jointly train on multiple datasets (NYU Depth v2 and KITTI) without a significant drop in performance and achieve unprecedented zero-shot generalization performance to eight unseen datasets from both indoor and outdoor domains. The code and pre-trained models are publicly available at https://github.com/isl-org/ZoeDepth . (@ZoeDepth)

Reiner Birkl, Diana Wofk, and Matthias Müller Midas v3.1 – a model zoo for robust monocular relative depth estimation *arXiv preprint arXiv:2307.14460*, 2023. **Abstract:** We release MiDaS v3.1 for monocular depth estimation, offering a variety of new models based on different encoder backbones. This release is motivated by the success of transformers in computer vision, with a large variety of pretrained vision transformers now available. We explore how using the most promising vision transformers as image encoders impacts depth estimation quality and runtime of the MiDaS architecture. Our investigation also includes recent convolutional approaches that achieve comparable quality to vision transformers in image classification tasks. While the previous release MiDaS v3.0 solely leverages the vanilla vision transformer ViT, MiDaS v3.1 offers additional models based on BEiT, Swin, SwinV2, Next-ViT and LeViT. These models offer different performance-runtime tradeoffs. The best model improves the depth estimation quality by 28% while efficient models enable downstream tasks requiring high frame rates. We also describe the general process for integrating new backbones. A video summarizing the work can be found at https://youtu.be/UjaeNNFf9sE and the code is available at https://github.com/isl-org/MiDaS. (@MiDaS_v3)

Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang Matterport3d: Learning from rgb-d data in indoor environments *International Conference on 3D Vision (3DV)*, 2017. **Abstract:** Access to large, diverse RGB-D datasets is critical for training RGB-D scene understanding algorithms. However, existing datasets still cover only a limited number of views or a restricted scale of spaces. In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and region classification. (@Matterport3D)

Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng Single-image depth perception in the wild *Advances in neural information processing systems*, 29, 2016. **Abstract:** This paper studies single-image depth perception in the wild, i.e., recovering depth from a single image taken in unconstrained settings. We introduce a new dataset "Depth in the Wild" consisting of images in the wild annotated with relative depth between pairs of random points. We also propose a new algorithm that learns to estimate metric depth using annotations of relative depth. Compared to the state of the art, our algorithm is simpler and performs better. Experiments show that our algorithm, combined with existing RGB-D data and our new relative depth annotations, significantly improves single-image depth perception in the wild. (@chen2016single)

Yichen Chen, Yuqi Pan, Ruyu Liu, Haoyu Zhang, Guodao Zhang, Bo Sun, and Jianhua Zhang 360orb-slam: A visual slam system for panoramic images with depth completion network In *2024 27th International Conference on Computer Supported Cooperative Work in Design (CSCWD)*, pages 717–722. IEEE, 2024. **Abstract:** —To enhance the performance and effect of AR/VR applications and visual assistance and inspection systems, visual simultaneous localization and mapping (vSLAM) is a fundamen- tal task in computer vision and robotics. However, traditional vSLAM systems are limited by the camera’s narrow field-of-view, resulting in challenges such as sparse feature distribution and lack of dense depth information. To overcome these limitations, this paper proposes a 360ORB-SLAM system for panoramic images that combines with a depth completion network. The system extracts feature points from the panoramic image, utilizes a panoramic triangulation module to generate sparse depth information, and employs a depth completion network to obtain a dense panoramic depth map. Experimental results on our novel panoramic dataset constructed based on Carla demonstrate that the proposed method achieves superior scale accuracy compared to existing monocular SLAM methods and effectively addresses the challenges of feature association and scale ambiguity. The integration of the depth completion network enhances system stability and mitigates the impact of dynamic elements on SLAM performance. (@chen2024360orb)

Hsien-Tzu Cheng, Chun-Hung Chao, Jin-Dong Dong, Hao-Kai Wen, Tyng-Luh Liu, and Min Sun Cube padding for weakly-supervised saliency prediction in 360 \\(\{\\)\\(\backslash\\)deg\\(\}\\) videos *arXiv preprint arXiv:1806.01320*, 2018. **Abstract:** Automatic saliency prediction in 360{\\}deg} videos is critical for viewpoint guidance applications (e.g., Facebook 360 Guide). We propose a spatial-temporal network which is (1) weakly-supervised trained and (2) tailor-made for 360{\\}deg} viewing sphere. Note that most existing methods are less scalable since they rely on annotated saliency map for training. Most importantly, they convert 360{\\}deg} sphere to 2D images (e.g., a single equirectangular image or multiple separate Normal Field-of-View (NFoV) images) which introduces distortion and image boundaries. In contrast, we propose a simple and effective Cube Padding (CP) technique as follows. Firstly, we render the 360{\\}deg} view on six faces of a cube using perspective projection. Thus, it introduces very little distortion. Then, we concatenate all six faces while utilizing the connectivity between faces on the cube for image padding (i.e., Cube Padding) in convolution, pooling, convolutional LSTM layers. In this way, CP introduces no image boundary while being applicable to almost all Convolutional Neural Network (CNN) structures. To evaluate our method, we propose Wild-360, a new 360{\\}deg} video saliency dataset, containing challenging videos with saliency heatmap annotations. In experiments, our method outperforms baseline methods in both speed and quality. (@Cubepad)

Jaehoon Cho, Dongbo Min, Youngjung Kim, and Kwanghoon Sohn Diml/cvl rgb-d dataset: 2m rgb-d images of natural indoor and outdoor scenes *arXiv preprint arXiv:2110.11590*, 2021. **Abstract:** This manual is intended to provide a detailed description of the DIML/CVL RGB-D dataset. This dataset is comprised of 2M color images and their corresponding depth maps from a great variety of natural indoor and outdoor scenes. The indoor dataset was constructed using the Microsoft Kinect v2, while the outdoor dataset was built using the stereo cameras (ZED stereo camera and built-in stereo camera). Table I summarizes the details of our dataset, including acquisition, processing, format, and toolbox. Refer to Section II and III for more details. (@DIML)

Benjamin Coors, Alexandru Paul Condurache, and Andreas Geiger Spherenet: Learning spherical representations for detection and classification in omnidirectional images In *Proceedings of the European Conference on Computer Vision (ECCV)*, pages 518–533, 2018. (@coors2018spherenet)

Brandon Yushan Feng, Wangjue Yao, Zheyuan Liu, and Amitabh Varshney Deep depth estimation on 360 images with a double quaternion loss In *2020 International Conference on 3D Vision (3DV)*, pages 524–533. IEEE, 2020. **Abstract:** While 360° images are becoming ubiquitous due to popularity of panoramic content, they cannot directly work with most of the existing depth estimation techniques developed for perspective images. In this paper, we present a deep-learning-based framework of estimating depth from 360° images. We present an adaptive depth refinement procedure that refines depth estimates using normal estimates and pixel-wise uncertainty scores. We introduce double quaternion approximation to combine the loss of the joint estimation of depth and surface normal. Furthermore, we use the double quaternion formulation to also measure stereo consistency between the horizontally displaced depth maps, leading to a new loss function for training a depth estimation CNN. Results show that the new double-quaternion-based loss and the adaptive depth refinement procedure lead to better network performance. Our proposed method can be used with monocular as well as stereo images. When evaluated on several datasets, our method surpasses state-of-the-art methods on most metrics. (@feng2020deep)

Qi Feng, Hubert PH Shum, and Shigeo Morishima depth estimation in the wild-the depth360 dataset and the segfuse network In *2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)*, pages 664–673. IEEE, 2022. **Abstract:** Single-view depth estimation from omnidirectional images has gained popularity with its wide range of applications such as autonomous driving and scene reconstruction. Although data-driven learning-based methods demonstrate significant potential in this field, scarce training data and ineffective 360 estimation algorithms are still two key limitations hindering accurate estimation across diverse domains. In this work, we first establish a large-scale dataset with varied settings called Depth360 to tackle the training data problem. This is achieved by exploring the use of a plenteous source of data, 360 videos from the internet, using a test-time training method that leverages unique information in each omnidirectional sequence. With novel geometric and temporal constraints, our method generates consistent and convincing depth samples to facilitate single-view estimation. We then propose an end-to-end two-branch multi-task learning network, SegFuse, that mimics the human eye to effectively learn from the dataset and estimate high-quality depth maps from diverse monocular RGB images. With a peripheral branch that uses equirectangular projection for depth estimation and a foveal branch that uses cubemap projection for semantic segmentation, our method predicts consistent global depth while maintaining sharp details at local regions. Experimental results show favorable performance against the state-of-the-art methods. (@feng2022360)

V. Guizilini, I. Vasiljevic, D. Chen, R. Ambrus, and A. Gaidon Towards zero-shot scale-aware monocular depth estimation In *2023 IEEE/CVF International Conference on Computer Vision (ICCV)*, pages 9199–9209, Los Alamitos, CA, USA, oct 2023. IEEE Computer Society. . URL <https://doi.ieeecomputersociety.org/10.1109/ICCV51070.2023.00847>. **Abstract:** Monocular depth estimation is scale-ambiguous, and thus requires scale supervision to produce metric predictions. Even so, the resulting models will be geometry-specific, with learned scales that cannot be directly transferred across domains. Because of that, recent works focus instead on relative depth, eschewing scale in favor of improved up-to-scale zero-shot transfer. In this work we introduce ZeroDepth, a novel monocular depth estimation framework capable of predicting metric scale for arbitrary test images from different domains and camera parameters. This is achieved by (i) the use of input-level geometric embeddings that enable the network to learn a scale prior over objects; and (ii) decoupling the encoder and decoder stages, via a variational latent representation that is conditioned on single frame information. We evaluated ZeroDepth targeting both outdoor (KITTI, DDAD, nuScenes) and indoor (NYUv2) benchmarks, and achieved a new state-of-the-art in both settings using the same pre-trained model, outperforming methods that train on in-domain data and require test-time scaling to produce metric estimates. Project page: https://sites.google.com/view/tri-zerodepth. (@Guizilini2023)

Yu-Kai Huang, Tsung-Han Wu, Yueh-Cheng Liu, and Winston H Hsu Indoor depth completion with boundary consistency and self-attention In *Proceedings of the IEEE/CVF international conference on computer vision workshops*, pages 0–0, 2019. **Abstract:** Depth estimation features are helpful for 3D recognition. Commodity-grade depth cameras are able to capture depth and color image in real-time. However, glossy, transparent or distant surface cannot be scanned properly by the sensor. As a result, enhancement and restoration from sensing depth is an important task. Depth completion aims at filling the holes that sensors fail to detect, which is still a complex task for machine to learn. Traditional hand-tuned methods have reached their limits, while neural network based methods tend to copy and interpolate the output from surrounding depth values. This leads to blurred boundaries, and structures of the depth map are lost. Consequently, our main work is to design an end-to-end network improving completion depth maps while maintaining edge clarity. We utilize self-attention mechanism, previously used in image inpainting fields, to extract more useful information in each layer of convolution so that the complete depth map is enhanced. In addition, we propose boundary consistency concept to enhance the depth map quality and structure. Experimental results validate the effectiveness of our self-attention and boundary consistency schema, which outperforms previous state-of-the-art depth completion work on Matterport3D dataset. (@huang2019indoor)

Hualie Jiang, Zhe Sheng, Siyu Zhu, Zilong Dong, and Rui Huang Unifuse: Unidirectional fusion for 360\\(^{\circ}\\) panorama depth estimation *IEEE Robotics and Automation Letters*, 2021. **Abstract:** Learning depth from spherical panoramas is becoming a popular research topic because a panorama has a full field-of-view of the environment and provides a relatively complete description of a scene. However, applying well-studied CNNs for perspective images to the standard representation of spherical panoramas, i.e., the equirectangular projection, is suboptimal, as it becomes distorted towards the poles. Another representation is the cubemap projection, which is distortion-free but discontinued on edges and limited in the field-of-view. This letter introduces a new framework to fuse features from the two projections, unidirectionally feeding the cubemap features to the equirectangular features only at the decoding stage. Unlike the recent bidirectional fusion approach operating at both the encoding and decoding stages, our fusion scheme is much more efficient. Besides, we also designed a more effective fusion module for our fusion scheme. Experiments verify the effectiveness of our proposed fusion strategy and module, and our model achieves state-of-the-art performance on four popular datasets. Additional experiments show that our model also has the advantages of model complexity and generalization capability. (@jiang2021unifuse)

Lei Jin, Yanyu Xu, Jia Zheng, Junfei Zhang, Rui Tang, Shugong Xu, Jingyi Yu, and Shenghua Gao Geometric structure based and regularized depth estimation from 360 indoor imagery In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 889–898, 2020. **Abstract:** Motivated by the correlation between the depth and the geometric structure of a 360 indoor image, we propose a novel learning-based depth estimation framework that leverages the geometric structure of a scene to conduct depth estimation. Specifically, we represent the geometric structure of an indoor scene as a collection of corners, boundaries and planes. On the one hand, once a depth map is estimated, this geometric structure can be inferred from the estimated depth map; thus, the geometric structure functions as a regularizer for depth estimation. On the other hand, this estimation also benefits from the geometric structure of a scene estimated from an image where the structure functions as a prior. However, furniture in indoor scenes makes it challenging to infer geometric structure from depth or image data. An attention map is inferred to facilitate both depth estimation from features of the geometric structure and also geometric inferences from the estimated depth map. To validate the effectiveness of each component in our framework under controlled conditions, we render a synthetic dataset, Shanghaitech-Kujiale Indoor 360 dataset with 3550 360 indoor images. Extensive experiments on popular datasets validate the effectiveness of our solution. We also demonstrate that our method can also be applied to counterfactual depth. (@jin2020geometric)

Masum Shah Junayed, Arezoo Sadeghzadeh, Md Baharul Islam, Lai-Kuan Wong, and Tarkan Aydın Himode: A hybrid monocular omnidirectional depth estimation model In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 5212–5221, 2022. **Abstract:** Monocular omnidirectional depth estimation is receiving considerable research attention due to its broad applications for sensing 360° surroundings. Existing approaches in this field suffer from limitations in recovering small object details and data lost during the ground-truth depth map acquisition. In this paper, a novel monocular omnidirectional depth estimation model, namely HiMODE is proposed based on a hybrid CNN+Transformer (encoder-decoder) architecture whose modules are efficiently designed to mitigate distortion and computational cost, without performance degradation. Firstly, we design a feature pyramid network based on the HNet block to extract high-resolution features near the edges. The performance is further improved, benefiting from a self and cross attention layer and spatial/temporal patches in the Transformer encoder and decoder, respectively. Besides, a spatial residual block is employed to reduce the number of parameters. By jointly passing the deep features extracted from an input image at each backbone block, along with the raw depth maps predicted by the transformer encoder-decoder, through a context adjustment layer, our model can produce resulting depth maps with better visual quality than the ground-truth. Comprehensive ablation studies demonstrate the significance of each individual module. Extensive experiments conducted on three datasets; Stanford3D, Matterport3D, and SunCG, demonstrate that HiMODE can achieve state-of-the-art performance for 360° monocular depth estimation. Complete project code and supplementary materials are available at https://github.com/himode5008/HiMODE. (@junayed2022himode)

Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler Repurposing diffusion-based image generators for monocular depth estimation In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 2024. **Abstract:** Monocular depth estimation is a fundamental computer vision task. Recovering 3D depth from a single image is geometrically ill-posed and requires scene understanding, so it is not surprising that the rise of deep learning has led to a breakthrough. The impressive progress of monocular depth estimators has mirrored the growth in model capacity, from relatively modest CNNs to large Transformer architectures. Still, monocular depth estimators tend to struggle when presented with images with unfamiliar content and layout, since their knowledge of the visual world is restricted by the data seen during training, and challenged by zero-shot generalization to new domains. This motivates us to explore whether the extensive priors captured in recent generative diffusion models can enable better, more generalizable depth estimation. We introduce Marigold, a method for affine-invariant monocular depth estimation that is derived from Stable Diffusion and retains its rich prior knowledge. The estimator can be fine-tuned in a couple of days on a single GPU using only synthetic training data. It delivers state-of-the-art performance across a wide range of datasets, including over 20% performance gains in specific cases. Project page: https://marigoldmonodepth.github.io. (@Marigold)

Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick Segment anything *arXiv:2304.02643*, 2023. **Abstract:** We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive – often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision. (@SAM)

Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale *IJCV*, 2020. **Abstract:** We present Open Images V4, a dataset of 9:2M images with uniﬁed annotations for image classiﬁcation, ob- ject detection and visual relationship detection. The images have a Creative Commons Attribution license that allows to share and adapt the material, and they have been collected from Flickr without a predeﬁned list of class names or tags, leading to natural class statistics and avoiding an initial de- sign bias. Open Images V4 offers large scale across several dimensions: 30:1M image-level labels for 19:8k concepts, 15:4M bounding boxes for 600object classes, and 375k vi- sual relationship annotations involving 57classes. For ob- ject detection in particular, we provide 15more bounding boxes than the next largest datasets ( 15:4M boxes on 1:9M images). The images often show complex scenes with sev- eral objects (8 annotated objects per image on average). We annotated visual relationships between them, which support visual relationship detection, an emerging task that requires structured reasoning. We provide in-depth comprehensive statistics about the dataset, we validate the quality of the annotations, we study how the performance of several mod- ern models evolves with increasing amounts of training data, and we demonstrate two applications made possible by hav- ing uniﬁed annotations of multiple types coexisting in the same images. We hope that the scale, quality, and variety of Open Images V4 will foster further research and innovation even beyond the areas of image classiﬁcation, object detec- tion, and visual relationship detection. (@OpenImages)

Dong-Hyun Lee Pseudo-label : The simple and efficient semi-supervised learning method for deep neural networks . URL <https://api.semanticscholar.org/CorpusID:18507866>. **Abstract:** Recently, deep learning with Convolutional Neural Networks (CNNs) and Transformers has shown encouraging results in fully supervised medical image segmentation. However, it is still challenging for them to achieve good performance with limited annotations for training. In this work, we present a very simple yet efficient framework for semi-supervised medical image segmentation by introducing the cross teaching between CNN and Transformer. Specifically, we simplify the classical deep co-training from consistency regularization to cross teaching, where the prediction of a network is used as the pseudo label to supervise the other network directly end-to-end. Considering the difference in learning paradigm between CNN and Transformer, we introduce the Cross Teaching between CNN and Transformer rather than just using CNNs. Experiments on a public benchmark show that our method outperforms eight existing semi-supervised learning methods just with a simpler framework. Notably, this work may be the first attempt to combine CNN and transformer for semi-supervised medical image segmentation and achieve promising results on a public benchmark. The code will be released at: https://github.com/HiLab-git/SSL4MIS. (@Lee2013PseudoLabelT)

Yuyan Li, Yuliang Guo, Zhixin Yan, Xinyu Huang, Duan Ye, and Liu Ren Omnifusion: 360 monocular depth estimation via geometry-aware fusion In *2022 Conference on Computer Vision and Pattern Recognition (CVPR)*, New Orleans, USA, June 2022. **Abstract:** A well-known challenge in applying deep-learning methods to omnidirectional images is spherical distortion. In dense regression tasks such as depth estimation, where structural details are required, using a vanilla CNN layer on the distorted 360 image results in undesired information loss. In this paper, we propose a 360 monocular depth estimation pipeline, OmniFusion, to tackle the spherical distortion issue. Our pipeline transforms a 360 image into less-distorted perspective patches (i.e. tangent images) to obtain patch-wise predictions via CNN, and then merge the patch-wise results for final output. To handle the discrepancy between patch-wise predictions which is a major issue affecting the merging quality, we propose a new framework with the following key components. First, we propose a geometry-aware feature fusion mechanism that combines 3D geometric features with 2D image features to compensate for the patch-wise discrepancy. Second, we employ the self-attention-based transformer architecture to conduct a global aggregation of patch-wise information, which further improves the consistency. Last, we introduce an iterative depth refinement mechanism, to further refine the estimated depth based on the more accurate geometric features. Experiments show that our method greatly mitigates the distortion issue, and achieves state-of-the-art performances on several 360 monocular depth estimation benchmark datasets. Our code is available at https://github.com/yuyanli0831/OmniFusion. (@OmniFusion)

Zhengqi Li and Noah Snavely Megadepth: Learning single-view depth prediction from internet photos In *Computer Vision and Pattern Recognition (CVPR)*, 2018. **Abstract:** Single-view depth prediction is a fundamental problem in computer vision. Recently, deep learning methods have led to significant progress, but such methods are limited by the available training data. Current datasets based on 3D sensors have key limitations, including indoor-only images (NYU), small numbers of training examples (Make3D), and sparse sampling (KITTI). We propose to use multi-view Internet photo collections, a virtually unlimited data source, to generate training data via modern structure-from-motion and multi-view stereo (MVS) methods, and present a large depth dataset called MegaDepth based on this idea. Data derived from MVS comes with its own challenges, including noise and unreconstructable objects. We address these challenges with new data cleaning methods, as well as automatically augmenting our data with ordinal depth relations generated using semantic segmentation. We validate the use of large amounts of Internet data by showing that models trained on MegaDepth exhibit strong generalization-not only to novel scenes, but also to other diverse datasets including Make3D, KITTI, and DIW, even when no images from those datasets are seen during training. (@MegaDepth)

Daniel Lichy, Hang Su, Abhishek Badki, Jan Kautz, and Orazio Gallo Fova-depth: Field-of-view agnostic depth estimation for cross-dataset generalization In *2024 International Conference on 3D Vision (3DV)*, pages 1–10, 2024. . **Abstract:** Wide field-of-view (FoV) cameras efficiently capture large portions of the scene, which makes them attractive in multiple domains, such as automotive and robotics. For such applications, estimating depth from multiple images is a critical task, and therefore, a large amount of ground truth (GT) data is available. Unfortunately, most of the GT data is for pinhole cameras, making it impossible to properly train depth estimation models for large-FoV cameras. We propose the first method to train a stereo depth estimation model on the widely available pinhole data, and to generalize it to data captured with larger FoVs. Our intuition is simple: We warp the training data to a canonical, large-FoV representation and augment it to allow a single network to reason about diverse types of distortions that otherwise would prevent generalization. We show strong generalization ability of our approach on both indoor and outdoor datasets, which was not possible with previous methods. (@10550831)

Ruyu Liu, Guodao Zhang, Jiangming Wang, and Shuwen Zhao Cross-modal 360 depth completion and reconstruction for large-scale indoor environment *IEEE Transactions on Intelligent Transportation Systems*, 23 (12): 25180–25190, 2022. **Abstract:** In a large-scale epidemic, reducing direct contact among medical personnel, attendants and patients has become a necessary means of epidemic prevention and control. Intelligent vehicles and mobile robots in the hospital environment, such as disinfection vehicles, logistics vehicles, nursing robots, and guiding robots, play an important role in improving the operational efficiency of the medical system and promoting epidemic prevention and governance. Powerful capabilities of environmental spatial perception and reconstruction are the keys to accurate localization, navigation, and obstacle avoidance for intelligent vehicles and autonomous robots in such operations. Omnidirectional perception is becoming increasingly important and proliferative in autonomous vehicles and robots since its wide field of view significantly enhances the perception ability. However, the lack of dense and accurate 360° depth datasets has brought the challenge to the omnidirectional perception. In this paper, we propose a depth-sensing and reconstruction system to address this challenge in the large-scale indoor environment. First, we design an omnidirectional depth completion convolutional neural network model, in which a spherical normalized convolutional and the unit sphere area-based loss are introduced to extract features from cross-modal omnidirectional input with unequal sparsity and deal with the imbalanced data distribution and distortion in the panoramic input. In addition, we present a 3D reconstruction system by integrating our depth completion into omnidirectional localization and dense mapping. We evaluate our method on 360D large-scale indoor datasets and real-world sequences of a challenging hospital scene. Extensive experiments show that the proposed method outperforms the other state-of-the-art (SoTA) approaches in terms of depth completion and 3D reconstruction. (@liu2022cross)

Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al Grounding dino: Marrying dino with grounded pre-training for open-set object detection *arXiv preprint arXiv:2303.05499*, 2023. **Abstract:** In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a $52.5$ AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean $26.1$ AP. Code will be available at \\}url{https://github.com/IDEA-Research/GroundingDINO}. (@groundingDINO)

Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski Dinov2: Learning robust visual features without supervision 2023. **Abstract:** The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels. (@oquab2023dinov2)

Timothy Langlois Pedro Morgado, Nuno Vasconcelos and Oliver Wang Self-supervised generation of spatial audio for 360\\(\deg\\) video In *Neural Information Processing Systems (NIPS)*, 2018. **Abstract:** We introduce an approach to convert mono audio recorded by a 360° video camera into spatial audio, a representation of the distribution of sound over the full viewing sphere. Spatial audio is an important component of immersive 360° video viewing, but spatial audio microphones are still rare in current 360° video production. Our system consists of end-to-end trainable neural networks that separate individual sound sources and localize them on the viewing sphere, conditioned on multi-modal analysis from the audio and 360° video frames. We introduce several datasets, including one filmed ourselves, and one collected in-the-wild from YouTube, consisting of 360° videos uploaded with spatial audio. During training, ground truth spatial audio serves as self-supervision and a mixed down mono track forms the input to our network. Using our approach we show that it is possible to infer the spatial localization of sounds based only on a synchronized 360° video and the mono audio track. (@spatialaudiogen)

Chi-Han Peng and Jiayao Zhang High-resolution depth estimation for 360-degree panoramas through perspective and panoramic depth images registration *arXiv preprint arXiv:2210.10414*, 2022. **Abstract:** We propose a novel approach to compute high-resolution (2048x1024 and higher) depths for panoramas that is significantly faster and qualitatively and qualitatively more accurate than the current state-of-the-art method (360MonoDepth). As traditional neural network-based methods have limitations in the output image sizes (up to 1024x512) due to GPU memory constraints, both 360MonoDepth and our method rely on stitching multiple perspective disparity or depth images to come out a unified panoramic depth map. However, to achieve globally consistent stitching, 360MonoDepth relied on solving extensive disparity map alignment and Poisson-based blending problems, leading to high computation time. Instead, we propose to use an existing panoramic depth map (computed in real-time by any panorama-based method) as the common target for the individual perspective depth maps to register to. This key idea made producing globally consistent stitching results from a straightforward task. Our experiments show that our method generates qualitatively better results than existing panorama-based methods, and further outperforms them quantitatively on datasets unseen by these methods. (@peng2022high)

Giovanni Pintore, Marco Agus, Eva Almansa, Jens Schneider, and Enrico Gobbetti : deep dense depth estimation from a single indoor panorama using a slice-based representation In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 11536–11545, June 2021. **Abstract:** We introduce a novel deep neural network to estimate a depth map from a single monocular indoor panorama. The network directly works on the equirectangular projection, exploiting the properties of indoor 360° images. Starting from the fact that gravity plays an important role in the design and construction of man-made indoor scenes, we propose a compact representation of the scene into vertical slices of the sphere, and we exploit long- and short-term relationships among slices to recover the equirectangular depth map. Our design makes it possible to maintain high-resolution information in the extracted features even with a deep network. The experimental results demonstrate that our method outperforms current state-of-the-art solutions in prediction accuracy, particularly for real-world data. (@SliceNet)

Giovanni Pintore, Eva Almansa, Armando Sanchez, Giorgio Vassena, and Enrico Gobbetti Deep panoramic depth prediction and completion for indoor scenes *Computational Visual Media*, pages 1–20, 2024. **Abstract:** Abstract We introduce a novel end-to-end deep-learning solution for rapidly estimating a dense spherical depth map of an indoor environment. Our input is a single equirectangular image registered with a sparse depth map, as provided by a variety of common capture setups. Depth is inferred by an efficient and lightweight single-branch network, which employs a dynamic gating system to process together dense visual data and sparse geometric data. We exploit the characteristics of typical man-made environments to efficiently compress multi-resolution features and find short- and long-range relations among scene parts. Furthermore, we introduce a new augmentation strategy to make the model robust to different types of sparsity, including those generated by various structured light sensors and LiDAR setups. The experimental results demonstrate that our method provides interactive performance and outperforms state-of-the-art solutions in computational efficiency, adaptivity to variable depth sparsity patterns, and prediction accuracy for challenging indoor data, even when trained solely on synthetic data without any fine tuning. (@pintore2024deep)

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al Learning transferable visual models from natural language supervision In *International conference on machine learning*, pages 8748–8763. PMLR, 2021. **Abstract:** State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP. (@CLIP)

René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun Vision transformers for dense prediction *ICCV*, 2021. **Abstract:** We introduce dense prediction transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense prediction transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense prediction transformers set a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art. Our models are available at https://github.com/intel-isl/DPT. (@DPT_MiDaS)

René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 44 (3), 2022. **Abstract:** The success of monocular depth estimation relies on large and diverse training sets. Due to the challenges associated with acquiring dense ground-truth depth across different environments at scale, a number of datasets with distinct characteristics and biases have emerged. We develop tools that enable mixing multiple datasets during training, even if their annotations are incompatible. In particular, we propose a robust training objective that is invariant to changes in depth range and scale, advocate the use of principled multi-objective learning to combine data from different sources, and highlight the importance of pretraining encoders on auxiliary tasks. Armed with these tools, we experiment with five diverse training datasets, including a new, massive data source: 3D films. To demonstrate the generalization power of our approach we use zero-shot cross-dataset transfer, i.e. we evaluate on datasets that were not seen during training. The experiments confirm that mixing data from complementary sources greatly improves monocular depth estimation. Our approach clearly outperforms competing methods across diverse datasets, setting a new state of the art for monocular depth estimation. (@MiDaS)

Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang Grounded sam: Assembling open-world models for diverse visual tasks 2024. **Abstract:** We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models. As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline. For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything. Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis. Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models. (@groundedSAM)

Manuel Rey-Area, Mingze Yuan, and Christian Richardt : High-resolution 360 monocular depth estimation In *CVPR*, 2022. **Abstract:** 360° cameras can capture complete environments in a single shot, which makes 360° imagery alluring in many computer vision tasks. However, monocular depth estimation remains a challenge for 360° data, particularly for high resolutions like 2K (2048 × 1 024) and beyond that are important for novel-view synthesis and virtual reality applications. Current CNN-based methods do not support such high resolutions due to limited GPU memory. In this work, we propose aflexible framework for monocular depth estimation from high-resolution 360° images using tangent images. We project the 360° input image onto a set of tangent planes that produce perspective views, which are suitable for the latest, most accurate state-of-the-art perspective monocular depth estimators. To achieve globally consistent disparity estimates, we recombine the individual depth estimates using deformable multi-scale alignment followed by gradient-domain blending. The result is a dense, high-resolution 360° depth map with a high level of detail, also for outdoor scenes which are not supported by existing methods. Our source code and data are available at https://manurare.github.io/360monodepth/. (@reyarea2021360monodepth)

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al Imagenet large scale visual recognition challenge *International journal of computer vision*, 115: 211–252, 2015. **Abstract:** The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classiﬁcation and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than ﬁfty insti- tutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key break- throughs in categorical object recognition, provide a detailed analysis of the current state of the ﬁeld of large-scale image classiﬁcation and object detection, and compare the state-of- the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements. (@russakovsky2015imagenet)

Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun Objects365: A large-scale, high-quality dataset for object detection In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, October 2019. **Abstract:** In this paper, we introduce a new large-scale object detection dataset, Objects365, which has 365 object categories over 600K training images. More than 10 million, high-quality bounding boxes are manually labeled through a three-step, carefully designed annotation pipeline. It is the largest object detection dataset (with full annotation) so far and establishes a more challenging benchmark for the community. Objects365 can serve as a better feature learning dataset for localization-sensitive tasks like object detection and semantic segmentation. The Objects365 pre-trained models significantly outperform ImageNet pre-trained models with 5.6 points gain (42 vs 36.4) based on the standard setting of 90K iterations on COCO benchmark. Even compared with much long training time like 540K iterations, our Objects365 pretrained model with 90K iterations still have 2.7 points gain (42 vs 39.3). Meanwhile, the finetuning time can be greatly reduced (up to 10 times) when reaching the same accuracy. Better generalization ability of Object365 has also been verified on CityPersons, VOC segmentation, and ADE tasks. The dataset as well as the pretrained-models have been released at www.objects365.org. (@Object365)

Zhijie Shen, Chunyu Lin, Kang Liao, Lang Nie, Zishuo Zheng, and Yao Zhao Panoformer: Panorama transformer for indoor 360 depth estimation In *European Conference on Computer Vision*, pages 195–211. Springer, 2022. **Abstract:** Existing panoramic depth estimation methods based on convolutional neural networks (CNNs) focus on removing panoramic distortions, failing to perceive panoramic structures efficiently due to the fixed receptive field in CNNs. This paper proposes the panorama transformer (named PanoFormer) to estimate the depth in panorama images, with tangent patches from spherical domain, learnable token flows, and panorama specific metrics. In particular, we divide patches on the spherical tangent domain into tokens to reduce the negative effect of panoramic distortions. Since the geometric structures are essential for depth estimation, a self-attention module is redesigned with an additional learnable token flow. In addition, considering the characteristic of the spherical domain, we present two panorama-specific metrics to comprehensively evaluate the panoramic depth estimation models’ performance. Extensive experiments demonstrate that our approach significantly outperforms the state-of-the-art (SOTA) methods. Furthermore, the proposed method can be effectively extended to solve semantic panorama segmentation, a similar pixel2pixel task. Code will be available. (@shen2022panoformer)

Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li Fixmatch: Simplifying semi-supervised learning with consistency and confidence *Advances in neural information processing systems*, 33: 596–608, 2020. **Abstract:** Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model’s performance. In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling. Our algorithm, FixMatch, first generates pseudo-labels using the model’s predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40 – just 4 labels per class. Since FixMatch bears many similarities to existing SSL methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch’s success. We make our code available at https://github.com/google-research/fixmatch. (@sohn2020fixmatch)

Yu-Chuan Su and Kristen Grauman Learning spherical convolution for fast features from 360 imagery *Advances in neural information processing systems*, 30, 2017. **Abstract:** While 360° cameras offer tremendous new possibilities in vision, graphics, and augmented reality, the spherical images they produce make core feature extraction non-trivial. Convolutional neural networks (CNNs) trained on images from perspective cameras yield “flat filters, yet 360° images cannot be projected to a single plane without significant distortion. A naive solution that repeatedly projects the viewing sphere to all tangent planes is accurate, but much too computationally intensive for real problems. We propose to learn a spherical convolutional network that translates a planar CNN to process 360° imagery directly in its equirectangular projection. Our approach learns to reproduce the flat filter outputs on 360° data, sensitive to the varying distortion effects across the viewing sphere. The key benefits are 1) efficient feature extraction for 360° images and video, and 2) the ability to leverage powerful pre-trained networks researchers have carefully honed (together with massive labeled image training sets) for perspective images. We validate our approach compared to several alternative methods in terms of both raw CNN output accuracy as well as applying a state-of-the-art “flat object detector to 360° data. Our method yields the most accurate results while saving orders of magnitude in computation versus the existing exact reprojection solution. (@SphConv)

Yu-Chuan Su and Kristen Grauman Kernel transformer networks for compact spherical convolution In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2019. **Abstract:** Ideally, 360° imagery could inherit the deep convolutional neural networks (CNNs) already trained with great success on perspective projection images. However, existing methods to transfer CNNs from perspective to spherical images introduce significant computational costs and/or degradations in accuracy. We present the Kernel Transformer Network (KTN) to efficiently transfer convolution kernels from perspective images to the equirectangular projection of 360° images. Given a source CNN for perspective images as input, the KTN produces a function parameterized by a polar angle and kernel as output. Given a novel 360° image, that function in turn can compute convolutions for arbitrary layers and kernels as would the source CNN on the corresponding tangent plane projections. Distinct from all existing methods, KTNs allow model transfer: the same model can be applied to different source CNNs with the same base architecture. This enables application to multiple recognition tasks without re-training the KTN. Validating our approach with multiple source CNNs and datasets, we show that KTNs improve the state of the art for spherical convolution. KTNs successfully preserve the source CNN’s accuracy, while offering transferability, scalability to typical image resolutions, and, in many cases, a substantially lower memory footprint. (@Su_2019_CVPR)

Cheng Sun, Chi-Wei Hsiao, Ning-Hsu Wang, Min Sun, and Hwann-Tzong Chen Indoor panorama planar 3d reconstruction via divide and conquer In *CVPR*, 2021. **Abstract:** Indoor panorama typically consists of human-made structures parallel or perpendicular to gravity. We leverage this phenomenon to approximate the scene in a 360-degree image with (H)orizontal-planes and (V)ertical-planes. To this end, we propose an effective divide-and-conquer strategy that divides pixels based on their plane orientation estimation; then, the succeeding instance segmentation module conquers the task of planes clustering more easily in each plane orientation group. Besides, parameters of V-planes depend on camera yaw rotation, but translation-invariant CNNs are less aware of the yaw change. We thus propose a yaw-invariant V-planar reparameterization for CNNs to learn. We create a benchmark for indoor panorama planar reconstruction by extending existing 360 depth datasets with ground truth H&V-planes (referred to as "PanoH&V" dataset) and adopt state-of-the-art planar reconstruction methods to predict H&V-planes as our baselines. Our method outperforms the baselines by a large margin on the proposed dataset. (@PanoPlane)

Cheng Sun, Min Sun, and Hwann-Tzong Chen Hohonet: 360 indoor holistic understanding with latent horizontal features In *CVPR*, 2021. **Abstract:** We present HoHoNet, a versatile and efficient framework for holistic understanding of an indoor 360-degree panorama using a Latent Horizontal Feature (LHFeat). The compact LHFeat flattens the features along the vertical direction and has shown success in modeling per-column modality for room layout reconstruction. HoHoNet advances in two important aspects. First, the deep architecture is redesigned to run faster with improved accuracy. Second, we propose a novel horizon-to-dense module, which relaxes the per-column output shape constraint, allowing per-pixel dense prediction from LHFeat. HoHoNet is fast: It runs at 52 FPS and 110 FPS with ResNet-50 and ResNet-34 backbones respectively, for modeling dense modalities from a high-resolution 512 × 1024 panorama. HoHoNet is also accurate. On the tasks of layout estimation and semantic segmentation, HoHoNet achieves results on par with current state-of-the-art. On dense depth estimation, HoHoNet outperforms all the prior arts by a large margin. Code is available at https://github.com/sunset1995/HoHoNet. (@Hohonet)

Fu-En Wang, Hou-Ning Hu, Hsien-Tzu Cheng, Juan-Ting Lin, Shang-Ta Yang, Meng-Li Shih, Hung-Kuo Chu, and Min Sun Self-supervised learning of depth and camera motion from 360° videos In *Asian Conference on Computer Vision*, 2018. URL <https://api.semanticscholar.org/CorpusID:53290169>. **Abstract:** As 360° cameras become prevalent in many autonomous systems (e.g., self-driving cars and drones), efficient 360° perception becomes more and more important. We propose a novel self-supervised learning approach for predicting the omnidirectional depth and camera motion from a 360° video. In particular, starting from the SfMLearner, which is designed for cameras with normal field-of-view, we introduce three key features to process 360° images efficiently. Firstly, we convert each image from equirectangular projection to cubic projection in order to avoid image distortion. In each network layer, we use Cube Padding (CP), which pads intermediate features from adjacent faces, to avoid image boundaries. Secondly, we propose a novel "spherical" photometric consistency constraint on the whole viewing sphere. In this way, no pixel will be projected outside the image boundary which typically happens in images with normal field-of-view. Finally, rather than naively estimating six independent camera motions (i.e., naively applying SfM-Learner to each face on a cube), we propose a novel camera pose consistency loss to ensure the estimated camera motions reaching consensus. To train and evaluate our approach, we collect a new PanoSUNCG dataset containing a large amount of 360° videos with groundtruth depth and camera motion. Our approach achieves state-of-the-art depth prediction and camera motion estimation on PanoSUNCG with faster inference speed comparing to equirectangular. In real-world indoor videos, our approach can also achieve qualitatively reasonable depth prediction by acquiring model pre-trained on PanoSUNCG. (@360-SelfNet)

Fu-En Wang, Yu-Hsuan Yeh, Min Sun, Wei-Chen Chiu, and Yi-Hsuan Tsai Bifuse: Monocular 360 depth estimation via bi-projection fusion In *The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2020. **Abstract:** Depth estimation from a monocular 360 image is an emerging problem that gains popularity due to the availability of consumer-level 360 cameras and the complete surrounding sensing capability. While the standard of 360 imaging is under rapid development, we propose to predict the depth map of a monocular 360 image by mimicking both peripheral and foveal vision of the human eye. To this end, we adopt a two-branch neural network leveraging two common projections: equirectangular and cubemap projections. In particular, equirectangular projection incorporates a complete field-of-view but introduces distortion, whereas cubemap projection avoids distortion but introduces discontinuity at the boundary of the cube. Thus we propose a bi-projection fusion scheme along with learnable masks to balance the feature map from the two projections. Moreover, for the cubemap projection, we propose a spherical padding procedure which mitigates discontinuity at the boundary of each face. We apply our method to four panorama datasets and show favorable results against the existing state-of-the-art methods. (@BiFuse20)

Fu-En Wang, Yu-Hsuan Yeh, Yi-Hsuan Tsai, Wei-Chen Chiu, and Min Sun Bifuse++: Self-supervised and efficient bi-projection fusion for 360° depth estimation *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 45 (5): 5448–5460, 2023. . **Abstract:** Due to the rise of spherical cameras, monocular 360 ° depth estimation becomes an important technique for many applications (e.g., autonomous systems). Thus, state-of-the-art frameworks for monocular 360 ° depth estimation such as bi-projection fusion in BiFuse are proposed. To train such a framework, a large number of panoramas along with the corresponding depth ground truths captured by laser sensors are required, which highly increases the cost of data collection. Moreover, since such a data collection procedure is time-consuming, the scalability of extending these methods to different scenes becomes a challenge. To this end, self-training a network for monocular depth estimation from 360 ° videos is one way to alleviate this issue. However, there are no existing frameworks that incorporate bi-projection fusion into the self-training scheme, which highly limits the self-supervised performance since bi-projection fusion can leverage information from different projection types. In this paper, we propose BiFuse++ to explore the combination of bi-projection fusion and the self-training scenario. To be specific, we propose a new fusion module and Contrast-Aware Photometric Loss to improve the performance of BiFuse and increase the stability of self-training on real-world videos. We conduct both supervised and self-supervised experiments on benchmark datasets and achieve state-of-the-art performance. (@BiFuse++)

Ning-Hsu Wang, Bolivar Solarte, Yi-Hsuan Tsai, Wei-Chen Chiu, and Min Sun 360sd-net: 360 stereo depth estimation with learnable cost volume In *2020 IEEE International Conference on Robotics and Automation (ICRA)*, pages 582–588. IEEE, 2020. **Abstract:** Recently, end-to-end trainable deep neural networks have significantly improved stereo depth estimation for perspective images. However, 360° images captured under equirectangular projection cannot benefit from directly adopting existing methods due to distortion introduced (i.e., lines in 3D are not projected onto lines in 2D). To tackle this issue, we present a novel architecture specifically designed for spherical disparity using the setting of top-bottom 360° camera pairs. Moreover, we propose to mitigate the distortion issue by (1) an additional input branch capturing the position and relation of each pixel in the spherical coordinate, and (2) a cost volume built upon a learnable shifting filter. Due to the lack of 360° stereo data, we collect two 360° stereo datasets from Matterport3D and Stanford3D for training and evaluation. Extensive experiments and ablation study are provided to validate our method against existing algorithms. Finally, we show promising results on real-world environments capturing images with two consumer-level cameras. Our project page is at https://albert100121.github.io/360SD-Net-Project-Page. (@wang2020360sd)

Ning-Hsu Wang, Ren Wang, Yu-Lun Liu, Yu-Hao Huang, Yu-Lin Chang, Chia-Ping Chen, and Kevin Jou Bridging unsupervised and supervised depth from focus via all-in-focus supervision In *Proceedings of the IEEE/CVF international conference on computer vision*, pages 12621–12631, 2021. **Abstract:** Depth estimation is a long-lasting yet important task in computer vision. Most of the previous works try to estimate depth from input images and assume images are all-in-focus (AiF), which is less common in real-world applications. On the other hand, a few works take defocus blur into account and consider it as another cue for depth estimation. In this paper, we propose a method to estimate not only a depth map but an AiF image from a set of images with different focus positions (known as a focal stack). We design a shared architecture to exploit the relationship between depth and AiF estimation. As a result, the proposed method can be trained either supervisedly with ground truth depth, or unsupervisedly with AiF images as supervisory signals. We show in various experiments that our method outperforms the state-of-the-art methods both quantitatively and qualitatively, and also has higher efficiency in inference time. (@wang2021bridging)

Qiang Wang, Shizhen Zheng, Qingsong Yan, Fei Deng, Kaiyong Zhao, and Xiaowen Chu Irs: A large naturalistic indoor robotics stereo dataset to train deep models for disparity and surface normal estimation In *2021 IEEE International Conference on Multimedia and Expo (ICME)*, pages 1–6. IEEE, 2021. **Abstract:** Indoor robotics applications heavily rely on scene understanding and reconstruction. Compared to monocular vision, stereo vision methods are more promising to produce accurate geometrical information, such as surface normal and depth/disparity. Besides, deep learning models have shown their superior performance in stereo vision tasks. However, existing stereo datasets rarely contain high-quality surface normal and disparity ground truth, hardly satisfying the demand of training a prospective deep model. To this end, we introduce a large-scale indoor robotics stereo (IRS) dataset with over 100K stereo images and high-quality surface normal and disparity maps. Leveraging the advanced techniques of our customized rendering engine, the dataset is considerably close to the real-world scenes. Besides, we present DTN-Net, a two-stage deep model for surface normal estimation. Extensive experiments show the advantages and effectiveness of IRS in training deep models for disparity estimation, and DTN-Net provides state-of-the-art results for normal estimation compared to existing methods. (@wang2021irs)

Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer Tartanair: A dataset to push the limits of visual slam In *2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*, pages 4909–4916. IEEE, 2020. **Abstract:** We present a challenging dataset, the TartanAir, for robot navigation tasks and more. The data is collected in photo-realistic simulation environments with the presence of moving objects, changing light and various weather conditions. By collecting data in simulations, we are able to obtain multi-modal sensor data and precise ground truth labels such as the stereo RGB image, depth image, segmentation, optical flow, camera poses, and LiDAR point cloud. We set up large numbers of environments with various styles and scenes, covering challenging viewpoints and diverse motion patterns that are difficult to achieve by using physical data collection platforms. In order to enable data collection at such a large scale, we develop an automatic pipeline, including mapping, trajectory sampling, data processing, and data verification. We evaluate the impact of various factors on visual SLAM algorithms using our data. The results of state-of-the-art algorithms reveal that the visual SLAM problem is far from solved. Methods that show good performance on established datasets such as KITTI do not perform well in more difficult scenarios. Although we use the simulation, our goal is to push the limits of Visual SLAM algorithms in the real world by providing a challenging benchmark for testing new methods, while also using a large diverse training data for learning-based methods. Our dataset is available at http://theairlab.org/tartanair-dataset. (@wang2020tartanair)

Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim Google landmarks dataset v2-a large-scale benchmark for instance-level recognition and retrieval In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 2575–2584, 2020. **Abstract:** While image retrieval and instance recognition techniques are progressing rapidly, there is a need for challenging datasets to accurately measure their performance – while posing novel challenges that are relevant for practical applications. We introduce the Google Landmarks Dataset v2 (GLDv2), a new benchmark for large-scale, fine-grained instance recognition and image retrieval in the domain of human-made and natural landmarks. GLDv2 is the largest such dataset to date by a large margin, including over 5M images and 200k distinct instance labels. Its test set consists of 118k images with ground truth annotations for both the retrieval and recognition tasks. The ground truth construction involved over 800 hours of human annotator work. Our new dataset has several challenging properties inspired by real-world applications that previous datasets did not consider: An extremely long-tailed class distribution, a large fraction of out-of-domain test photos and large intra-class variability. The dataset is sourced from Wikimedia Commons, the world’s largest crowdsourced collection of landmark photos. We provide baseline results for both recognition and retrieval tasks based on state-of-the-art methods as well as competitive results from a public challenge. We further demonstrate the suitability of the dataset for transfer learning by showing that image embeddings trained on it achieve competitive retrieval performance on independent datasets. The dataset images, ground-truth and metric scoring code are available at https://github.com/cvdfoundation/google-landmark. (@weyand2020google)

Ke Xian, Chunhua Shen, ZHIGUO CAO, Hao Lu, Yang Xiao, Ruibo Li, and Zhenbo Luo Monocular relative depth perception with web stereo data supervision *2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 311–320, 2018. URL <https://api.semanticscholar.org/CorpusID:52860134>. **Abstract:** In this paper we study the problem of monocular relative depth perception in the wild. We introduce a simple yet effective method to automatically generate dense relative depth annotations from web stereo images, and propose a new dataset that consists of diverse images as well as corresponding dense relative depth maps. Further, an improved ranking loss is introduced to deal with imbalanced ordinal relations, enforcing the network to focus on a set of hard pairs. Experimental results demonstrate that our proposed approach not only achieves state-of-the-art accuracy of relative depth perception in the wild, but also benefits other dense per-pixel prediction tasks, e.g., metric depth estimation and semantic segmentation. (@Xian2018MonocularRD)

Ke Xian, Jianming Zhang, Oliver Wang, Long Mai, Zhe Lin, and Zhiguo Cao Structure-guided ranking loss for single image depth prediction In *The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2020. **Abstract:** Single image depth prediction is a challenging task due to its ill-posed nature and challenges with capturing ground truth for supervision. Large-scale disparity data generated from stereo photos and 3D videos is a promising source of supervision, however, such disparity data can only approximate the inverse ground truth depth up to an affine transformation. To more effectively learn from such pseudo-depth data, we propose to use a simple pair-wise ranking loss with a novel sampling strategy. Instead of randomly sampling point pairs, we guide the sampling to better characterize structure of important regions based on the low-level edge maps and high-level object instance masks. We show that the pair-wise ranking loss, combined with our structure-guided sampling strategies, can significantly improve the quality of depth map prediction. In addition, we introduce a new relative depth dataset of about 21K diverse high-resolution web stereo photos to enhance the generalization ability of our model. In experiments, we conduct cross-dataset evaluation on six benchmark datasets and show that our method consistently improves over the baselines, leading to superior quantitative and qualitative results. (@HRWSI)

Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le Self-training with noisy student improves imagenet classification In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 10687–10698, 2020. **Abstract:** We present a simple self-training method that achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. To achieve this result, we first train an EfficientNet model on labeled ImageNet images and use it as a teacher to generate pseudo labels on 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the generation of the pseudo labels, the teacher is not noised so that the pseudo labels are as accurate as possible. However, during the learning of the student, we inject noise such as dropout, stochastic depth and data augmentation via RandAugment to the student so that the student generalizes better than the teacher. (@xie2020self)

Zhiqiang Yan, Xiang Li, Kun Wang, Zhenyu Zhang, Jun Li, and Jian Yang Multi-modal masked pre-training for monocular panoramic depth completion In *European Conference on Computer Vision*, pages 378–395. Springer, 2022. **Abstract:** In this paper, we formulate a potentially valuable panoramic depth completion (PDC) task as panoramic 3D cameras often produce 360◦depth with missing data in complex scenes. Its goal is to recover dense panoramic depths from raw sparse ones and panoramic RGB im- ages. To deal with the PDC task, we train a deep network that takes both depth and image as inputs for the dense panoramic depth recov- ery. However, it needs to face a challenging optimization problem of the network parameters due to its non-convex objective function. To address this problem, we propose a simple yet effective approach termed M3PT: multi-modal masked pre-training. Specifically, during pre-training, we si- multaneously cover up patches of the panoramic RGB image and sparse depth by shared random mask, then reconstruct the sparse depth in the masked regions. To our best knowledge, it is the first time that we show the effectiveness of masked pre-training in a multi-modal vision task, in- stead of the single-modal task resolved by masked autoencoders (MAE). Different from MAE where fine-tuning completely discards the decoder part of pre-training, there is no architectural difference between the pre- training and fine-tuning stages in our M3PT as they only differ in the prediction density, which potentially makes the transfer learning more convenient and effective. Extensive experiments verify the effectivenessarXiv:2203.09855v5 \[cs.CV\] 12 Jul 20222 Yan et al. of M3PT on three panoramic datasets. Notably, we improve the state- of-the-art baselines by averagely 29.2% in RMSE, 51.7% in MRE, 49.7% in MAE, and 37.5% in RMSElog on three benchmark datasets. (@yan2022multi)

Zhiqiang Yan, Xiang Li, Kun Wang, Shuo Chen, Jun Li, and Jian Yang Distortion and uncertainty aware loss for panoramic depth completion In *International Conference on Machine Learning*, pages 39099–39109. PMLR, 2023. (@yan2023distortion)

Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao Depth anything: Unleashing the power of large-scale unlabeled data In *CVPR*, 2024. **Abstract:** This work presents Depth Anything, a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (\~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability. Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet. Our models are released at https://github.com/LiheYoung/Depth-Anything. (@depthanything)

Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan Blendedmvs: A large-scale dataset for generalized multi-view stereo networks In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 1790–1799, 2020. **Abstract:** While deep learning has recently achieved great success on multi-view stereo (MVS), limited training data makes the trained model hard to be generalized to unseen scenarios. Compared with other computer vision tasks, it is rather difficult to collect a large-scale MVS dataset as it requires expensive active scanners and labor-intensive process to obtain ground truth 3D structures. In this paper, we introduce BlendedMVS, a novel large-scale dataset, to provide sufficient training ground truth for learning-based MVS. To create the dataset, we apply a 3D reconstruction pipeline to recover high-quality textured meshes from images of well-selected scenes. Then, we render these mesh models to color images and depth maps. To introduce the ambient lighting information during training, the rendered color images are further blended with the input images to generate the training input. Our dataset contains over 17k high-resolution images covering a variety of scenes, including cities, architectures, sculptures and small objects. Extensive experiments demonstrate that BlendedMVS endows the trained model with significantly better generalization ability compared with other MVS datasets. The dataset and pretrained models are available at https://github.com/YoYo000/BlendedMVS. (@yao2020blendedmvs)

Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen Metric3d: Towards zero-shot metric 3d prediction from a single image In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 9043–9053, 2023. **Abstract:** Reconstructing accurate 3D scenes from images is a long-standing vision task. Due to the ill-posedness of the single-image reconstruction problem, most well-established methods are built upon multi-view geometry. State-of-the-art (SOTA) monocular metric depth estimation methods can only handle a single camera model and are unable to perform mixed-data training due to metric ambiguity. Meanwhile, SOTA monocular methods trained on large mixed datasets achieve zero-shot generalization by learning affine-invariant depths, which cannot recover real-world metrics. In this work, we show that the key to a zero-shot single-view metric depth model lies in the combination of large-scale data training and resolving the metric ambiguity from various camera models. We propose a canonical camera space transformation module, which explicitly addresses the ambiguity problems and can be effortlessly plugged into existing monocular models. Equipped with our module, monocular models can be stably trained over 8 millions of images with thousands of camera models, resulting in zero-shot generalization to in-the-wild images with unseen camera settings. Experiments demonstrate SOTA performance of our method on 7 zero-shot benchmarks. Notably, our method won the championship in the 2nd Monocular Depth Estimation Challenge. Our method enables the accurate recovery of metric 3D structures on randomly collected internet images, paving the way for plausible single-image metrology. The potential benefits extend to downstream tasks, which can be significantly improved by simply plugging in our model. For example, our model relieves the scale drift issues of monocular-SLAM (Fig. 1), leading to high-quality metric scale dense mapping. The code is available at https://github.com/YvanYin/Metric3D. (@yin2023metric3d)

Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop *arXiv preprint arXiv:1506.03365*, 2015. **Abstract:** While there has been remarkable progress in the performance of visual recognition algorithms, the state-of-the-art models tend to be exceptionally data-hungry. Large labeled training datasets, expensive and tedious to produce, are required to optimize millions of parameters in deep network models. Lagging behind the growth in model capacity, the available datasets are quickly becoming outdated in terms of size and density. To circumvent this bottleneck, we propose to amplify human effort through a partially automated labeling scheme, leveraging deep learning with humans in the loop. Starting from a large set of candidate images for each category, we iteratively sample a subset, ask people to label them, classify the others with a trained model, split the set into positives, negatives, and unlabeled based on the classification confidence, and then iterate with the unlabeled set. To assess the effectiveness of this cascading procedure and enable further progress in visual recognition research, we construct a new image dataset, LSUN. It contains around one million labeled images for each of 10 scene categories and 20 object categories. We experiment with training popular convolutional networks and find that they achieve substantial performance gains when trained on this dataset. (@yu15lsun)

Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell Bdd100k: A diverse driving dataset for heterogeneous multitask learning In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 2636–2645, 2020. **Abstract:** Datasets drive vision progress, yet existing driving datasets are impoverished in terms of visual content and supported tasks to study multitask learning for autonomous driving. Researchers are usually constrained to study a small set of problems on one dataset, while real-world computer vision applications require performing tasks of various complexities. We construct BDD100K, the largest driving video dataset with 100K videos and 10 tasks to evaluate the exciting progress of image recognition algorithms on autonomous driving. The dataset possesses geographic, environmental, and weather diversity, which is useful for training models that are less likely to be surprised by new conditions. Based on this diverse dataset, we build a benchmark for heterogeneous multitask learning and study how to solve the tasks together. Our experiments show that special training strategies are needed for existing models to perform such heterogeneous tasks. BDD100K opens the door for future studies in this important venue. (@yu2020bdd100k)

Ilwi Yun, Chanyong Shin, Hyunku Lee, Hyuk-Jae Lee, and Chae Eun Rhee Egformer: Equirectangular geometry-biased transformer for 360 depth estimation In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, pages 6101–6112, October 2023. **Abstract:** Estimating the depths of equirectangular (i.e., 360°) images (EIs) is challenging given the distorted 180° × 360° field-of-view, which is hard to be addressed via convolutional neural network (CNN). Although a transformer with global attention achieves significant improvements over CNN for EI depth estimation task, it is computationally inefficient, which raises the need for transformer with local attention. However, to apply local attention successfully for EIs, a specific strategy, which addresses distorted equirectangular geometry and limited receptive field simultaneously, is required. Prior works have only cared either of them, resulting in unsatisfactory depths occasionally. In this paper, we propose an equirectangular geometry-biased transformer termed EGformer. While limiting the computational cost and the number of network parameters, EGformer enables the extraction of the equirectangular geometry-aware local attention with a large receptive field. To achieve this, we actively utilize the equirectangular geometry as the bias for the local attention instead of struggling to reduce the distortion of EIs. As compared to the most recent EI depth estimation studies, the proposed approach yields the best depth outcomes overall with the lowest computational cost and the fewest parameters, demonstrating the effectiveness of the proposed methods. (@EGFormer)

Jia Zheng, Junfei Zhang, Jing Li, Rui Tang, Shenghua Gao, and Zihan Zhou Structured3d: A large photo-realistic dataset for structured 3d modeling In *Proceedings of The European Conference on Computer Vision (ECCV)*, 2020. **Abstract:** Recently, there has been growing interest in developing learning-based methods to detect and utilize salient semi-global or global structures, such as junctions, lines, planes, cuboids, smooth surfaces, and all types of symmetries, for 3D scene modeling and understanding. How- ever, the ground truth annotations are often obtained via human labor, which is particularly challenging and inecient for such tasks due to the large number of 3D structure instances ( e.g., line segments) and other factors such as viewpoints and occlusions. In this paper, we present a new synthetic dataset, Structured3D, with the aim of providing large- scale photo-realistic images with rich 3D structure annotations for a wide spectrum of structured 3D modeling tasks. We take advantage of the availability of professional interior designs and automatically extract 3D structures from them. We generate high-quality images with an industry- leading rendering engine. We use our synthetic dataset in combination with real images to train deep networks for room layout estimation and demonstrate improved performance on benchmark datasets. (@Structured3D)

Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba Places: A 10 million image database for scene recognition *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2017. **Abstract:** The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification performance at tasks such as visual object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Using the state-of-the-art Convolutional Neural Networks (CNNs), we provide scene classification CNNs (Places-CNNs) as baselines, that significantly outperform the previous approaches. Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classification. With its high-coverage and high-diversity of exemplars, the Places Database along with the Places-CNNs offer a novel resource to guide future progress on scene recognition problems. (@zhou2017places)

Tinghui Zhou, Matthew Brown, Noah Snavely, and David G. Lowe Unsupervised learning of depth and ego-motion from video In *2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 6612–6619, 2017. . **Abstract:** We present an unsupervised learning framework for the task of monocular depth and camera motion estimation from unstructured video sequences. In common with recent work \[10, 14, 16\], we use an end-to-end learning approach with view synthesis as the supervisory signal. In contrast to the previous work, our method is completely unsupervised, requiring only monocular video sequences for training. Our method uses single-view depth and multiview pose networks, with a loss based on warping nearby views to the target using the computed depth and pose. The networks are thus coupled by the loss during training, but can be applied independently at test time. Empirical evaluation on the KITTI dataset demonstrates the effectiveness of our approach: 1) monocular depth performs comparably with supervised methods that use either ground-truth pose or depth for training, and 2) pose estimation performs favorably compared to established SLAM systems under comparable input settings. (@8100183)

Chuanqing Zhuang, Zhengda Lu, Yiqun Wang, Jun Xiao, and Ying Wang Acdnet: Adaptively combined dilated convolution for monocular panorama depth estimation In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 36, pages 3653–3661, 2022. **Abstract:** Depth estimation is a crucial step for 3D reconstruction with panorama images in recent years. Panorama images maintain the complete spatial information but introduce distortion with equirectangular projection. In this paper, we propose an ACDNet based on the adaptively combined dilated convolution to predict the dense depth map for a monocular panoramic image. Specifically, we combine the convolution kernels with different dilations to extend the receptive field in the equirectangular projection. Meanwhile, we introduce an adaptive channel-wise fusion module to summarize the feature maps and get diverse attention areas in the receptive field along the channels. Due to the utilization of channel-wise attention in constructing the adaptive channel-wise fusion module, the network can capture and leverage the cross-channel contextual information efficiently. Finally, we conduct depth estimation experiments on three datasets (both virtual and real-world) and the experimental results demonstrate that our proposed ACDNet substantially outperforms the current state-of-the-art (SOTA) methods. Our codes and model parameters are accessed in https://github.com/zcq15/ACDNet. (@ACDNet)

Nikolaos Zioulis, Antonis Karakottas, Dimitrios Zarpalas, and Petros Daras Omnidepth: Dense depth estimation for indoors spherical panoramas In *Proceedings of the European Conference on Computer Vision (ECCV)*, pages 448–465, 2018. **Abstract:** Recent work on depth estimation up to now has only focused on projective images ignoring 360 content which is now increasingly and more easily produced. We show that monocular depth estimation models trained on traditional images produce sub-optimal results on omnidirectional images, showcasing the need for training directly on 360 datasets, which however, are hard to acquire. In this work, we circumvent the challenges associated with acquiring high quality 360 datasets with ground truth depth annotations, by re-using recently released large scale 3D datasets and re-purposing them to 360 via rendering. This dataset, which is considerably larger than similar projective datasets, is publicly offered to the community to enable future research in this direction. We use this dataset to learn in an end-to-end fashion the task of depth estimation from 360 images. We show promising results in our synthesized data as well as in unseen realistic images. (@OmniDepth)

Nikolaos Zioulis, Antonis Karakottas, Dimitris Zarpalas, Federic Alvarez, and Petros Daras Spherical view synthesis for self-supervised \\(360^o\\) depth estimation In *International Conference on 3D Vision (3DV)*, September 2019. **Abstract:** Learning based approaches for depth perception are limited by the availability of clean training data. This has led to the utilization of view synthesis as an indirect objective for learning depth estimation using efficient data acquisition procedures. Nonetheless, most research focuses on pinhole based monocular vision, with scarce works presenting results for omnidirectional input. In this work, we explore spherical view synthesis for learning monocular 360 depth in a self-supervised manner and demonstrate its feasibility. Under a purely geometrically derived formulation we present results for horizontal and vertical baselines, as well as for the trinocular case. Further, we show how to better exploit the expressiveness of traditional CNNs when applied to the equirectangular domain in an efficient manner. Finally, given the availability of ground truth depth data, our work is uniquely positioned to compare view synthesis against direct supervision in a consistent and fair manner. The results indicate that alternative research directions might be better suited to enable higher quality depth perception. Our data, models and code are publicly available at https://vcl3d.github.io/SphericalViewSynthesis/. (@Spherecoord)

Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin Dogus Cubuk, and Quoc V. Le Rethinking pre-training and self-training *ArXiv*, abs/2006.06882, 2020. URL <https://api.semanticscholar.org/CorpusID:219635973>. **Abstract:** Pre-training is a dominant paradigm in computer vision. For example, supervised ImageNet pre-training is commonly used to initialize the backbones of object detection and segmentation models. He et al., however, show a surprising result that ImageNet pre-training has limited impact on COCO object detection. Here we investigate self-training as another method to utilize additional data on the same setup and contrast it against ImageNet pre-training. Our study reveals the generality and flexibility of self-training with three additional insights: 1) stronger data augmentation and more labeled data further diminish the value of pre-training, 2) unlike pre-training, self-training is always helpful when using stronger data augmentation, in both low-data and high-data regimes, and 3) in the case that pre-training is helpful, self-training improves upon pre-training. For example, on the COCO object detection dataset, pre-training benefits when we use one fifth of the labeled data, and hurts accuracy when we use all labeled data. Self-training, on the other hand, shows positive improvements from +1.3 to +3.4AP across all dataset sizes. In other words, self-training works well exactly on the same setup that pre-training does not work (using ImageNet to help COCO). On the PASCAL segmentation dataset, which is a much smaller dataset than COCO, though pre-training does help significantly, self-training improves upon the pre-trained model. On COCO object detection, we achieve 54.3AP, an improvement of +1.5AP over the strongest SpineNet model. On PASCAL segmentation, we achieve 90.5 mIOU, an improvement of +1.5% mIOU over the previous state-of-the-art result by DeepLabv3+. (@Zoph2020RethinkingPA)

</div>

# Appendix / Supplemental Material

## Experimental Setup

#### Implementation details. [sec:implementation_details]

Our work is divided into two stages: (1) offline mask generation and (2) online joint training. (1) In the first stage, we use Grounded-Segment-Anything `\cite{groundedSAM}`{=latex}, which combines state-of-the-art detection and segmentation models. We set the `BOX_THRESHOLD` and `TEXT_THRESHOLD` to 0.3 and 0.25, respectively, following the recommendations of the official code. We use “sky” and “watermark” as text prompts. All pixels with these labels are set to False to form our valid mask for the second stage of training. (2) In the second stage, each batch consists of an equal mix of labeled and unlabeled data. We follow the backbone model’s official settings for batch size, learning rate, optimizer, augmentation, and other hyperparameters, changing only the loss function to affine-invariant loss. Unlike Depth Anything, which sets invalid sky regions to zero disparity, we ignore these invalid pixels during loss calculation, consistent with ground truth training settings. We average the loss for ground truth and pseudo ground truth during updates. All our experiments are conducted on a single RTX 4090, both offline and online. However, if future 360-degree state-of-the-art methods or perspective foundation models require higher VRAM usage, the computational resource requirements may increase.

#### Metrics. [metric]

In line with previous cross-dataset works, all evaluation metrics are presented in percentage terms. The primary metric is Absolute Mean Relative Error (AbsRel), calculated as: \\(\frac{1}{M}\sum^{M}_{i=1}{|a_i - d_i| / d_i}\\), where \\(M\\) is the total number of pixels, \\(a_i\\) is the predicted depth, and \\(d_i\\) is the ground truth depth. The second metric, \\(\delta_j\\) accuracy, measures the proportion of pixels where \\(max(ai/di
, di/ai)\\) is within \\(1.25^j\\). During evaluations, we follow `\cite{jiang2021unifuse, BiFuse20, BiFuse++}`{=latex} to ignore areas where ground truth depth values are larger than 10 or equal to 0. Given the ambiguous scale of self-training results, we apply median alignment after converting disparity output to depth before evaluation, as per the method used in `\cite{8100183}`{=latex}: \\[d' = d \cdot \frac{\text{median}(\hat{d})}{\text{median}(d)},\\] where \\(d\\) is the predicted depth from inverse disparity and \\(\hat{d}\\) is the ground truth depth. This ensures a fair comparison by aligning the median depth values of predictions and ground truths.

## More Qualitative

We demonstrate additional zero-shot qualitative results in Figure <a href="#fig:more_zeroshot" data-reference-type="ref" data-reference="fig:more_zeroshot">9</a> and in-the-wild results in Figure <a href="#fig:rebuttal_in_the_wild" data-reference-type="ref" data-reference="fig:rebuttal_in_the_wild">12</a>. In-domain results on the Matterport3D test sets are showcased in Figure <a href="#fig:matterport_unifuse" data-reference-type="ref" data-reference="fig:matterport_unifuse">10</a> and Figure <a href="#fig:matterport_bifuse" data-reference-type="ref" data-reference="fig:matterport_bifuse">11</a>.

<figure id="fig:more_zeroshot">
<img src="./figures/zero-shot_more_quali.png"" style="width:100.0%" />
<figcaption><strong>More qualitative tested on Stanford2D3D with zero-shot setting.</strong></figcaption>
</figure>

<figure id="fig:matterport_unifuse">
<img src="./figures/unifuse_mp3d.png"" style="width:100.0%" />
<figcaption><strong>In-domain qualitative with UniFuse.</strong></figcaption>
</figure>

<figure id="fig:matterport_bifuse">
<img src="./figures/bifusev2_mp3d.png"" style="width:100.0%" />
<figcaption><strong>In-domain qualitative with BiFuse++.</strong></figcaption>
</figure>

## Dataset Statistic

As described in Sec.<a href="#sec:dataset statistic" data-reference-type="ref" data-reference="sec:dataset statistic">3.1.0.1</a> of the main paper, there is a significant difference in the number of images between the perspective and equirectangular datasets. Detailed statistics of the datasets are listed in Table <a href="#Dataset statistic detailed" data-reference-type="ref" data-reference="Dataset statistic detailed">6</a>.

## Ground Truth and Pseudo Label Ratio Ablation

Unlike many previous knowledge distillation approaches that use a higher proportion of pseudo labels during model training, we opt for an equal ratio of ground truth to pseudo labels. Through an ablation study exploring the relationship between this data ratio and model performance, we observe a robust improvement starting from \\(1:1\\) in Table <a href="#tab:gt_pseudo_ratio" data-reference-type="ref" data-reference="tab:gt_pseudo_ratio">7</a>.

## Perspective Camera Projection Ablation

There are various perspective camera projections for panoramic imagery, with cube and tangent image projections being the most common, both widely used in previous works. In Table <a href="#tab:cube_tangent" data-reference-type="ref" data-reference="tab:cube_tangent">8</a>, we compare these two projections and observe similar improvements when applying our proposed training pipeline, demonstrating the effectiveness of our method. For robust knowledge distillation in relative depth estimation, we select the cube projection due to its wider field of view coverage.

<div id="Dataset statistic detailed" markdown="1">

<table>
<caption><strong>360 monocular depth estimation lacks a large amount of training data.</strong> This table lists datasets used in 360-degree monocular depth estimation alongside perspective depth datasets from the Depth Anything methodology. The volume of training data for 360-degree imagery (<em>right column</em>) is significantly smaller than that for perspective imagery (<em>left column</em>) by about 200 times. This highlights the need for using perspective distillation techniques to enhance the limited data available for 360-degree depth estimation. Ground truth (GT) labels are noted where applicable, showing the available resources for training in these domains.</caption>
<tbody>
<tr>
<td colspan="4" style="text-align: center;">Perspective</td>
<td colspan="4" style="text-align: center;">Equirectangular</td>
</tr>
<tr>
<td style="text-align: left;">Dataset</td>
<td style="text-align: center;">Venue</td>
<td style="text-align: center;"># of images</td>
<td style="text-align: center;">GT labels</td>
<td style="text-align: left;">Dataset</td>
<td style="text-align: center;">Venue</td>
<td style="text-align: center;"># of images</td>
<td style="text-align: center;">GT labels</td>
</tr>
<tr>
<td style="text-align: left;">MegaDepth <span class="citation" data-cites="MegaDepth"></span></td>
<td style="text-align: center;">CVPR 2018</td>
<td style="text-align: center;">128K</td>
<td style="text-align: center;"></td>
<td style="text-align: left;">Stanford2D3D <span class="citation" data-cites="Stanford2D3D"></span></td>
<td style="text-align: center;">arXiv 2017</td>
<td style="text-align: center;">1.4K</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">TartanAir <span class="citation" data-cites="wang2020tartanair"></span></td>
<td style="text-align: center;">IROS 2020</td>
<td style="text-align: center;">306K</td>
<td style="text-align: center;"></td>
<td style="text-align: left;">Matterport3D <span class="citation" data-cites="Matterport3D"></span></td>
<td style="text-align: center;">3DV 2017</td>
<td style="text-align: center;">10.8K</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">DIML <span class="citation" data-cites="DIML"></span></td>
<td style="text-align: center;">arXiv 2021</td>
<td style="text-align: center;">927K</td>
<td style="text-align: center;"></td>
<td style="text-align: left;">Structured3D <span class="citation" data-cites="Structured3D"></span></td>
<td style="text-align: center;">ECCV 2020</td>
<td style="text-align: center;">21.8K</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">BlendedMVS <span class="citation" data-cites="yao2020blendedmvs"></span></td>
<td style="text-align: center;">CVRP 2020</td>
<td style="text-align: center;">115 K</td>
<td style="text-align: center;"></td>
<td style="text-align: left;">SpatialAudioGen <span class="citation" data-cites="spatialaudiogen"></span></td>
<td style="text-align: center;">NeurIPS 2018</td>
<td style="text-align: center;">344K</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">HRWSI <span class="citation" data-cites="HRWSI"></span></td>
<td style="text-align: center;">CVPR 2020</td>
<td style="text-align: center;">20K</td>
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">IRS <span class="citation" data-cites="wang2021irs"></span></td>
<td style="text-align: center;">ICME 2021</td>
<td style="text-align: center;">103K</td>
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ImageNet-21K <span class="citation" data-cites="russakovsky2015imagenet"></span></td>
<td style="text-align: center;">IJCV 2015</td>
<td style="text-align: center;">13.1M</td>
<td style="text-align: center;">-</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">BDD100K <span class="citation" data-cites="yu2020bdd100k"></span></td>
<td style="text-align: center;">CVPR 2020</td>
<td style="text-align: center;">8.2M</td>
<td style="text-align: center;">-</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Google Landmarks <span class="citation" data-cites="weyand2020google"></span></td>
<td style="text-align: center;">CVPR 2020</td>
<td style="text-align: center;">4.1M</td>
<td style="text-align: center;">-</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">LSUN <span class="citation" data-cites="yu15lsun"></span></td>
<td style="text-align: center;">arXiv 2015</td>
<td style="text-align: center;">9.8M</td>
<td style="text-align: center;">-</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Objects365 <span class="citation" data-cites="Object365"></span></td>
<td style="text-align: center;">ICCV 2019</td>
<td style="text-align: center;">1.7M</td>
<td style="text-align: center;">-</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Open Images V7 <span class="citation" data-cites="OpenImages"></span></td>
<td style="text-align: center;">IJCV 2020</td>
<td style="text-align: center;">7.8M</td>
<td style="text-align: center;">-</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Places365 <span class="citation" data-cites="zhou2017places"></span></td>
<td style="text-align: center;">TPAMI 2017</td>
<td style="text-align: center;">6.5M</td>
<td style="text-align: center;">-</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">SA-1B <span class="citation" data-cites="SAM"></span></td>
<td style="text-align: center;">ICCV 2023</td>
<td style="text-align: center;">11.1M</td>
<td style="text-align: center;">-</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>

</div>

<div id="tab:gt_pseudo_ratio" markdown="1">

| Ratio | train(GT) | train(Pseudo) | test | Abs Rel ↓ | \\(\delta_1\\) ↑ | \\(\delta_2\\) ↑ | \\(\delta_3\\) ↑ |
|:---|:---|:---|:---|:--:|:--:|:--:|:--:|
| 1:1 | M-all | ST-all(p) | SF | 0.086 | 0.924 | 0.977 | 0.990 |
| 1:2 | M-all | ST-all(p) | SF | 0.087 | 0.923 | 0.977 | 0.990 |
| 1:4 | M-all | ST-all(p) | SF | 0.085 | 0.923 | 0.977 | 0.990 |

**Ratios of GT and Pseudo label during training.** We conduct additional experiments with varying ratios, which shows our method is robust across different ratios starting from 1:1.

</div>

<div id="tab:cube_tangent" markdown="1">

| Projection | Method | train | test | Abs Rel ↓ | \\(\delta_1\\) ↑ | \\(\delta_2\\) ↑ | \\(\delta_3\\) ↑ |
|:---|:---|:---|:---|:--:|:--:|:--:|:--:|
| Cube | UniFuse | M-all+ST-all(p) | SF | **0.086** | **0.924** | 0.977 | 0.990 |
| Tangent | UniFuse | M-all+ST-all(p) | SF | 0.087 | 0.923 | **0.978** | **0.991** |

**Comparison between Tangent Image and Cube Projection.** We compare two of the most commonly used perspective camera projections for panorama images. As shown in the table, both projections yield similar quantitative results. However, we select the cube projection for knowledge distillation due to its broader field of view coverage.

</div>

<figure id="fig:rebuttal_in_the_wild">
<img src="./figures/rebuttal_in_the_wild.png"" style="width:100.0%" />
<figcaption><strong>Additional in-the-wild results.</strong> We compare our proposed joint-training method (Matterport3D (GT) + SpatialAudioGen (Pseudo)) with a model trained only on the Matterport3D dataset, using data randomly downloaded from the internet. This comparison demonstrates the significant improvement of our method along with its generalization ability and effectiveness on real-world data.</figcaption>
</figure>

[^1]: Stig Nygaard, https://www.flickr.com/photos/stignygaard/49659694937, CC BY 2.0 DEED  
    Dominic Alves, https://www.flickr.com/photos/dominicspics/28296671029/, CC BY 2.0 DEED  
    Luca Biada, https://www.flickr.com/photos/pedroscreamerovsky/6873256488/, CC BY 2.0 DEED  
    Luca Biada, https://www.flickr.com/photos/pedroscreamerovsky/6798474782/, CC BY 2.0 DEED
