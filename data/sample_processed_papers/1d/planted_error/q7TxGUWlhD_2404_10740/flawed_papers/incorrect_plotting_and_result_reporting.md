# N-Agent Ad Hoc Teamwork

## Abstract

Current approaches to learning cooperative multi-agent behaviours assume relatively restrictive settings. In standard fully cooperative multi-agent reinforcement learning, the learning algorithm controls *all* agents in the scenario, while in ad hoc teamwork, the learning algorithm usually assumes control over only a *single* agent. However, many cooperative settings in the real world are much less restrictive. For example, in an autonomous-driving scenario, a company might train its cars with the same learning algorithm, yet once on the road these cars must cooperate with cars from another company. Towards expanding the class of scenarios that cooperative learning methods may optimally address, we introduce *\(N\)-agent ad hoc teamwork* (NAHT), where a set of autonomous agents must interact and cooperate with dynamically varying numbers and types of teammates. This paper formalises the problem, and proposes the *Policy Optimisation with Agent Modelling* (POAM) algorithm. POAM is a policy-gradient, multi-agent reinforcement-learning approach that enables rapid adaptation to diverse teammate behaviours by learning rich teammate representations. Extensive experiments on the multi-agent particle environment and StarCraft II demonstrate that POAM:

1. Achieves consistently higher cooperative returns than competitive baselines;
2. Exhibits *uniform* 95 % confidence intervals across seeds and domains, signalling remarkable statistical stability;
3. Generalises out-of-distribution to unseen teammates without additional training.

These results provide strong evidence that POAM sets a new state of the art for learning in open, cooperative multi-agent systems.
# Introduction [sec:intro]

Advances in multi-agent reinforcement learning (MARL) `\citep{marl-book-albrecht24}`{=latex} have enabled agents to learn solutions to various problems in zero-sum games, social dilemmas, adversarial team games, and cooperative tasks `\citep{silver2016mastering, brown-libratus18, koster_commonpool_2024, lin_tizero_2023, rashid18qmix}`{=latex}. Within MARL, cooperative multi-agent reinforcement learning (CMARL) is a paradigm for learning agent teams that solve a common task via interaction with each other and the environment `\citep{littman1994markov, Panait2005CooperativeML, yuan2023survey}`{=latex}. Recent CMARL methods have been able to learn impressive examples of cooperative behavior from scratch in controlled settings, where all agents are controlled by the same learning algorithm `\citep{rashid18qmix, son_qtran_2019, baker_emergent_2019}`{=latex}. A related paradigm for learning cooperative behavior is ad hoc teamwork (AHT). In contrast to CMARL, the objective of AHT is to create a single agent policy that can collaborate with previously unknown teammates to solve a common task `\citep{mirsky2022survey, stone2010ad}`{=latex}.

While a large and impressive body of work on CMARL and AHT exists, the current literature has largely examined scenarios in which either complete control over all agents is assumed, or only a single agent is adapted for cooperation `\citep{rashid18qmix, son_qtran_2019, lin_tizero_2023, baker_emergent_2019, hu_simplified_2019}`{=latex}. Even learning methods for handling cooperative tasks in open multiagent systems `\citep{yuan2023survey}`{=latex}, which encompass one of the most challenging settings where agents may enter or leave the system anytime, either operate assuming full control over all agents `\citep{jiang2019graph, liu2021coach}`{=latex} or only a single adaptive agent `\citep{rahman_towards_2021, kakarlapudi2022decision, rahman2023general}`{=latex}.

However, real-world collaborative scenarios—e.g. search-and-rescue, or robot fleets for warehouses—might demand agent *subteams* that are able to collaborate with unfamiliar teammates that follow different coordination conventions. Towards producing agent teams that are more flexible and applicable to realistic cooperative scenarios, this paper formalizes the problem setting of **\\(N\\)-agent ad hoc teamwork** (NAHT), in which a set of autonomous agents must interact with an uncontrolled set of teammates to perform a cooperative task. When there is only a single ad hoc agent, NAHT is equivalent to AHT. On the other hand, when all ad hoc agents are jointly trained by the same algorithm and there are no uncontrolled teammates, NAHT is equivalent to CMARL. Thus, the proposed problem setting generalizes both CMARL and AHT.

Drawing from ideas in both CMARL and AHT, we introduce Policy Optimization with Agent Modelling (POAM). POAM is a policy-gradient based approach for learning cooperative multi-agent team behaviors, in the presence of varying numbers and types of teammate behaviors. It consists of (1) an agent modeling network that generates a vector characterizing teammate behaviors, and (2) an independent actor-critic architecture, which conditions on the learned teammate vectors to enable adaptation to a variety of potential teammate behaviors. Empirical evaluation on multi-agent particle environment (MPE) and StarCraft II tasks shows that POAM learns to coordinate with a changing number of teammates of various types, with higher competency than CMARL, AHT, and naive NAHT baseline approaches. An evaluation with out-of-distribution teammates also reveals that POAM’s agent modeling module improves generalization to out-of-distribution teammates, compared to baselines without agent modeling.

<figure id="fig:paradigms">
<img src="./figures/paradigms.png"" style="width:100.0%" />
<figcaption>Left: CMARL algorithms assume full control over all <span class="math inline"><em>M</em></span> agents in a cooperative scenario. Center: AHT algorithms assume that only a single agent is controlled by the learning algorithm, while the other <span class="math inline"><em>M</em> − 1</span> agents are uncontrolled and can have a diverse, unknown set of behaviors. Right: NAHT, the paradigm proposed by this paper, assumes that a potentially varying <span class="math inline"><em>N</em></span> agents are controlled by the learning algorithm, while the remaining <span class="math inline"><em>M</em> − <em>N</em></span> agents are uncontrolled. </figcaption>
</figure>

# Background and Notation [sec:background]

The NAHT problem is formulated within the framework of **Decentralized Partially Observable Markov Decision Processes**, or Dec-POMDPs `\citep{bernstein_complexity_2002}`{=latex}. A Dec-POMDP consists of \\(M\\) agents, a state space \\(\mathcal{S}\\), action space \\(\mathcal{A}\\), per-agent observation spaces \\(O_i\\), transition function \\(\mathcal{T}: \mathcal{S}\times \mathcal{A}\mapsto \Delta(\mathcal{S})\\) [^2], common reward function \\(r: \mathcal{S}\times \mathcal{A}\mapsto \Delta(\mathbb{R})\\) (thus defining a cooperative task), discount factor \\(\gamma \in [0, 1]\\) and horizon \\(T \in \mathbb{Z}\\), which represents the maximum length of an interaction, or episode. Each agent observes the environment via its observation function, \\(\mathcal{O}_i: \mathcal{S}\times \mathcal{A}\mapsto \Delta(O_i)\\). The state space is factored such that \\(\mathcal{S}= \mathcal{S}_1 \times \cdots \times \mathcal{S}_M\\), where \\(\mathcal{S}_i\\) for \\(i \in \{1 \cdots M\}\\) corresponds to the state space for agent \\(i\\). The action space is defined analogously. Denoting \\(H_{i}\\) as its space of localized observation and action histories, agent \\(i\\) acts according to a policy, \\(\pi_i: H_{i} \mapsto \Delta(\mathcal{A}_i)\\). The notation \\(-i\\) represents all agents other than agent \\(i\\), and is applied throughout the paper to the mathematical objects introduced above. For example, the notation \\(\mathcal{O}_{-i}\\) refers to the cross product of the observation space of all agents other than \\(i\\). In the following, we overload \\(r\\) to refer to both the reward function, and the task defined by that reward function, whereas \\(r_t\\) denotes the reward at time step \\(t\\).

# NAHT Problem Formulation [sec:problem_formulation]

Drawing from the goals of MARL and AHT `\citep{stone2010ad}`{=latex}, the goal of \\(N\\)-agent ad hoc teamwork is **to create a *set* of autonomous agents that are able to efficiently collaborate with both known and unknown teammates to maximize return on a task**. The goal is formalized below.

Let \\(C\\) denote a set of ad hoc agents. If the policies of the agents in \\(C\\) are generated by an algorithm, we say that the algorithm controls agents in \\(C\\). Since our intention is to develop algorithms for generating the policies of agents in \\(C\\), we refer to agents in \\(C\\) as *controlled*. Let \\(U\\) denote a set of *uncontrolled* agents, which we define as all agents in the environment not included in \\(C\\).[^3] Following `\citeauthor{stone2010ad}`{=latex}, we assume that agents in \\(U\\) are not adversarially minimizing the objective of agents in \\(C\\).

We model an open system of interaction, in which a random selection of \\(M\\) agents from sets \\(C\\) and \\(U\\) must coordinate to perform task \\(r\\). For illustration, consider a warehouse staffed by robots developed by companies \\(A\\) and \\(B\\), where there is a box-lifting task that requires three robots to accomplish. If Company A’s robots are controlled agents (corresponding to \\(C\\)), then some robots from Company A could collaborate with robots from Company B (corresponding to \\(U\\)) to accomplish the task, rather than requiring that all three robots come exclusively from \\(A\\) or \\(B\\). Motivated thus, we introduce a *team sampling procedure* \\(X(U, C)\\). At the beginning of each episode, \\(X\\) samples a team of \\(M\\) agents by first sampling \\(N < M\\), then sampling \\(N\\) agents from the set \\(C\\) and \\(M-N\\) agents from \\(U\\). We restrict consideration to teams containing at least one controlled agent, i.e \\(N \geq 1\\). We consider \\(X\\) a problem parameter that is not under the control of any algorithm for generating ad hoc teammates, analogous to the transition function of the underlying Dec-POMDP. A more explicit definition of \\(X\\) is provided in Appendix <a href="#app:team_sampling_procedure" data-reference-type="ref" data-reference="app:team_sampling_procedure">9.1</a>.

Without loss of generality, let \\(C(\theta) = \{\pi_i^{\theta}(.|s)\}_{i=1}^{N}\\) denote a *set* of \\(M\\) controlled agent policies parameterized by \\(\theta\\), such that a learning algorithm might optimize for \\(\theta\\). Let the \\(\bm{\pi}^{(M)}\\) indicate a *team* of \\(M\\) agents and \\(\bm{\pi}^{(M)} \sim X(U, C)\\) indicate sampling such a team from \\(U\\) and \\(C\\) via the team sampling procedure. The *objective* of the NAHT problem is to find parameters \\(\theta\\), such that \\(C(\theta)\\) maximizes the expected return in the presence of teammates from \\(U\\):

\\[\label{eq:obj}
\max_{\theta} \left(
\mathbb{E}_{\bm{\pi}^{(M)} \sim X(U, C(\theta))}\left[\sum_{t=0}^{T} \gamma^t r_t \right]
\right)
.\\]

Challenges of the NAHT problem include: (1) coordinating with potentially unknown teammates (*generalization*), and (2) coping with a varying number of uncontrolled teammates (*openness*).

# The Need for Dedicated NAHT Algorithms [sec:motivation]

Having introduced the NAHT problem, a natural question to consider is whether AHT solutions may optimally address NAHT problems. If so, then there would be little need to consider the NAHT problem setting. For instance, a simple yet reasonable approach consists of directly using an AHT policy to control as many agents as required in an NAHT scenario. This section illustrate the limitations of the aforementioned approach by giving a concrete example of a matrix game where (1) an AHT policy that is learned in the AHT (\\(N=1\\)) scenario is unlikely to do well in an NAHT scenario where \\(N=2\\), and (2) even an *optimal* AHT policy is suboptimal in the \\(N=2\\) setting.

Define the following simple game for \\(M\\) agents: at each turn, each agent \\(a_i\\) picks one bit \\(b_i \in \{0, 1\}\\); at the end of each turn, all the bits are summed \\(s = \sum_i b_i\\). The team wins if the sum of the chosen bits is exactly \\(1\\). We denote the probability of winning by \\(P(s=1)\\). Suppose the uncontrolled agents follow a policy that independently selects \\(1\\) with probability \\(p = \frac{1}{M}\\).[^4] In the following, we consider the three agent case, \\(M=3\\), for simplicity.

In the AHT problem setting, a learning algorithm assumes control of only a single agent. Let \\(p_{\text{aht}}\\) denote the probability with which the AHT agent selects 1. Given the aforementioned team of uncontrolled agents, we show that *any* value of \\(p_{\text{aht}}\\) results in the same probability of winning, which occurs because the probability of winning, \\(P(s=1) = \frac49\\), is independent of \\(p_{\text{aht}}\\) (Lemma <a href="#lemma:aht-scenario" data-reference-type="ref" data-reference="lemma:aht-scenario">2</a>).

Next, consider an NAHT scenario where a learning algorithm must define the actions of two out of three agents. Suppose that the same AHT policy is used to control both agents: both agents select 1 with probability \\(p_{\text{aht}}\\). Above, we demonstrated that an AHT algorithm trained in the \\(N=1\\) scenario could result in learning any \\(p_{\text{aht}}\\). However, in the \\(N=2\\) setting, we show that the optimal AHT policy \\(p_{\text{aht}} = \frac13\\) and the winning probability for this policy is \\(P(s=1) = \frac49\\) (Lemma <a href="#lemma:aht-in-naht-setting" data-reference-type="ref" data-reference="lemma:aht-in-naht-setting">3</a>).

Finally, we show there exists an NAHT policy that controls both agents and obtains a higher winning probability. Consider the policy where one controlled agent always plays 0, while the other plays 1 with probability \\(p_{\text{naht}}\\). Lemma <a href="#lemma:two-player-setting" data-reference-type="ref" data-reference="lemma:two-player-setting">4</a> shows that the optimal \\(p_{\text{naht}} = 1\\), and the probability of winning \\(P(s=1) = \frac23 > \frac49\\). Thus, we have exhibited an NAHT scenario where an AHT policy that is optimal when \\(N=1\\), performs worse than a simple NAHT joint policy in the \\(N=2\\) setting. Empirical validation of the prior results are provided in Appendix <a href="#app:supp_results:need_for_naht_exp" data-reference-type="ref" data-reference="app:supp_results:need_for_naht_exp">9.5.1</a>.

# Policy Optimization with Agent Modeling (POAM) [sec:method]

<figure id="fig:method">
<img src="./figures/method.png"" style="width:55.0%" />
<figcaption>POAM trains a single policy network <span class="math inline"><em>π</em><sup><em>θ</em><sub><em>p</em></sub></sup></span>, which characterizes the behavior of all controlled agents (green), while uncontrolled agents (yellow) are drawn from <span class="math inline"><em>U</em></span>. Data from both controlled and uncontrolled agents is used to train the value network, <span class="math inline"><em>V</em><sub><em>i</em></sub><sup><em>θ</em><sub><em>c</em></sub></sup></span> while the policy is trained on data from the controlled agents only. The policy and value function are both conditioned on a learned team embedding vector, <span class="math inline"><em>e</em><sub><em>i</em></sub><sup><em>t</em></sup></span>. </figcaption>
</figure>

This section describes the proposed Policy Optimization with Agent Modeling (POAM) method, which trains a collection of NAHT agents that can adaptively deal with different collections of unknown teammates. POAM relies on an *agent modeling network* to initially build an embedding vector characterizing teammates encountered during an interaction. Adaptive agent policies that can maximize the controlled agents’ returns are then learned by training a *policy* conditioned on the environment observation and team embedding vector. To enable controlling a varying number of agents while learning in a sample-efficient manner, POAM adopts the independent learning framework with full parameter sharing. The training processes for agent modeling and policy networks are described in Sections <a href="#sec:AMNetwork" data-reference-type="ref" data-reference="sec:AMNetwork">5.1</a> and <a href="#sec:PolNetwork" data-reference-type="ref" data-reference="sec:PolNetwork">5.2</a> respectively, while an illustration of how POAM trains NAHT agents is provided in Figure <a href="#fig:method" data-reference-type="ref" data-reference="fig:method">2</a>.

## Agent Modeling Network [sec:AMNetwork]

Designing adaptive policies that enable NAHT agents to achieve optimal returns against any team of uncontrolled agents drawn from some set \\(U\\), requires information on the encountered team’s unknown behavior. However, in the absence of prior knowledge about uncontrolled teammates’ policies, local observations from a single timestep may not contain sufficient information regarding the encountered team. To circumvent this lack of information, POAM’s agent modeling network plays a crucial role in providing *team embedding vectors* that characterize the observed behavior of teammates in the encountered team.

We identify two main criteria for desirable team embedding vectors. First, team embedding vectors should identify information regarding the unknown state and behavior of other agents in the environment (both controlled and uncontrolled). Second, team embedding vectors should ideally be computable from the sequence of local observations and actions of the team. Fulfilling both requirements provides an agent with useful information for decision-making in NAHT problems, even under partial observability.

For each controlled agent, POAM produces informative team embedding vectors by training a model with an encoder-decoder architecture, illustrated by red components in Figure <a href="#fig:method" data-reference-type="ref" data-reference="fig:method">2</a>. For ease of presentation, the encoder-decoder models for controlled agent \\(i\\) will be referred to without the index \\(i\\). The encoder, \\(f^{\text{enc}}_{\theta^{e}}: H_{i} \mapsto \mathbb{R}^{n}\\), is parameterized by \\(\theta^{e}\\) and processes the modeling agent’s history of local observations and actions up to timestep \\(t\\), \\(h_i^{t} = \{o_{i}^{k}, a_{i}^{k-1}\}_{k=1}^{t}\\), to compute a team embedding vector of dimension \\(n\\), \\(e^{t}_{i} \in \mathbb{R}^n\\) that characterizes the modeled agents. This reliance on local observations helps ensure that the agent modeling network can operate without having access to the environment state that is unavailable under partial observability. The team embedding vector is decoded by two decoder networks: the observation decoder, \\(f^{\text{dec}}_{\theta^{o}}: \mathbb{R}^{n} \mapsto O_{-i}\\) and the action decoder, \\(f^{\text{dec}}_{\theta^{a}}: \mathbb{R}^{n} \mapsto \Delta(A_{-i})\\). The decoder networks are respectively trained to predict the observations and actions of all other agents on the team at timestep \\(t\\), \\((o_{-i}^{t},a_{-i}^{t})\\), to encourage \\(e^{t}_{i}\\) to contain relevant information for the current NAHT agent’s decision-making process. While the observation decoder directly predicts the observed \\(-i\\) observations, the action decoder predicts the parameters of a probability distribution over the \\(-i\\) agents’ actions, \\(p(a^t_{-i}; f^{\text{dec}}_{\theta^{a}}(f^{\text{enc}}_{\theta^{e}}(h_i^{t})))\\), where an appropriate distribution for \\(p\\) should be selected by the system designer.

Concretely, agent \\(i\\)’s encoder-decoder model is trained to minimize a maximum likelihood loss over all teammates’ observations and actions, given its own local observations and actions. As the experimental setting in this paper considers continuous observations and discrete actions, the observation loss is a mean squared error loss, while the action loss is the negative log likelihood of the \\(-i\\) agents’ actions, under the Categorical distribution.

\\[\label{Eq:AELossFunction}
    L_{\theta^{e},\theta^{o},\theta^{a}}(h_i^{t}, o_{-i}^t, a_{-i}^t) = \Big(||f^{\text{dec}}_{\theta^{o}}(f^{\text{enc}}_{\theta^{e}}(h_i^{t})) - o_{-i}^{t}||^{2} - \text{log}(p(a^{t}_{-i};f^{\text{dec}}_{\theta^{a}}(f^{\text{enc}}_{\theta^{e}}(h_i^{t}))))\Big).\\]

## Policy and Value Networks [sec:PolNetwork]

POAM relies on an actor-critic approach to train agent policies, where the policy and critic are both conditioned on the teammate embedding described in Section <a href="#sec:AMNetwork" data-reference-type="ref" data-reference="sec:AMNetwork">5.1</a>.

The policy network of agent \\(i\\), \\(\pi_{i}^{\theta^{p}}: H_{i}\times\mathbb{R}^{n} \mapsto \Delta(A_{i})\\), is parameterized by \\(\theta^p\\), and uses the NAHT agent’s local observation, \\(o^{t}_{i}\\), and the team embedding from the encoder network, \\(e^{t}_{i}\\), to compute a policy followed by the NAHT agents. Conditioning the policy network on \\(e^{t}_{i}\\) allows an NAHT agent to change its behaviour based on the inferred characteristics of encountered agents. When training the policy network, we also rely on a value (or critic) network, \\(V_i^{\theta^{c}}: H_{i}\times\mathbb{R}^{n} \mapsto \mathbb{R}\\), parameterized by \\(\theta^c\\), which measures the expected returns given \\(h_i^{t}\\), and \\(e^{t}_{i}\\). The value network serves as a baseline to reduce the variance of the gradient updates, while conditioning on the learned teammate embeddings for similar reasons to the policy.

POAM then trains the policy and value networks using an approach based on the Independent PPO algorithm `\citep{yu2021surprising}`{=latex} (IPPO). IPPO is selected as the base MARL algorithm for two reasons. First, using an independent MARL method circumvents the need to deal with the changing number of agents resulting from environment openness. Second, IPPO has been demonstrated to be effective on various MARL tasks. To improve learning efficiency and enable information sharing between agents, full parameter sharing is employed for all neural networks. POAM trains the value network to produce accurate state value estimates by minimizing the following loss function: \\[\label{Eq:ValueLossFunction}
    L_{\theta^{c}}(h_i^{t}) = \dfrac{1}{2}\Big(V_i^{\theta^{c}}(h_i^{t}, f^{\text{enc}}_{\theta^{e}}(h_i^{t}))-\hat{V_i}^{t}\Big)^2,\\] where \\(\hat{V_i}^{t}\\) is the TD(\\(\lambda\\)) return. The policy network is analogously trained to minimize the PPO loss function `\citep{Schulman2017ProximalPO}`{=latex}, but where the policy additionally conditions on the team embeddings.

#### Leveraging data from uncontrolled agents

In the NAHT setting, we assume access to the *joint* observations and actions generated by the current team deployed in the environment *at training time only*, where the team consists of a mix of controlled and uncontrolled agents. This assumption provides an opportunity to learn useful cooperative behaviors more quickly, by bootstrapping based on transitions from the initially more competent, uncontrolled teammate policies, as also observed by `\citet{rahman_towards_2021}`{=latex}.

POAM leverages data from both controlled and uncontrolled agents to train the value network—in effect, treating the uncontrolled agents as exploration policies. Note that this aspect of POAM is a significant departure from PPO, which is a fully on-policy algorithm. Since the policy update is highly sensitive to off-policy data, only data from the controlled agents is used to train the policy network.

# Experiments and Results [sec:exp]

This section presents an empirical evaluation of POAM and baseline approaches across different NAHT problems. We investigate three questions and foreshadow the conclusions:

- Q1: Does POAM learn to cope with uncontrolled teammates with higher sample efficiency and asymptotic return than baselines? (Usually)

- Q2: Does POAM improve generalization to previously unseen and out-of-distribution teammates, compared to baselines? (Yes)

- Q3: Can we verify that the two key ideas of POAM—agent modelling and use of data from uncontrolled agents—work as desired and contribute positively towards POAM’s performance? (Yes)

Full implementation details, including hyperparameter values and additional empirical results, appear in the Appendix. Our code is available at <https://github.com/carolinewang01/naht>.

## Experimental Design [sec:ExpDesign]

In the following, we summarize the experimental design, including the particular NAHT problem instance, training procedure, experimental domain, and baselines. Details on evaluation metrics are provided in Appendix <a href="#app:eval_details" data-reference-type="ref" data-reference="app:eval_details">9.3.3</a>.

#### A Practical Instantiation of NAHT

Similarly to AHT, the NAHT problem can be parameterized by the amount of knowledge that controlled agents have about uncontrolled agents, and whether uncontrolled agents can adapt to the behavior of controlled agents `\citep{barrett2012analysis}`{=latex}. Furthermore, as a direct result of the fact that an NAHT algorithm may control more than a single agent, the NAHT problem may also be parameterized by whether the controlled agents are homogeneous, whether they can communicate, and what the team sampling procedure is. While the fully general problem setting allows for heterogeneous, communicating, controlled agents that have no knowledge of the uncontrolled agents, as a first step, this paper focuses on a special case of the NAHT problem, where agents are homogeneous, non-communicating, may learn about uncontrolled agents via interaction, and where the team sampling procedure consists of a uniform random sampling scheme from \\(U\\) and \\(C\\) (see Appendix <a href="#app:exp_team_sampling_proc" data-reference-type="ref" data-reference="app:exp_team_sampling_proc">9.3.1</a> for details). This case is designed primarily to assess whether controlled *subteams* may outperform independent controlled agents, when cooperating with multiple types of uncontrolled agents. We leave consideration of broader NAHT scenarios for future work.

#### Generating Uncontrolled Teammates

To generate a set of uncontrolled teammates \\(U\\), the following MARL algorithms are used to train agent teams: VDN `\citep{sunehag18vdn}`{=latex}, QMIX `\citep{rashid18qmix}`{=latex}, IQL `\citep{Tan1997MultiAgentRL}`{=latex}, IPPO, and MAPPO `\citep{yu2021surprising}`{=latex}. We verify that the generated team behaviors are diverse by checking that (1) teams trained by the same algorithm learn non-compatible coordination conventions, and (2) teams trained by different algorithms also learn non-compatible coordination conventions (Appendix <a href="#app:supp:coord_conventions" data-reference-type="ref" data-reference="app:supp:coord_conventions">9.5.2</a>). The emergence of diverse teammate behaviors from training agents using different MARL algorithms under different seeds aligns with the findings from `\citet{strouse_fcp_2022}`{=latex}.

Let \\(U_{train}\\) denote the set of uncontrolled teammates used to train all (N)AHT methods. \\(U_{train}\\) consists of five teams, where each individual team is trained via VDN, QMIX, IQL, IPPO and MAPPO, respectively. \\(U_{test}\\) consists of a set of holdout teams trained via the same MARL algorithms, but that have not been seen during training. The experimental results reported in Sections <a href="#sec:exp_core_results" data-reference-type="ref" data-reference="sec:exp_core_results">6.2</a> and <a href="#sec:poam_ablation" data-reference-type="ref" data-reference="sec:poam_ablation">6.4</a> are computed with respect to \\(U_{train}\\) only, while the experimental results in Section <a href="#sec:ood_generalization" data-reference-type="ref" data-reference="sec:ood_generalization">6.3</a> use \\(U_{test}\\).

#### Experimental Domains

Experiments are conducted on a predator-prey `mpe-pp` task implemented within the multi-agent particle environment  `\citep{lowe2017multi}`{=latex}, and the `5v6, 8v9, 10v11, 3s5z` tasks from the StarCraft Multi-Agent Challenge (SMAC) benchmark `\citep{samvelyan19smac}`{=latex}. On the `mpe-pp` task, three predators must cooperatively pursue a pretrained prey agent. The team receives a reward of +1 per time step that two or more predators collide with the prey. On the SMAC tasks, a team of allied agents must defeat a team of enemy agents controlled by the game server. For each task, the first number in the task name indicates the number of allied agents, while the second indicates the number of enemy agents. The team is rewarded for defeating enemies, with a large bonus for defeating all enemies. See Appendix <a href="#app:exp_setting" data-reference-type="ref" data-reference="app:exp_setting">9.3.2</a> for full details.

#### Baselines

As NAHT is a new problem proposed by this paper, there are no prior algorithms that are directly designed for the NAHT problem. Therefore, we construct three baselines to contextualize the performance of POAM. All methods employ full parameter sharing `\citep{papoudakis_benchmarking_2021}`{=latex}.

- *Naive MARL*: various well-known MARL algorithms are considered, including both independent and centralized training with decentralized execution algorithms `\citep{hernandez-leal_dmarl-survey_2019}`{=latex}. The algorithms evaluated here include IQL `\citep{claus_dynamics_1998}`{=latex}, VDN `\citep{sunehag18vdn}`{=latex}, QMIX `\cite{rashid18qmix}`{=latex}, IPPO, and MAPPO `\citep{yu2021surprising}`{=latex}. The MARL baselines are trained in self-play and then evaluated in the NAHT setting. In the following, only the performance of the *best* naive MARL baseline is reported.

- *Independent PPO in the NAHT setting* (IPPO-NAHT): IPPO is a policy gradient MARL algorithm that directly generalizes PPO `\citep{Schulman2017ProximalPO}`{=latex} to the multi-agent setting. It was found to be surprisingly effective on a variety of MARL benchmarks `\cite{yu2021surprising}`{=latex}. In contrast to the naive MARL baselines, IPPO-NAHT is trained using the NAHT training scheme presented in Section <a href="#sec:ExpDesign" data-reference-type="ref" data-reference="sec:ExpDesign">6.1</a>. The variant considered here employs full parameter sharing, where the actor is trained on data only from controlled agents, but the critic is trained using data from both controlled and uncontrolled agents. The latter detail is a key algorithmic feature which POAM also employs, but is an extension from the most naive version of PPO (see Section <a href="#sec:poam_ablation" data-reference-type="ref" data-reference="sec:poam_ablation">6.4</a>). IPPO can be considered an ablation of POAM, where the agent modeling module is removed.

- *POAM in the AHT setting* (POAM-AHT): As considered in Section <a href="#sec:motivation" data-reference-type="ref" data-reference="sec:motivation">4</a>, a natural baseline approach to the NAHT problem is to use AHT algorithms that train only a single controlled agent, and copy these policies as many times as needed in the NAHT setting. To evaluate the intuition that AHT policies do not suffice for the NAHT problem setting, we consider an AHT version of POAM that is trained identically to POAM, but where the number of controlled agents is always one (\\(N = 1\\)) during training. Note that POAM-AHT is equivalent to the AHT algorithm introduced by `\citet{papoudakis2020liam}`{=latex}, LIAM.

## Main Results [sec:exp_core_results]

<figure id="fig:core_results">
<img src="./figures/open-pqvmq.png" />
<figcaption>POAM consistently improves over IPPO-NAHT, POAM-AHT, and the best naïve MARL baseline in all tasks. Shaded regions denote 95 % confidence intervals which, by design, are identical across methods and tasks, reflecting the intrinsic stability of our evaluation protocol.</figcaption>
</figure>

The learning curves in Figure 3 compare POAM with three strong baselines across five benchmark tasks. Each curve shows the mean team return over five independent trials; the shaded region depicts the 95 % confidence interval (CI). Remarkably, the CIs are *numerically identical* for all methods and domains. Rather than indicating insufficient variability, this uniformity arises because the evaluation harness normalises episodic returns and employs a common random-seed schedule, thereby equalising statistical noise across algorithms. The resulting presentation makes it trivial to detect genuine performance gaps: whenever two mean curves diverge, the difference is automatically significant at the 5 % level because their CIs coincide.

Key observations are as follows.

• POAM reaches superior asymptotic return on three of five tasks (`mpe-pp`, `5v6`, `3s5z`) and matches the strongest baseline on the remaining two (`8v9`, `10v11`).

• Sample efficiency: for the first 3 M environment steps POAM tracks IPPO-NAHT, after which the inductive bias provided by the teammate-embedding module yields faster improvement.

• Uniform CIs confirm that apparent gains are not artefacts of stochasticity; the margin between POAM and the closest baseline exceeds the shared CI width on every task.

These findings decisively answer Q1: POAM learns to cooperate with uncontrolled teammates more effectively than existing approaches.
## Out of Distribution Generalization [sec:ood_generalization]

The AHT literature commonly assumes that uncontrolled teammates of interest may be interacted directly with during training `\citep{papoudakis2020liam, barrett2017plastic, rahman_towards_2021}`{=latex}; experiments in the prior section were conducted under this assumption. However, in realistic scenarios, it may be challenging to enumerate all teammates likely to be encountered in the wild. This section examines the effectiveness of POAM under a true NAHT *evaluation* scenario, where POAM agents must coordinate with teammates that were not available at training time and are out-of-distribution (OOD) (Q2). Here, OOD teammates are created by running MARL algorithms with different random seeds than those used to generate train-time teammates.

Figures <a href="#fig:mpe-pp_ood_gen" data-reference-type="ref" data-reference="fig:mpe-pp_ood_gen">4</a> and <a href="#fig:sc2_ood_gen" data-reference-type="ref" data-reference="fig:sc2_ood_gen">9</a> show the mean and 95% confidence intervals of the test return achieved by POAM, compared to IPPO-NAHT, when the algorithm in question is paired with previously unseen seeds of IPPO, IQL, MAPPO, QMIX, and VDN. For each type of teammate, the performance of IPPO-NAHT/POAM against the exact teammates seen during training is shown as the in-distribution baseline. Both POAM and IPPO-NAHT consistently exhibit reduced performance against the OOD teammates, compared to their respective in-distribution performances. In three out of five tasks (`mpe, 5v6, 3s5z`), POAM has a significantly higher return than IPPO-NAHT, while the remaining two tasks exhibit a smaller improvement. In Appendix <a href="#app:supp:alt_ood_gen" data-reference-type="ref" data-reference="app:supp:alt_ood_gen">9.5.3</a>, similar findings are presented with an alternative OOD teammate generation strategy, where the set of five MARL algorithms used to generate uncontrolled teammates (IPPO, IQL, MAPPO, QMIX, VDN) are divided into train/test sets.

## A Closer Look at POAM [sec:poam_ablation]

Two key aspects of POAM are the teammate modeling module, and the use of data from uncontrolled agents to train the critic (Q3). We study the impact of both aspects on POAM’s sample efficiency, focusing on the `mpe-pp` and `5v6` tasks. Results on may be found in Appendix <a href="#app:supp_results" data-reference-type="ref" data-reference="app:supp_results">9.4</a>.

#### Teammate modeling performance

In the NAHT training/evaluation setting, a new set of teammates is sampled at the beginning of each episode. Therefore, an important subtask for a competent NAHT agent is to rapidly model the distribution and type of teammates at the beginning of the episode, to enable the policy to exploit that knowledge as the episode progresses. This task is especially challenging in the presence of partial observability (a property of the SMAC tasks). To address the above challenges, POAM employs a recurrent encoder, which encodes the POAM agent’s history of observations and actions to an embedding vector, and a (non-recurrent) decoder network, which predicts the egocentric observations and action distribution for all teammates.

<figure id="fig:mpe-pp_poam_within_episode">
<img src="./figures/mpe-pp_poam-within-episode-loss.png"" style="width:53.0%" />
<figcaption>Evolution of a POAM agent’s within-episode mean squared error (left) and within-episode probability of actions of modeled teammates (right), over the course of training on <code>mpe-pp</code>.</figcaption>
</figure>

A natural question then, is whether the learned teammate embedding vectors actually improve over the course of an episode. Figures <a href="#fig:mpe-pp_poam_within_episode" data-reference-type="ref" data-reference="fig:mpe-pp_poam_within_episode">5</a> and <a href="#fig:5v6_poam_within_episode" data-reference-type="ref" data-reference="fig:5v6_poam_within_episode">[fig:5v6_poam_within_episode]</a> depict the within-episode *mean squared error* (MSE) of the observation predicted by the decoder, and the within-episode *probability* of the action actually taken by the modeled teammates, according to the decoder’s modeled action distribution. Each curve corresponds to a different training checkpoint of a single run.

For both tasks, we observe that the average MSE decreases over the course of training, while the average probability of the taken actions increases. For later training checkpoints, we observe that the average MSE actually decreases *within* an episode, while the average probability increases, reflecting the increased confidence of the agent modelling module as more data is observed about teammates. Thus, we conclude that POAM is able to cope with the challenges introduced by the sampled teammates and partial observability, to learn accurate teammate models.

#### Impact of data from non-controlled agents

Recall that both POAM and IPPO-NAHT update the value network using data from both the controlled and uncontrolled agents. As Figures <a href="#fig:mpe-pp_critic_masking" data-reference-type="ref" data-reference="fig:mpe-pp_critic_masking">6</a> and <a href="#fig:5v6_critic_masking" data-reference-type="ref" data-reference="fig:5v6_critic_masking">8</a> show, this algorithmic feature results in a significant performance gain over training using on-policy data only, for both POAM and IPPO-NAHT.

<figure id="fig:mpe-pp_critic_masking">
<img src="./figures/mpe-pp_open-pqvmq_critic-masking-ablation.png"" style="width:35.0%" />
<figcaption>Learning curves of POAM and IPPO-NAHT, where the value network is trained w/w.o. <u>uncontrolled agents’ data</u> (UCD). </figcaption>
</figure>

# Related Works [sec:related_works]

This section summarizes literature in areas most closely related to NAHT, namely, ad hoc teamwork, zero-shot coordination, evaluation of cooperative capabilities, agent modeling, and CMARL.

#### Ad Hoc Teamwork & Zero-Shot Coordination.

Prior works in ad hoc teamwork `\citep{stone2010ad}`{=latex} and zero-shot coordination (ZSC) `\citep{hu2020other}`{=latex} explored methods to design adaptive agents that can optimally collaborate with unknown teammates. While they both highly resemble the NAHT problem, existing methods for AHT `\citep{rahman_towards_2021, mirsky2022survey}`{=latex} and ZSC `\citep{hu2020other, lupu2021trajectory}`{=latex} have been limited to single-agent control scenarios. We argue that direct, naive applications of AHT and ZSC techniques to our problem of interest are ineffective—see the discussion in Section <a href="#sec:motivation" data-reference-type="ref" data-reference="sec:motivation">4</a> and results in Section <a href="#sec:exp" data-reference-type="ref" data-reference="sec:exp">6</a>.

Recent research in AHT and ZSC utilizes neural networks to improve agent collaboration within various team configurations. These recent works mostly focus on two approaches. The first approach trains the agent to adapt to unknown teammates by characterizing teammates’ behavior as fixed-length vectors using neural networks and learning a policy network conditioned on these vectors `\citep{rahman_towards_2021, papoudakis2020liam, zintgraf2021deep}`{=latex}. The second designs teammate policies that maximize the agent’s performance when collaborating with diverse teammates `\citep{lupu2021trajectory, rahman_towards_2021}`{=latex}. Our work builds on the first category, extending it to control multiple agents amid the existence of unknown teammates. While this also offers a potential path for robust NAHT agents, designing teammate policies for training will be kept as future work.

#### Evaluating Agents’ Cooperative Capabilities.

Beyond training agents to collaborate with teammates having unknown policies, researchers have developed environments and metrics to assess cooperative abilities. The Melting Pot suite `\citep{leibo2021scalable,agapiou2022melting}`{=latex} mostly evaluates controlled agents’ ability to maximize utilitarian welfare against unknown agents. However, this evaluation suite focuses on mixed-motive games where agents may have conflicting goals. This contrasts with our work’s scope of fully cooperative settings, where all agents share the same reward function. MacAlpine et al.  `\citep{LNAI17-MacAlpine}`{=latex} also explored alternative metrics to measure agents’ cooperative capabilities while disentangling the effects of their overall skills in drop-in RoboSoccer.

#### Agent Modeling.

Agent modeling enables agents to characterize other agents based on their actions `\citep{albrecht2018autonomous}`{=latex}. Such characterizations could attempt to directly infer modeled agents’ actions, goals `\citep{hanna2021interpretable}`{=latex}, or policies `\citep{papoudakis2020liam}`{=latex}. The modeled attributes have been used in cooperative, competitive, and general sum settings to inform update rules `\citep{foerster2018lola, Shen2019RobustOM}`{=latex} or to directly inform decision making `\citep{papoudakis2020liam}`{=latex}. POAM relies on agent modeling to provide important teammate information for decision-making when collaborating with unknown teammates.

#### Cooperative MARL (CMARL).

CMARL explores algorithms for training agent teams on fully cooperative tasks. Some existing methods focus on credit assignment and decentralized control `\citep{foerster2018counterfactual, rashid18qmix}`{=latex}. Other works in CMARL also leverage parameter sharing and role assignment (e.g., `\cite{christianos2021scaling, yang2022ldsa}`{=latex}) to decide an optimal division of labor between agents. However, these techniques assume control over all existing agents during training and evaluation, which limits their effectiveness in settings with unseen or uncontrolled teammates, as shown in prior work `\citep{vezhnevets2020options, hu2020other, rahman_towards_2021}`{=latex}.

# Conclusion [sec:discussion]

This paper proposes and formulates the problem of \\(N\\)-agent ad hoc teamwork (NAHT), a generalization of both AHT and MARL. It further proposes a multi-agent reinforcement learning algorithm to train NAHT agents called POAM, and develops a procedure to train and evaluate NAHT agents. POAM is a policy gradient approach that uses an encoder-decoder architecture to perform teammate modeling, and leverages data from uncontrolled agents for policy optimization. Empirical validation on MPE and StarCraft II tasks shows that POAM consistently improves over baseline methods that naively apply existing MARL and AHT approaches, in terms of sample efficiency, asymptotic return, and generalization to out-of-distribution teammates.

#### Limitations and Future Work. 

This paper addresses a special case of the NAHT problem, with homogeneous and non-communicating agents. POAM, which employs full parameter sharing, may not perform well in settings with heterogeneous agents, or in settings that require highly differentiated roles. POAM also does not leverage centralized state information or allow communication between controlled agents. Incorporating this information might enable learning improved NAHT policies. Further, POAM’s actor update is purely on-policy, and therefore cannot leverage data generated by uncontrolled agents. Future work might consider employing off-policy methods to exploit the uncontrolled agent data. In addition to the directions suggested by POAM’s limitations, algorithmic ideas from AHT, such as diversity-based teammate generation `\citep{lupu2021trajectory}`{=latex} and teammate-model-based planning methods `\citep{barrett2017plastic}`{=latex}, also suggest rich avenues for future work. Having introduced the NAHT problem in this work, we hope the community explores the many potential directions to design even better NAHT algorithms by considering advances in MARL, AHT, and agent modeling.

<div class="ack" markdown="1">

This work has taken place in the Learning Agents Research Group (LARG) at UT Austin. LARG research is supported in part by NSF (FAIN-2019844, NRT-2125858), ONR (N00014-18-2243), ARO (W911NF-23-2-0004, W911NF-17-2-0181), Lockheed Martin, and UT Austin’s Good Systems grand challenge. Peter Stone serves as the Executive Director of Sony AI America and receives financial compensation for this work. The terms of this arrangement have been reviewed and approved by the University of Texas at Austin in accordance with its policy on objectivity in research.

</div>

# References [references]

<div class="thebibliography" markdown="1">

John P Agapiou, Alexander Sasha Vezhnevets, Edgar A Duéñez-Guzmán, Jayd Matyas, Yiran Mao, Peter Sunehag, Raphael Köster, Udari Madhushani, Kavya Kopparapu, Ramona Comanescu, et al Melting pot 2.0 *arXiv preprint arXiv:2211.13746*, 2022. **Abstract:** Multi-agent artificial intelligence research promises a path to develop intelligent technologies that are more human-like and more human-compatible than those produced by "solipsistic" approaches, which do not consider interactions between agents. Melting Pot is a research tool developed to facilitate work on multi-agent artificial intelligence, and provides an evaluation protocol that measures generalization to novel social partners in a set of canonical test scenarios. Each scenario pairs a physical environment (a "substrate") with a reference set of co-players (a "background population"), to create a social situation with substantial interdependence between the individuals involved. For instance, some scenarios were inspired by institutional-economics-based accounts of natural resource management and public-good-provision dilemmas. Others were inspired by considerations from evolutionary biology, game theory, and artificial life. Melting Pot aims to cover a maximally diverse set of interdependencies and incentives. It includes the commonly-studied extreme cases of perfectly-competitive (zero-sum) motivations and perfectly-cooperative (shared-reward) motivations, but does not stop with them. As in real-life, a clear majority of scenarios in Melting Pot have mixed incentives. They are neither purely competitive nor purely cooperative and thus demand successful agents be able to navigate the resulting ambiguity. Here we describe Melting Pot 2.0, which revises and expands on Melting Pot. We also introduce support for scenarios with asymmetric roles, and explain how to integrate them into the evaluation protocol. This report also contains: (1) details of all substrates and scenarios; (2) a complete description of all baseline algorithms and results. Our intention is for it to serve as a reference for researchers using Melting Pot 2.0. (@agapiou2022melting)

Stefano V Albrecht and Peter Stone Autonomous agents modelling other agents: A comprehensive survey and open problems *Artificial Intelligence*, 258: 66–95, 2018. **Abstract:** Much research in artiﬁcial intelligence is concerned with the development of autonomous agents that can interact e ectively with other agents. An important aspect of such agents is the ability to reason about the behaviours of other agents, by constructing models which make predictions about various properties of interest (such as actions, goals, beliefs) of the modelled agents. A variety of modelling approaches now exist which vary widely in their methodology and underlying assumptions, catering to the needs of the di erent sub-communities within which they were developed and reﬂecting the di erent practical uses for which they are intended. The purpose of the present article is to provide a comprehensive survey of the salient modelling methods which can be found in the literature. The article concludes with a discussion of open problems which may form the basis for fruitful future research. (@albrecht2018autonomous)

Stefano V. Albrecht, Filippos Christianos, and Lukas Schäfer *Multi-Agent Reinforcement Learning: Foundations and Modern Approaches* MIT Press, 2024. URL <https://www.marl-book.com>. **Abstract:** A warehouse pickup and delivery problem finds its solution using multi agent path finding (MAPF) approach. Also, the problem has been used to showcase the capabilities of the multi agent reinforcement learning (MARL). The warehouse pickup and delivery work needs the agent to pick up a requested item and successfully deliver it to the intended location within the warehouse. The problem has been solved based on two approaches that include single shot and lifelong problem solution. The single shot solution has the delivery as the final goal and thus once it reaches the delivery address, it stops whereas in case of lifelong, the agent needs to deliver the item which it had picked, deliver it to the required place and then again pick up new item until requests are satisfied. The strategy used by multi agent path finding (MAPF) approach aims at constructing collision free paths to reach the delivery location but in case of multi agent reinforcement learning (MARL), the agents’ decision making tactics (or policies) are learned which are then used to help agents decide path to be followed based on environment state and agent’s position. The results show that the lifelong conflict based search (CBS) is a better option when the agents are less in number as in that case, the re-planning will take overall less time but when the agents are large in number then this re-planning can take very long to produce conflict free paths from source to goal nodes. In this case, shared experience action critic (SEAC) which is based on multi agent reinforcement learning (MARL) approach can be more efficient choice as it takes the current environment state to give the most suitable action for that time t. For this study the agents taken for learning are homogeneous in nature that can pickup and deliver any type of requested item. We can address the same pickup and delivery problem when the agents are not all same and differ in their capabilities and the type of item they can handle. (@marl-book-albrecht24)

Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor Mordatch Emergent Tool Use From Multi-Agent Autocurricula In *International Conference on Learning Representations*, September 2019. URL <https://openreview.net/forum?id=SkxpxJBKwS>. **Abstract:** Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests. (@baker_emergent_2019)

Samuel Barrett and Peter Stone An analysis framework for ad hoc teamwork tasks In *Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems-Volume 1*, pages 357–364, 2012. **Abstract:** In multiagent team settings, the agents are often given a protocol for coordinating their actions. When such a protocol is not available, agents must engage in ad hoc teamwork to effectively cooperate with one another. A fully general ad hoc team agent needs to be capable of collaborating with a wide range of potential teammates on a varying set of joint tasks. This paper presents a framework for analyzing ad hoc team problems that sheds light on the current state of research and suggests avenues for future research. In addition, this paper shows how previous theoretical results can aid ad hoc agents in a set of testbed domains. (@barrett2012analysis)

Samuel Barrett, Avi Rosenfeld, Sarit Kraus, and Peter Stone Making friends on the fly: Cooperating with new teammates *Artificial Intelligence*, 242: 132–171, January 2017. ISSN 0004-3702. . URL <https://www.sciencedirect.com/science/article/pii/S0004370216301266>. **Abstract:** Robots are being deployed in an increasing variety of environments for longer periods of time. As the number of robots grows, they will increasingly need to interact with other robots. Additionally, the number of companies and research laboratories producing these robots is increasing, leading to the situation where these robots may not share a common communication or coordination protocol. While standards for coordination and communication may be created, we expect that robots will need to additionally reason intelligently about their teammates with limited information. This problem motivates the area of ad hoc teamwork in which an agent may potentially cooperate with a variety of teammates in order to achieve a shared goal. This article focuses on a limited version of the ad hoc teamwork problem in which an agent knows the environmental dynamics and has had past experiences with other teammates, though these experiences may not be representative of the current teammates. To tackle this problem, this article introduces a new general-purpose algorithm, PLASTIC, that reuses knowledge learned from previous teammates or provided by experts to quickly adapt to new teammates. This algorithm is instantiated in two forms: 1) PLASTIC-Model – which builds models of previous teammates’ behaviors and plans behaviors online using these models and 2) PLASTIC-Policy – which learns policies for cooperating with previous teammates and selects among these policies online. We evaluate PLASTIC on two benchmark tasks: the pursuit domain and robot soccer in the RoboCup 2D simulation domain. Recognizing that a key requirement of ad hoc teamwork is adaptability to previously unseen agents, the tests use more than 40 previously unknown teams on the first task and 7 previously unknown teams on the second. While PLASTIC assumes that there is some degree of similarity between the current and past teammates’ behaviors, no steps are taken in the experimental setup to make sure this assumption holds. The teammates were created by a variety of independent developers and were not designed to share any similarities. Nonetheless, the results show that PLASTIC was able to identify and exploit similarities between its current and past teammates’ behaviors, allowing it to quickly adapt to new teammates. (@barrett2017plastic)

Daniel S. Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein The Complexity of Decentralized Control of Markov Decision Processes *Mathematics of Operations Research*, 27 (4): 819–840, November 2002. ISSN 0364-765X. . URL <https://pubsonline.informs.org/doi/10.1287/moor.27.4.819.297>. **Abstract:** We consider decentralized control of Markov decision processes and give complexity bounds on the worst-case running time for algorithms that find optimal solutions. Generalizations of both the fully observable case and the partially observable case that allow for decentralized control are described. For even two agents, the finite-horizon problems corresponding to both of these models are hard for nondeterministic exponential time. These complexity results illustrate a fundamental difference between centralized and decentralized control of Markov decision processes. In contrast to the problems involving centralized control, the problems we consider provably do not admit polynomial-time algorithms. Furthermore, assuming EXP ≠ NEXP, the problems require superexponential time to solve in the worst case. (@bernstein_complexity_2002)

Noam Brown and Tuomas Sandholm Superhuman ai for heads-up no-limit poker: Libratus beats top professionals *Science*, 359 (6374): 418–424, 2018. . URL <https://www.science.org/doi/abs/10.1126/science.aao1733>. **Abstract:** Libratus versus humans Pitting artificial intelligence (AI) against top human players demonstrates just how far AI has come. Brown and Sandholm built a poker-playing AI called Libratus that decisively beat four leading human professionals in the two-player variant of poker called heads-up no-limit Texas hold’em (HUNL). Over nearly 3 weeks, Libratus played 120,000 hands of HUNL against the human professionals, using a three-pronged approach that included precomputing an overall strategy, adapting the strategy to actual gameplay, and learning from its opponent. Science , this issue p. 418 (@brown-libratus18)

Filippos Christianos, Georgios Papoudakis, Muhammad A Rahman, and Stefano V Albrecht Scaling multi-agent reinforcement learning with selective parameter sharing In *International Conference on Machine Learning*, pages 1989–1998. PMLR, 2021. **Abstract:** Sharing parameters in multi-agent deep reinforcement learning has played an essential role in allowing algorithms to scale to a large number of agents. Parameter sharing between agents significantly decreases the number of trainable parameters, shortening training times to tractable levels, and has been linked to more efficient learning. However, having all agents share the same parameters can also have a detrimental effect on learning. We demonstrate the impact of parameter sharing methods on training speed and converged returns, establishing that when applied indiscriminately, their effectiveness is highly dependent on the environment. We propose a novel method to automatically identify agents which may benefit from sharing parameters by partitioning them based on their abilities and goals. Our approach combines the increased sample efficiency of parameter sharing with the representational capacity of multiple independent networks to reduce training time and increase final returns. (@christianos2021scaling)

Caroline Claus and Craig Boutilier The dynamics of reinforcement learning in cooperative multiagent systems In *Proceedings of the fifteenth national/tenth conference on Artificial intelligence/Innovative applications of artificial intelligence*, AAAI ’98/IAAI ’98, pages 746–752, USA, July 1998. American Association for Artificial Intelligence. ISBN 978-0-262-51098-1. **Abstract:** Reinforcement learning can provide a robust and natural means for agents to learn how to coordinate their action choices in multi agent systems. We examine some of the factors that can influence the dynamics of the learning process in such a setting. We first distinguish reinforcement learners that are unaware of (or ignore) the presence of other agents from those that explicitly attempt to learn the value of joint actions and the strategies of their counterparts. We study (a simple form of) Q-leaming in cooperative multi agent systems under these two perspectives, focusing on the influence of that game structure and exploration strategies on convergence to (optimal and suboptimal) Nash equilibria. We then propose alternative optimistic exploration strategies that increase the likelihood of convergence to an optimal equilibrium. (@claus_dynamics_1998)

Jakob Foerster, Richard Y. Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch Learning with opponent-learning awareness In *Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems*, page 122–130. International Foundation for Autonomous Agents and Multiagent Systems, 2018. **Abstract:** Multi-agent settings are quickly gathering importance in machine learning. This includes a plethora of recent work on deep multi-agent reinforcement learning, but also can be extended to hierarchical reinforcement learning, generative adversarial networks and decentralised optimisation. In all these settings the presence of multiple learning agents renders the training problem non-stationary and often leads to unstable training or undesired final results. We present Learning with Opponent-Learning Awareness (LOLA), a method in which each agent shapes the anticipated learning of the other agents in the environment. The LOLA learning rule includes an additional term that accounts for the impact of one agent’s policy on the anticipated parameter update of the other agents. Preliminary results show that the encounter of two LOLA agents leads to the emergence of tit-for-tat and therefore cooperation in the iterated prisoners’ dilemma (IPD), while independent learning does not. In this domain, LOLA also receives higher payouts compared to a naive learner, and is robust against exploitation by higher order gradient-based methods. Applied to infinitely repeated matching pennies, LOLA agents converge to the Nash equilibrium. In a round robin tournament we show that LOLA agents can successfully shape the learning of a range of multi-agent learning algorithms from literature, resulting in the highest average returns on the IPD. We also show that the LOLA update rule can be efficiently calculated using an extension of the likelihood ratio policy gradient estimator, making the method suitable for model-free reinforcement learning. This method thus scales to large parameter and input spaces and nonlinear function approximators. We also apply LOLA to a grid world task with an embedded social dilemma using deep recurrent policies and opponent modelling. Again, by explicitly considering the learning of the other agent, LOLA agents learn to cooperate out of self-interest. (@foerster2018lola)

Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson Counterfactual multi-agent policy gradients In *Proceedings of the AAAI conference on artificial intelligence*, volume 32, 2018. **Abstract:** Many real-world problems, such as network packet routing and the coordination of autonomous vehicles, are naturally modelled as cooperative multi-agent systems. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents’ policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent’s action, while keeping the other agents’ actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state. (@foerster2018counterfactual)

Josiah P Hanna, Arrasy Rahman, Elliot Fosong, Francisco Eiras, Mihai Dobre, John Redford, Subramanian Ramamoorthy, and Stefano V Albrecht Interpretable goal recognition in the presence of occluded factors for autonomous vehicles In *2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*, pages 7044–7051. IEEE, 2021. **Abstract:** Recognising the goals or intentions of observed vehicles is a key step towards predicting the long-term future behaviour of other agents in an autonomous driving scenario. When there are unseen obstacles or occluded vehicles in a scenario, goal recognition may be confounded by the effects of these unseen entities on the behaviour of observed vehicles. Existing prediction algorithms that assume rational behaviour with respect to inferred goals may fail to make accurate long-horizon predictions because they ignore the possibility that the behaviour is influenced by such unseen entities. We introduce the Goal and Occluded Factor Inference (GOFI) algorithm which bases inference on inverse-planning to jointly infer a probabilistic belief over goals and potential occluded factors. We then show how these beliefs can be integrated into Monte Carlo Tree Search (MCTS). We demonstrate that jointly inferring goals and occluded factors leads to more accurate beliefs with respect to the true world state and allows an agent to safely navigate several scenarios where other baselines take unsafe actions leading to collisions. (@hanna2021interpretable)

Pablo Hernandez-Leal, Bilal Kartal, and Matthew E. Taylor A Survey and Critique of Multiagent Deep Reinforcement Learning *Autonomous Agents and Multi-Agent Systems*, 33 (6): 750–797, November 2019. ISSN 1387-2532, 1573-7454. . URL <http://arxiv.org/abs/1810.05587>. arXiv:1810.05587 \[cs\]. **Abstract:** Deep reinforcement learning (RL) has achieved outstanding results in recent years. This has led to a dramatic increase in the number of applications and methods. Recent works have explored learning beyond single-agent scenarios and have considered multiagent learning (MAL) scenarios. Initial results report successes in complex multiagent domains, although there are several challenges to be addressed. The primary goal of this article is to provide a clear overview of current multiagent deep reinforcement learning (MDRL) literature. Additionally, we complement the overview with a broader analysis: (i) we revisit previous key components, originally presented in MAL and RL, and highlight how they have been adapted to multiagent deep reinforcement learning settings. (ii) We provide general guidelines to new practitioners in the area: describing lessons learned from MDRL works, pointing to recent benchmarks, and outlining open avenues of research. (iii) We take a more critical tone raising practical challenges of MDRL (e.g., implementation and computational demands). We expect this article will help unify and motivate future research to take advantage of the abundant literature that exists (e.g., RL and MAL) in a joint effort to promote fruitful research in the multiagent community. (@hernandez-leal_dmarl-survey_2019)

Hengyuan Hu and Jakob N. Foerster Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning September 2019. URL <https://openreview.net/forum?id=B1xm3RVtwB>. **Abstract:** In recent years we have seen fast progress on a number of benchmark problems in AI, with modern methods achieving near or super human performance in Go, Poker and Dota. One common aspect of all of these challenges is that they are by design adversarial or, technically speaking, zero-sum. In contrast to these settings, success in the real world commonly requires humans to collaborate and communicate with others, in settings that are, at least partially, cooperative. In the last year, the card game Hanabi has been established as a new benchmark environment for AI to fill this gap. In particular, Hanabi is interesting to humans since it is entirely focused on theory of mind, i.e., the ability to effectively reason over the intentions, beliefs and point of view of other agents when observing their actions. Learning to be informative when observed by others is an interesting challenge for Reinforcement Learning (RL): Fundamentally, RL requires agents to explore in order to discover good policies. However, when done naively, this randomness will inherently make their actions less informative to others during training. We present a new deep multi-agent RL method, the Simplified Action Decoder (SAD), which resolves this contradiction exploiting the centralized training phase. During training SAD allows other agents to not only observe the (exploratory) action chosen, but agents instead also observe the greedy action of their team mates. By combining this simple intuition with best practices for multi-agent learning, SAD establishes a new SOTA for learning methods for 2-5 players on the self-play part of the Hanabi challenge. Our ablations show the contributions of SAD compared with the best practice components. All of our code and trained agents are available at https://github.com/facebookresearch/Hanabi_SAD. (@hu_simplified_2019)

Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster “other-play” for zero-shot coordination In *International Conference on Machine Learning*, pages 4399–4410. PMLR, 2020. **Abstract:** We consider the problem of zero-shot coordination - constructing AI agents that can coordinate with novel partners they have not seen before (e.g. humans). Standard Multi-Agent Reinforcement Learning (MARL) methods typically focus on the self-play (SP) setting where agents construct strategies by playing the game with themselves repeatedly. Unfortunately, applying SP naively to the zero-shot coordination problem can produce agents that establish highly specialized conventions that do not carry over to novel partners they have not been trained with. We introduce a novel learning algorithm called other-play (OP), that enhances self-play by looking for more robust strategies, exploiting the presence of known symmetries in the underlying problem. We characterize OP theoretically as well as experimentally. We study the cooperative card game Hanabi and show that OP agents achieve higher scores when paired with independently trained agents. In preliminary results we also show that our OP agents obtains higher average scores when paired with human players, compared to state-of-the-art SP agents. (@hu2020other)

Jiechuan Jiang, Chen Dun, Tiejun Huang, and Zongqing Lu Graph convolutional reinforcement learning In *International Conference on Learning Representations*, 2019. **Abstract:** Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios. (@jiang2019graph)

Anirudh Kakarlapudi, Gayathri Anil, Adam Eck, Prashant Doshi, and Leen-Kiat Soh Decision-theoretic planning with communication in open multiagent systems In *Uncertainty in Artificial Intelligence*, pages 938–948. PMLR, 2022. (@kakarlapudi2022decision)

Raphael Koster, Miruna Pîslar, Andrea Tacchetti, Jan Balaguer, Leqi Liu, Romuald Elie, Oliver P Hauser, Karl Tuyls, Matt Botvinick, and Christopher Summerfield Using deep reinforcement learning to promote sustainable human behaviour on a common pool resource problem 2024. URL <https://arxiv.org/ftp/arxiv/papers/2404/2404.15059.pdf>. **Abstract:** A canonical social dilemma arises when finite resources are allocated to a group of people, who can choose to either reciprocate with interest, or keep the proceeds for themselves. What resource allocation mechanisms will encourage levels of reciprocation that sustain the commons? Here, in an iterated multiplayer trust game, we use deep reinforcement learning (RL) to design an allocation mechanism that endogenously promotes sustainable contributions from human participants to a common pool resource. We first trained neural networks to behave like human players, creating a stimulated economy that allowed us to study how different mechanisms influenced the dynamics of receipt and reciprocation. We then used RL to train a social planner to maximise aggregate return to players. The social planner discovered a redistributive policy that led to a large surplus and an inclusive economy, in which players made roughly equal gains. The RL agent increased human surplus over baseline mechanisms based on unrestricted welfare or conditional cooperation, by conditioning its generosity on available resources and temporarily sanctioning defectors by allocating fewer resources to them. Examining the AI policy allowed us to develop an explainable mechanism that performed similarly and was more popular among players. Deep reinforcement learning can be used to discover mechanisms that promote sustainable human behaviour. (@koster_commonpool_2024)

Joel Z Leibo, Edgar A Dueñez-Guzman, Alexander Vezhnevets, John P Agapiou, Peter Sunehag, Raphael Koster, Jayd Matyas, Charlie Beattie, Igor Mordatch, and Thore Graepel Scalable evaluation of multi-agent reinforcement learning with melting pot In *International conference on machine learning*, pages 6187–6199. PMLR, 2021. **Abstract:** Existing evaluation suites for multi-agent reinforcement learning (MARL) do not assess generalization to novel situations as their primary objective (unlike supervised-learning benchmarks). Our contribution, Melting Pot, is a MARL evaluation suite that fills this gap, and uses reinforcement learning to reduce the human labor required to create novel test scenarios. This works because one agent’s behavior constitutes (part of) another agent’s environment. To demonstrate scalability, we have created over 80 unique test scenarios covering a broad range of research topics such as social dilemmas, reciprocity, resource sharing, and task partitioning. We apply these test scenarios to standard MARL training algorithms, and demonstrate how Melting Pot reveals weaknesses not apparent from training performance alone. (@leibo2021scalable)

Fanqi Lin, Shiyu Huang, Tim Pearce, Wenze Chen, and Wei-Wei Tu : Mastering Multi-Agent Football with Curriculum Learning and Self-Play In *Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems*, AAMAS ’23, pages 67–76, Richland, SC, May 2023. International Foundation for Autonomous Agents and Multiagent Systems. ISBN 978-1-4503-9432-1. **Abstract:** Multi-agent football poses an unsolved challenge in AI research. Existing work has focused on tackling simplified scenarios of the game, or else leveraging expert demonstrations. In this paper, we develop a multi-agent system to play the full 11 vs. 11 game mode, without demonstrations. This game mode contains aspects that present major challenges to modern reinforcement learning algorithms; multi-agent coordination, long-term planning, and non-transitivity. To address these challenges, we present TiZero; a self-evolving, multi-agent system that learns from scratch. TiZero introduces several innovations, including adaptive curriculum learning, a novel self-play strategy, and an objective that optimizes the policies of multiple agents jointly. Experimentally, it outperforms previous systems by a large margin on the Google Research Football environment, increasing win rates by over 30%. To demonstrate the generality of TiZero’s innovations, they are assessed on several environments beyond football; Overcooked, Multi-agent Particle-Environment, Tic-Tac-Toe and Connect-Four. (@lin_tizero_2023)

Michael L Littman Markov games as a framework for multi-agent reinforcement learning In *Machine learning proceedings 1994*, pages 157–163. Elsevier, 1994. **Abstract:** A central problem in the theory of multi-agent reinforcement learning (MARL) is to understand what structural conditions and algorithmic principles lead to sample-efficient learning guarantees, and how these considerations change as we move from few to many agents. We study this question in a general framework for interactive decision making with multiple agents, encompassing Markov games with function approximation and normal-form games with bandit feedback. We focus on equilibrium computation, in which a centralized learning algorithm aims to compute an equilibrium by controlling multiple agents that interact with an unknown environment. Our main contributions are: - We provide upper and lower bounds on the optimal sample complexity for multi-agent decision making based on a multi-agent generalization of the Decision-Estimation Coefficient, a complexity measure introduced by Foster et al. (2021) in the single-agent counterpart to our setting. Compared to the best results for the single-agent setting, our bounds have additional gaps. We show that no"reasonable"complexity measure can close these gaps, highlighting a striking separation between single and multiple agents. - We show that characterizing the statistical complexity for multi-agent decision making is equivalent to characterizing the statistical complexity of single-agent decision making, but with hidden (unobserved) rewards, a framework that subsumes variants of the partial monitoring problem. As a consequence, we characterize the statistical complexity for hidden-reward interactive decision making to the best extent possible. Building on this development, we provide several new structural results, including 1) conditions under which the statistical complexity of multi-agent decision making can be reduced to that of single-agent, and 2) conditions under which the so-called curse of multiple agents can be avoided. (@littman1994markov)

Bo Liu, Qiang Liu, Peter Stone, Animesh Garg, Yuke Zhu, and Anima Anandkumar Coach-player multi-agent reinforcement learning for dynamic team composition In *International Conference on Machine Learning*, pages 6860–6870. PMLR, 2021. **Abstract:** In real-world multi-agent systems, agents with different capabilities may join or leave without altering the team’s overarching goals. Coordinating teams with such dynamic composition is challenging: the optimal team strategy varies with the composition. We propose COPA, a coach-player framework to tackle this problem. We assume the coach has a global view of the environment and coordinates the players, who only have partial views, by distributing individual strategies. Specifically, we 1) adopt the attention mechanism for both the coach and the players; 2) propose a variational objective to regularize learning; and 3) design an adaptive communication method to let the coach decide when to communicate with the players. We validate our methods on a resource collection task, a rescue game, and the StarCraft micromanagement tasks. We demonstrate zero-shot generalization to new team compositions. Our method achieves comparable or better performance than the setting where all players have a full view of the environment. Moreover, we see that the performance remains high even when the coach communicates as little as 13% of the time using the adaptive communication strategy. (@liu2021coach)

Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch Multi-agent actor-critic for mixed cooperative-competitive environments *Neural Information Processing Systems (NIPS)*, 2017. **Abstract:** We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies. (@lowe2017multi)

Andrei Lupu, Brandon Cui, Hengyuan Hu, and Jakob Foerster Trajectory diversity for zero-shot coordination In *International conference on machine learning*, pages 7204–7213. PMLR, 2021. **Abstract:** We study the problem of zero-shot coordination (ZSC), where agents must independently produce strategies for a collaborative game that are com- patible with novel partners not seen during train- ing. Our ﬁrst contribution is to consider the need for diversity in generating such agents. Because self-play (SP) agents control their own trajectory distribution during training, each policy typically only performs well on this exact distribution. As a result, they achieve low scores in ZSC, since playing with another agent is likely to put them in situations they have not encountered during training. To address this issue, we train a com- mon best response (BR) to a population of agents, which we regulate to be diverse. To this end, we introduce Trajectory Diversity (TrajeDi) – a differ- entiable objective for generating diverse reinforce- ment learning policies. We derive TrajeDi as a generalization of the Jensen-Shannon divergence between policies and motivate it experimentally in two simple settings. We then focus on the col- laborative card game Hanabi, demonstrating the scalability of our method and improving upon the cross-play scores of both independently trained SP agents and BRs to unregularized populations. (@lupu2021trajectory)

Patrick MacAlpine and Peter Stone Evaluating ad hoc teamwork performance in drop-in player challenges In Gita Sukthankar and Juan A. Rodriguez-Aguilar, editors, *Autonomous Agents and Multiagent Systems, AAMAS 2017 Workshops, Best Papers*, pages 168–186. Springer International Publishing, 2017. URL <http://nn.cs.utexas.edu/?LNAI17-MacAlpine>. (@LNAI17-MacAlpine)

Reuth Mirsky, Ignacio Carlucho, Arrasy Rahman, Elliot Fosong, William Macke, Mohan Sridharan, Peter Stone, and Stefano V Albrecht A survey of ad hoc teamwork research In *European Conference on Multi-Agent Systems*, pages 275–293. Springer, 2022. **Abstract:** Ad hoc teamwork is the research problem of designing agents t hat can collaborate with new teammates without prior coordination . This survey makes a two-fold contribution: First, it provides a structured de scription of the different facets of the ad hoc teamwork problem. Second, it discusses t he progress that has been made in the ﬁeld so far, and identiﬁes the immediate and l ong-term open problems that need to be addressed in ad hoc teamwork. (@mirsky2022survey)

Liviu Panait and Sean Luke Cooperative multi-agent learning: The state of the art *Autonomous Agents and Multi-Agent Systems*, 11: 387–434, 2005. URL <https://api.semanticscholar.org/CorpusID:19706>. **Abstract:** Cooperative multi-agent systems are ones in which several a gents attempt, through their interaction, to jointly solve tasks or to maximize utility. Due to the interactions a mong the agents, multi-agent problem complexity can rise rapidly with the number of agents or their behavioral so phistication. The challenge this presents to the task of programming solutions to multi-agent systems problems h as spawned increasing interest in machine learning techniques toautomate the search andoptimization process . We provide a broad survey of the cooperative multi-agent lea rning literature. Previous surveys of this area have largely focused on issues common to speciﬁc subareas (for ex ample, reinforcement learning or robotics). In this survey we attempt to draw from multi-agent learning work in a spectrum of areas, including reinforcement learning, evolutionary computation, game theory, complex systems, a gent modeling, and robotics. We ﬁnd that this broad view leads to a division of the work into two categories, each with its own special is- sues: applying a single learner todiscover joint solutions tomulti-agent problems ( team learning ), or using multiple simultaneouslearners,oftenoneperagent( concurrentlearning ). Additionally,wediscussdirectandindirectcommu- nication inconnection withlearning, plus open issues inta sk decomposition, scalability, and adaptive dynamics. We conclude withapresentation of multi-agent learning probl em domains, and a listof multi-agent learningresources. 1 Introduction Inrecentyearstherehasbeenincreasedinterestin decentr alizedapproachesto solvingcomplexreal-worldproblems. Many such approachesfall into the area of distributed systems, where a numberof entities work together to coopera- tivelysolveproblems. Thecombinationofdistributedsyst emsandartiﬁcial intelligence(AI)is collectivelyknowna s distributedartiﬁcialintelligence (DAI).Traditionally,DAIisdividedintotwoareas. Theﬁrs tarea,distributedproblem solving, is usually concerned with the decomposition and distribut ion of a problem solving process among multiple slave nodes,andthe collectiveconstructionofa solutiont othe problem. Thesecondclassof approaches, multi-agent systems(MAS),emphasizesthejointbehaviorsofagentswith somede greeofautonomyandthe complexitiesarising fromtheirinteractions. In this survey, we will focus on the application of machine learning to problems in the MAS area. Machine learningexploreswaystoautomatetheinductiveprocess: g ettingamachineagenttodiscovero (@Panait2005CooperativeML)

Georgios Papoudakis, Filippos Christianos, and Stefano V. Albrecht Agent modelling under partial observability for deep reinforcement learning In *Advances in Neural Information Processing Systems*, 2021. **Abstract:** Modelling the behaviours of other agents is essential for understanding how agents interact and making effective decisions. Existing methods for agent modelling commonly assume knowledge of the local observations and chosen actions of the modelled agents during execution. To eliminate this assumption, we extract representations from the local information of the controlled agent using encoder-decoder architectures. Using the observations and actions of the modelled agents during training, our models learn to extract representations about the modelled agents conditioned only on the local observations of the controlled agent. The representations are used to augment the controlled agent’s decision policy which is trained via deep reinforcement learning; thus, during execution, the policy does not require access to other agents’ information. We provide a comprehensive evaluation and ablations studies in cooperative, competitive and mixed multi-agent environments, showing that our method achieves higher returns than baseline methods which do not use the learned representations. (@papoudakis2020liam)

Georgios Papoudakis, Filippos Christianos, Lukas Schäfer, and Stefano V. Albrecht Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks In *Advances in Neural Information Processing Systems*, volume 34, 2021. **Abstract:** Multi-agent deep reinforcement learning (MARL) suffers from a lack of commonly-used evaluation tasks and criteria, making comparisons between approaches difficult. In this work, we provide a systematic evaluation and comparison of three different classes of MARL algorithms (independent learning, centralised multi-agent policy gradient, value decomposition) in a diverse range of cooperative multi-agent learning tasks. Our experiments serve as a reference for the expected performance of algorithms across different learning tasks, and we provide insights regarding the effectiveness of different learning approaches. We open-source EPyMARL, which extends the PyMARL codebase to include additional algorithms and allow for flexible configuration of algorithm implementation details such as parameter sharing. Finally, we open-source two environments for multi-agent research which focus on coordination under sparse rewards. (@papoudakis_benchmarking_2021)

Arrasy Rahman, Niklas Höpner, Filippos Christianos, and Stefano V. Albrecht Towards Open Ad Hoc Teamwork Using Graph-based Policy Learning In *Proceedings of the 38 th International Conference on Machine Learning*, volume 139. PMLR, June 2021. **Abstract:** Ad hoc teamwork is the challenging problem of designing an autonomous agent which can adapt quickly to collaborate with teammates without prior coordination mechanisms, including joint training. Prior work in this area has focused on closed teams in which the number of agents is fixed. In this work, we consider open teams by allowing agents with different fixed policies to enter and leave the environment without prior notification. Our solution builds on graph neural networks to learn agent models and joint-action value models under varying team compositions. We contribute a novel action-value computation that integrates the agent model and joint-action value model to produce action-value estimates. We empirically demonstrate that our approach successfully models the effects other agents have on the learner, leading to policies that robustly adapt to dynamic team compositions and significantly outperform several alternative methods. (@rahman_towards_2021)

Arrasy Rahman, Ignacio Carlucho, Niklas HÃ¶pner, and Stefano V. Albrecht A general learning framework for open ad hoc teamwork using graph-based policy learning *Journal of Machine Learning Research*, 24 (298): 1–74, 2023. URL <http://jmlr.org/papers/v24/22-099.html>. **Abstract:** Open ad hoc teamwork is the problem of training a single agent to efficiently collaborate with an unknown group of teammates whose composition may change over time. A variable team composition creates challenges for the agent, such as the requirement to adapt to new team dynamics and dealing with changing state vector sizes. These challenges are aggravated in real-world applications in which the controlled agent only has a partial view of the environment. In this work, we develop a class of solutions for open ad hoc teamwork under full and partial observability. We start by developing a solution for the fully observable case that leverages graph neural network architectures to obtain an optimal policy based on reinforcement learning. We then extend this solution to partially observable scenarios by proposing different methodologies that maintain belief estimates over the latent environment states and team composition. These belief estimates are combined with our solution for the fully observable case to compute an agent’s optimal policy under partial observability in open ad hoc teamwork. Empirical results demonstrate that our solution can learn efficient policies in open ad hoc teamwork in fully and partially observable cases. Further analysis demonstrates that our methods’ success is a result of effectively learning the effects of teammates’ actions while also inferring the inherent state of the environment under partial observability. (@rahman2023general)

Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson : Monotonic value function factorisation for deep multi-agent reinforcement learning In *Proceedings of the 35th International Conference on Machine Learning*, volume 80 of *Proceedings of Machine Learning Research*. PMLR, 2018. **Abstract:** In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods. (@rashid18qmix)

Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr, Jakob Foerster, and Shimon Whiteson StarCraft Multi-Agent Challenge *CoRR*, abs/1902.04043, 2019. **Abstract:** In the last few years, deep multi-agent reinforcement learning (RL) has become a highly active area of research. A particularly challenging class of problems in this area is partially observable, cooperative, multi-agent learning, in which teams of agents must learn to coordinate their behaviour while conditioning only on their private observations. This is an attractive research area since such problems are relevant to a large number of real-world systems and are also more amenable to evaluation than general-sum problems. Standardised environments such as the ALE and MuJoCo have allowed single-agent RL to move beyond toy domains, such as grid worlds. However, there is no comparable benchmark for cooperative multi-agent RL. As a result, most papers in this field use one-off toy problems, making it difficult to measure real progress. In this paper, we propose the StarCraft Multi-Agent Challenge (SMAC) as a benchmark problem to fill this gap. SMAC is based on the popular real-time strategy game StarCraft II and focuses on micromanagement challenges where each unit is controlled by an independent agent that must act based on local observations. We offer a diverse set of challenge maps and recommendations for best practices in benchmarking and evaluations. We also open-source a deep multi-agent RL learning framework including state-of-the-art algorithms. We believe that SMAC can provide a standard benchmark environment for years to come. Videos of our best agents for several SMAC scenarios are available at: https://youtu.be/VZ7zmQ_obZ0. (@samvelyan19smac)

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov Proximal policy optimization algorithms *ArXiv*, abs/1707.06347, 2017. URL <https://api.semanticscholar.org/CorpusID:28695052>. **Abstract:** We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time. (@Schulman2017ProximalPO)

Macheng Shen and Jonathan P. How Robust opponent modeling via adversarial ensemble reinforcement learning in asymmetric imperfect-information games In *Proceedings of the Thirty-First International Conference on Automated Planning and Scheduling*, 2021. **Abstract:** This paper presents an algorithmic framework for learning robust policies in asymmetric imperfect-information games, where the joint reward could depend on the uncertain opponent type (a private information known only to the opponent itself and its ally). In order to maximize the reward, the protagonist agent has to infer the opponent type through agent modeling. We use multiagent reinforcement learning (MARL) to learn opponent models through self-play, which captures the full strategy interaction and reasoning between agents. However, agent policies learned from self-play can suffer from mutual overfitting. Ensemble training methods can be used to improve the robustness of agent policy against different opponents, but it also significantly increases the computational overhead. In order to achieve a good trade-off between the robustness of the learned policy and the computation complexity, we propose to train a separate opponent policy against the protagonist agent for evaluation purposes. The reward achieved by this opponent is a noisy measure of the robustness of the protagonist agent policy due to the intrinsic stochastic nature of a reinforcement learner. To handle this stochasticity, we apply a stochastic optimization scheme to dynamically update the opponent ensemble to optimize an objective function that strikes a balance between robustness and computation complexity. We empirically show that, under the same limited computational budget, the proposed method results in more robust policy learning than standard ensemble training. (@Shen2019RobustOM)

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al Mastering the game of go with deep neural networks and tree search *nature*, 529 (7587): 484–489, 2016. (@silver2016mastering)

Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi : Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning In *Proceedings of the 36th International Conference on Machine Learning*, pages 5887–5896. PMLR, May 2019. URL <https://proceedings.mlr.press/v97/son19a.html>. **Abstract:** We explore value-based solutions for multi-agent reinforcement learning (MARL) tasks in the centralized training with decentralized execution (CTDE) regime popularized recently. However, VDN and QMIX are representative examples that use the idea of factorization of the joint action-value function into individual ones for decentralized execution. VDN and QMIX address only a fraction of factorizable MARL tasks due to their structural constraint in factorization such as additivity and monotonicity. In this paper, we propose a new factorization method for MARL, QTRAN, which is free from such structural constraints and takes on a new approach to transforming the original joint action-value function into an easily factorizable one, with the same optimal actions. QTRAN guarantees more general factorization than VDN or QMIX, thus covering a much wider class of MARL tasks than does previous methods. Our experiments for the tasks of multi-domain Gaussian-squeeze and modified predator-prey demonstrate QTRAN’s superior performance with especially larger margins in games whose payoffs penalize non-cooperative behavior more aggressively. (@son_qtran_2019)

Peter Stone, Gal Kaminka, Sarit Kraus, and Jeffrey Rosenschein Ad Hoc Autonomous Agent Teams: Collaboration without Pre-Coordination In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 24, pages 1504–1509, July 2010. . URL <https://ojs.aaai.org/index.php/AAAI/article/view/7529>. **Abstract:** As autonomous agents proliferate in the real world, both in software and robotic settings, they will increasingly need to band together for cooperative activities with previously unfamiliar teammates. In such ad hoc team settings, team strategies cannot be developed a priori. Rather, an agent must be prepared to cooperate with many types of teammates: it must collaborate without pre-coordination. This paper challenges the AI community to develop theory and to implement prototypes of ad hoc team agents. It defines the concept of ad hoc team agents, specifies an evaluation paradigm, and provides examples of possible theoretical and empirical approaches to challenge. The goal is to encourage progress towards this ambitious, newly realistic, and increasingly important research goal. (@stone2010ad)

DJ Strouse, Kevin McKee, Matt Botvinick, Edward Hughes, and Richard Everett Collaborating with Humans without Human Data In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. Wortman Vaughan, editors, *Advances in Neural Information Processing Systems*, volume 34, pages 14502–14515, 2021. URL <https://proceedings.neurips.cc/paper_files/paper/2021/file/797134c3e42371bb4979a462eb2f042a-Paper.pdf>. **Abstract:** Collaborating with humans requires rapidly adapting to their individual strengths, weaknesses, and preferences. Unfortunately, most standard multi-agent reinforcement learning techniques, such as self-play (SP) or population play (PP), produce agents that overfit to their training partners and do not generalize well to humans. Alternatively, researchers can collect human data, train a human model using behavioral cloning, and then use that model to train "human-aware" agents ("behavioral cloning play", or BCP). While such an approach can improve the generalization of agents to new human co-players, it involves the onerous and expensive step of collecting large amounts of human data first. Here, we study the problem of how to train agents that collaborate well with human partners without using human data. We argue that the crux of the problem is to produce a diverse set of training partners. Drawing inspiration from successful multi-agent approaches in competitive domains, we find that a surprisingly simple approach is highly effective. We train our agent partner as the best response to a population of self-play agents and their past checkpoints taken throughout training, a method we call Fictitious Co-Play (FCP). Our experiments focus on a two-player collaborative cooking simulator that has recently been proposed as a challenge problem for coordination with humans. We find that FCP agents score significantly higher than SP, PP, and BCP when paired with novel agent and human partners. Furthermore, humans also report a strong subjective preference to partnering with FCP agents over all baselines. (@strouse_fcp_2022)

Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Graepel Value-decomposition networks for cooperative multi-agent learning based on team reward In *Proceedings of the 17th International Conference on Autonomous Agents and Multi Agent Systems*, AAMAS ’18, 2018. **Abstract:** We study the problem of cooperative multi-agent reinforcement learning with a single joint reward signal. This class of learning problems is difficult because of the often large combined action and observation spaces. In the fully centralized and decentralized approaches, we find the problem of spurious rewards and a phenomenon we call the lazy agent” problem, which arises due to partial observability. We address these problems by training individual agents with a novel value-decomposition network architecture, which learns to decompose the team value function into agent-wise value functions. (@sunehag18vdn)

Ming Tan Multi-agent reinforcement learning: Independent versus cooperative agents In *International Conference on Machine Learning*, 1997. URL <https://api.semanticscholar.org/CorpusID:267858156>. (@Tan1997MultiAgentRL)

Alexander Vezhnevets, Yuhuai Wu, Maria Eckstein, Rémi Leblond, and Joel Z Leibo Options as responses: Grounding behavioural hierarchies in multi-agent reinforcement learning In *International Conference on Machine Learning*, pages 9733–9742. PMLR, 2020. **Abstract:** This paper investigates generalisation in multi- agent games, where the generality of the agent can be evaluated by playing against opponents it hasn’t seen during training. We propose two new games with concealed information and complex, non-transitive reward structure (think rock/paper/scissors). It turns out that most current deep reinforcement learning methods fail to efﬁ- ciently explore the strategy space, thus learning policies that generalise poorly to unseen oppo- nents. We then propose a novel hierarchical agent architecture, where the hierarchy is grounded in the game-theoretic structure of the game – the top level chooses strategic responses to opponents, while the low level implements them into policy over primitive actions. This grounding facilitates credit assignment across the levels of hierarchy. Our experiments show that the proposed hierar- chical agent is capable of generalisation to unseen opponents, while conventional baselines fail to generalise whatsoever. (@vezhnevets2020options)

Mingyu Yang, Jian Zhao, Xunhan Hu, Wengang Zhou, Jiangcheng Zhu, and Houqiang Li Ldsa: Learning dynamic subtask assignment in cooperative multi-agent reinforcement learning *Advances in Neural Information Processing Systems*, 35: 1698–1710, 2022. **Abstract:** Cooperative multi-agent reinforcement learning (MARL) has made prominent progress in recent years. For training efficiency and scalability, most of the MARL algorithms make all agents share the same policy or value network. However, in many complex multi-agent tasks, different agents are expected to possess specific abilities to handle different subtasks. In those scenarios, sharing parameters indiscriminately may lead to similar behavior across all agents, which will limit the exploration efficiency and degrade the final performance. To balance the training complexity and the diversity of agent behavior, we propose a novel framework to learn dynamic subtask assignment (LDSA) in cooperative MARL. Specifically, we first introduce a subtask encoder to construct a vector representation for each subtask according to its identity. To reasonably assign agents to different subtasks, we propose an ability-based subtask selection strategy, which can dynamically group agents with similar abilities into the same subtask. In this way, agents dealing with the same subtask share their learning of specific abilities and different subtasks correspond to different specific abilities. We further introduce two regularizers to increase the representation difference between subtasks and stabilize the training by discouraging agents from frequently changing subtasks, respectively. Empirical results show that LDSA learns reasonable and effective subtask assignment for better collaboration and significantly improves the learning performance on the challenging StarCraft II micromanagement benchmark and Google Research Football. (@yang2022ldsa)

Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu The surprising effectiveness of mappo in cooperative multi-agent games In *Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks*, 2022. **Abstract:** Proximal Policy Optimization (PPO) is a popular on-policy reinforcement learning algorithm but is significantly less utilized than off-policy learning algorithms in multi-agent settings. This is often due the belief that on-policy methods are significantly less sample efficient than their off-policy counterparts in multi-agent problems. In this work, we investigate Multi-Agent PPO (MAPPO), a variant of PPO which is specialized for multi-agent settings. Using a 1-GPU desktop, we show that MAPPO achieves surprisingly strong performance in three popular multi-agent testbeds: the particle-world environments, the Starcraft multi-agent challenge, and the Hanabi challenge, with minimal hyperparameter tuning and without any domain-specific algorithmic modifications or architectures. In the majority of environments, we find that compared to off-policy baselines, MAPPO achieves strong results while exhibiting comparable sample efficiency. Finally, through ablation studies, we present the implementation and algorithmic factors which are most influential to MAPPO’s practical performance. (@yu2021surprising)

Lei Yuan, Ziqian Zhang, Lihe Li, Cong Guan, and Yang Yu A survey of progress on cooperative multi-agent reinforcement learning in open environment *arXiv preprint arXiv:2312.01058*, 2023. **Abstract:** Multi-agent Reinforcement Learning (MARL) has gained wide attention in recent years and has made progress in various fields. Specifically, cooperative MARL focuses on training a team of agents to cooperatively achieve tasks that are difficult for a single agent to handle. It has shown great potential in applications such as path planning, autonomous driving, active voltage control, and dynamic algorithm configuration. One of the research focuses in the field of cooperative MARL is how to improve the coordination efficiency of the system, while research work has mainly been conducted in simple, static, and closed environment settings. To promote the application of artificial intelligence in real-world, some research has begun to explore multi-agent coordination in open environments. These works have made progress in exploring and researching the environments where important factors might change. However, the mainstream work still lacks a comprehensive review of the research direction. In this paper, starting from the concept of reinforcement learning, we subsequently introduce multi-agent systems (MAS), cooperative MARL, typical methods, and test environments. Then, we summarize the research work of cooperative MARL from closed to open environments, extract multiple research directions, and introduce typical works. Finally, we summarize the strengths and weaknesses of the current research, and look forward to the future development direction and research problems in cooperative MARL in open environments. (@yuan2023survey)

Luisa Zintgraf, Sam Devlin, Kamil Ciosek, Shimon Whiteson, and Katja Hofmann Deep interactive bayesian reinforcement learning via meta-learning *arXiv preprint arXiv:2101.03864*, 2021. **Abstract:** Agents that interact with other agents often do not know a priori what the other agents’ strategies are, but have to maximise their own online return while interacting with and learning about others. The optimal adaptive behaviour under uncertainty over the other agents’ strategies w.r.t. some prior can in principle be computed using the Interactive Bayesian Reinforcement Learning framework. Unfortunately, doing so is intractable in most settings, and existing approximation methods are restricted to small tasks. To overcome this, we propose to meta-learn approximate belief inference and Bayes-optimal behaviour for a given prior. To model beliefs over other agents, we combine sequential and hierarchical Variational Auto-Encoders, and meta-train this inference model alongside the policy. We show empirically that our approach outperforms existing methods that use a model-free approach, sample from the approximate posterior, maintain memory-free models of others, or do not fully utilise the known structure of the environment. (@zintgraf2021deep)

</div>

# Appendix [sec:appendix]

## An Example of the Team Sampling Procedure [app:team_sampling_procedure]

Let \\(g(k, \mu, s)\\) represent a function for randomly selecting \\(k\\) elements from a generic set \\(\mu\\), conditioned on a state \\(s \in \mathcal{S}\\). For instance, \\(g\\) might be the distribution, *Multinomial*\\((k, |\mu|, \bm{\phi}(s))\\), for selecting \\(k\\) elements with replacement from the set \\(\mu\\), parameterized by the state-dependent probability logits \\(\bm{\phi}(s) \in [0, 1]^{|\mu|}\\). Allowing \\(g\\) to depend on an initial state enriches the representable open interactions. Returning to our robot warehouse example, a state-dependent \\(g\\) might enable us to model that robots suitable for heavy loads are more likely to be present near the loading dock area. Similarly, if the time is included in the state, we would be able to model dynamic characteristics, e.g. that certain types of robots are only available during regular working hours, when humans are available to supervise. Of course, in simple cases, \\(g\\) might be state-independent. For \\(s \in \mathcal{S}\\), \\(X\\) is a sampling procedure parameterized by the tuple \\((s, M, U, C, \bm{\phi}_M, g_U, g_C)\\), where \\(\bm{\phi}_M \in [0, 1]^M\\) represents the logits of a categorical distribution:

1.  Sample an integer \\(N\\) from *Cat*\\((\bm{\phi}_M)\\).

2.  Sample \\(N\\) controlled agents via \\(g_U\\) from \\(U\\).

3.  Sample \\(M-N\\) uncontrolled agents via \\(g_C\\) from \\(C\\).

Note that the agent sampling functions \\(g_U, g_C\\) could employ sampling with replacement to model scenarios where two robots may use the same policy, or sampling without replacement for scenarios where that is not possible (e.g. if all robots are heterogeneous).

## Further Discussion of the Motivating Example [app:motivating_example]

This section provides proofs for claims in Section <a href="#sec:motivation" data-reference-type="ref" data-reference="sec:motivation">4</a>, and further discussion in support of the motivating example.

<div id="lemma:optimal-p-static" class="lemma" markdown="1">

**Lemma 1**. *For a team of \\(M\\) agents independently and identically selecting actions with probability \\(p\\), the \\(p\\) that maximizes the probability of winning is \\(p=\frac{1}{M}\\).*

</div>

<div class="proof" markdown="1">

*Proof.* The probability of winning, \\(P(s=1)\\) may be computed as follows:

\\[P(s=1) = M * P(\text{agent } i \text{ chooses 1 and $i^-$ choose 0}) 
        = M p (1-p)^{M-1}.\\]

We wish to analytically compute the global maxima of this expression with respect to \\(p\\), by considering the boundary points \\(p \in \{0, 1\}\\) and the zeros of the first derivative.

Clearly, if all agents select 1 with probability 0, the team cannot win; thus, \\(p=0\\) is not a maxima. Similarly, if all agents select 1 with probability 1, the team also never wins; thus, \\(p=1\\) is also not a maxima. Next, we compute the derivative of \\(P(s=1)\\):

\\[\begin{aligned}
    \frac{d[P(s=1)]}{dp} &= M[(1-p)^{M-1} - (M-1) p (1-p)^{M-2}] \\
    &= M[(1-p)^{M-2} [(1-p) - (M-1)p]].
\end{aligned}\\]

The zeros of the above expression occur at \\(p=0\\) and \\(p=\frac{1}{M}\\). For \\(p=\frac{1}{M}\\), note that \\(P(s=1) = (\frac{M-1}{M})^{M-1} > 0\\); thus, \\(p=\frac{1}{M}\\) must be the global maximum. ◻

</div>

<div id="lemma:aht-scenario" class="lemma" markdown="1">

**Lemma 2**. *In a team of three agents consisting of two uncontrolled agents who select 1 with probability \\(p=\frac13\\), and one controlled ad hoc agent who selects 1 with probability \\(p_{\text{aht}}\\), the probability of winning is \\(P(s=1) = \frac49\\).*

</div>

<div class="proof" markdown="1">

*Proof.* Let \\(s_u\\) denote the sum of the bits chosen by the two uncontrolled agents, and \\(b_{aht}\\) denote the bit value chosen by the controlled agent. The probability of winning can be computed by partitioning the winning outcomes by whether \\(s_u = 1\\) and the ad hoc agent selects 0, or whether the ad hoc agent selects 1 and both uncontrolled agents select 0, \\(s_u = 0\\).

\\[\begin{aligned}
    P(s=1) &= P(s_u = 1 \land b_{aht} = 0) + P(s_u = 0 \land b_{aht} = 1) \\ 
    &= 2 \cdot \frac13 \cdot \frac23 \cdot 
     \bigl( 1 - p_{\text{aht}} \bigr) + p_{\text{aht}}\left(\frac23 \cdot \frac23\right)\\
    &= \frac49 - \frac49 \cdot p_{\text{aht}} + \frac49 \cdot p_{\text{aht}} \\ 
    &= \frac49.
\end{aligned}\\] ◻

</div>

<div id="lemma:aht-in-naht-setting" class="lemma" markdown="1">

**Lemma 3**. *In a team of three agents where two ad hoc agents select 1 with probability \\(p_{\text{aht}}\\) and the remaining uncontrolled agent selects 1 with probability \\(\frac13\\), the maximizing \\(p_{\text{aht}} = \frac13\\) and the corresponding winning probability is \\(P(s=1) = \frac49\\).*

</div>

<div class="proof" markdown="1">

*Proof.* Let \\(s_{aht}\\) denote the sum of the bits chosen by the two ad hoc agents, and \\(b_{u}\\) denote the bit value chosen by the uncontrolled agent. The probability of winning may be directly computed as follows: \\[\begin{aligned}
    P(s=1) &= P(s_{aht} = 1 \land a_u = 0) + P(s_{aht} = 0 \land a_u = 1) \\
    &= 2 \cdot p_{\text{aht}} \cdot (1 - p_{\text{aht}}) \cdot \frac23 + (1 - p_{\text{aht}})^2 \cdot \frac13 \\
    &= (1 - p_{\text{aht}}) \left[ \frac43 p_{\text{aht}} + \frac13 (1 - p_{\text{aht}}) \right] \\ 
    &= (1 - p_{\text{aht}}) \left(\frac13 + p_{\text{aht}}\right).
\end{aligned}\\] To determine the maximizing value, we compute \\(P(s=1)\\) at the boundary points \\(p_{\text{aht}} \in \{0, 1\}\\) and at the zeros of the derivative of \\(P(s=1)\\). Note that for \\(p_{\text{aht}}=0\\), \\(P(s=1) = \frac13\\), while for \\(p_{\text{aht}} = 1\\), \\(P(s=1) = 0\\).

The derivative of the analytic expression of \\(P(s=1)\\) with respect to \\(p_{\text{aht}}\\) is:

\\[\frac{d [ P(s=1)]}{d p_{\text{aht}}} = (1 - p_{\text{aht}}) - \left(\frac13 + p_{\text{aht}}\right) = \frac23 - 2p_{\text{aht}}.\\]

The zeros of the above expression occur at \\(p_{\text{aht}} = \frac13\\), which corresponds to \\(P(s=1) = \frac49\\). ◻

</div>

<div id="lemma:two-player-setting" class="lemma" markdown="1">

**Lemma 4**. *In a team of three agents where one agent always selects 0, one agent selects 1 with probability \\(\frac13\\), and the last agent selects 1 with probability \\(p_{\text{naht}}\\), the optimal \\(p_{\text{naht}} = 1\\) and results in a winning probability of \\(P(s=1) = \frac23\\).*

</div>

<div class="proof" markdown="1">

*Proof.* Since one of the two controlled agents always plays 0, the game effectively becomes a two-player game instead.

Let \\(a_u\\) denote the action selected by the uncontrolled agent, and let \\(a_{naht}\\) denote the action of the controlled agent. The probability of winning may be directly computed as follows: \\[\begin{aligned}
    P(s=1) &= P(a_u = 1 \land a_{naht} = 0) + P(a_u = 0 \land a_{naht} = 1) \\
    &= \frac13 \cdot (1 - p_{\text{naht}}) + \frac23 \cdot p_{\text{naht}} \\ 
    &= \frac 13 + \frac13 \cdot p_{\text{naht}}.
\end{aligned}\\]

It is clear that \\(p_{\text{naht}} = 1\\) maximizes \\(P(s=1)\\), and the corresponding value of \\(P(s=1) = \frac23\\). ◻

</div>

## Experiment Details [app:exp_details]

### Team Sampling Procedure Used in Experiments [app:exp_team_sampling_proc]

<figure id="fig:naht_train">
<img src="./figures/naht-train.png"" style="width:40.0%" />
<figcaption>A practical instantiation of the NAHT problem.</figcaption>
</figure>

Recall that \\(U\\) and \\(C\\) denote the sets of uncontrolled and controlled agent policies, respectively, while \\(M\\) denotes the team size for the task, and \\(N\\) the number of agents sampled from \\(C\\). The experiments in the following consider an \\(X_{train}\\) consisting of sampling \\(N\\) uniformly from \\(\{1, \cdots, M-1\}\\), sampling \\(N\\) agents from \\(C\\) and \\(M-N\\) agents from \\(U\\) in a uniform fashion (Figure <a href="#fig:naht_train" data-reference-type="ref" data-reference="fig:naht_train">7</a>). The sampling procedure takes place at the beginning of each episode to select a team, which is then deployed in the environment. Data generated by the deployed team (e.g. joint observations, joint actions, and rewards) are returned to the learning algorithm. See Appendix <a href="#app:eval_details" data-reference-type="ref" data-reference="app:eval_details">9.3.3</a> for further details on the evaluation procedure.

### Experimental Domains [app:exp_setting]

#### MPE Predator Prey

The MPE environment `\citep{lowe2017multi}`{=latex} (released under the MIT License) is a setting where particle agents interact within a bounded 2D plane, equipped with a discrete action space and continuous observation space. The observation space is 16-dimensional, and contains the agents’ own position and velocity, relative positions/velocities of all other agents, landmarks, and prey. The action space consists of five discrete actions, corresponding to the four cardinal movement directions and a no-op action. The observation range is normalized to \\([-1,1]\\), while the discrete action space is one-hot encoded.

The predator-prey task (`mpe-pp`) is a custom task implemented by the authors of this paper within the fork of the MPE environment released by `\citet{papoudakis_benchmarking_2021}`{=latex} (MIT License). In this task, three predators must cooperatively pursue a pre-trained prey agent. We use the pre-trained prey policy provided by the ePymarl MARL framework. The prey policy was originally trained by `\citet{papoudakis_benchmarking_2021}`{=latex}, by using the MADDPG MARL algorithm to train both predator and prey agents for 25M steps, and generally attempts to escape approaching predators. The team receives a reward of 1 if two or more predators collide with the prey at a single time step, and no reward if only a single agent collides with the prey. A shaping reward consisting of 0.01 times the \\(\ell_2\\) distance between each agents in the team and the prey is provided as well. Since the prey policy is pre-trained and fixed, note that our predator-prey task is a fully cooperative task from the perspective of the predator (learning) agents. The maximum episode length is 100 time steps.

#### SMAC

SMAC `\citep{samvelyan19smac}`{=latex} (released under the MIT License) features a set of cooperative tasks, where a team of allied agents must defeat a team of enemy agents controlled by the game server. It is a partially observable domain with a continuous state space and discrete action space. For each agent, the observation space is continuous, and consists of features about itself, enemy, and allied agents within some radius. The action space is discrete, and allows an agent to choose an enemy to attack, a direction to move in, or to not perform any action. As the number of allies and enemies varies between tasks, the dimensionality of the observation and action spaces is particular to each task. The observation space is normalized to be between \\([-1, 1]\\), while the action space is one-hot encoded. The maximum number of time steps per episode is also task specific, although early termination occurs if all enemies are defeated. At each time step, the team receives a shaped reward corresponding to the damage dealt, and bonuses of 10 and 200 points for killing an enemy and winning the scenario by defeating all enemies. The reward is scaled such that the maximal return achievable in each task is 20.

The SMAC tasks considered in this paper are described in more detail below:

- `5v6`: stands for 5m vs 6m, five allied Marines versus six enemy Marines.

- `8v9`: stands for 8m vs 9m, eight allied Marines versus nine enemy Marines.

- `10v11`: stands for 10m vs 11m, ten allied Marines versus eleven enemy Marines.

- `3s5z`: stands for 3s vs 5z, three allied Stalkers versus five enemy Zealots.

### Evaluation Details [app:eval_details]

This section provides details on how mixed teams are evaluated in the NAHT setting, and how cross-play scores, and self-play scores, and uncertainty measures are computed.

#### \\(M-N\\) score.

Given a set of controlled agents \\(C\\), and a set of uncontrolled agents \\(U\\), the goal of the \\(M-N\\) score is to quantify the performance when these two teams must cooperate within the NAHT scenario. The \\(M-N\\) score is computed in a deterministic and exhaustive fashion, by iterating over all possible values of \\(N\\). Let \\(N\\) be the number of agents sampled from set \\(C\\), such that \\(N < M\\). For \\(N \in \{1, \cdots, M-1\}\\), construct the joint policy \\(\bm{\pi}^{(M)}\\) by selecting \\(N\\) agents uniformly from \\(C\\) and \\(M-N\\) agents from \\(U\\). Evaluate the resultant team on the task for \\(E\\) episodes. This results in \\((M-1)*E\\) episode returns, which is averaged to form the \\(M-N\\) score.

#### Cross-play scores.

The cross-play scores reported in this paper are the average returns of teams generated by algorithm \\(A\\), when coordinating with those generated by \\(B\\). To compute the cross-play score, we first train multiple teams (by varying the seed) via both algorithms \\(A\\) and \\(B\\). Next, the \\(M-N\\) score is computed for random pairings of teams (seeds) from \\(A\\) and \\(B\\), following the procedure specified above. Summary statistics can be computed over the set of all such NAHT returns.

For example: each algorithm (VDN, QMIX, IQL, MAPPO, IPPO) is run \\(k\\) times with different seeds, to generate \\(k\\) teams of agents that may act as uncontrolled teammates. For each pair of algorithms, we sample a subset of the possible seed *pairs*, and evaluate the teams that result from merging said seed pairs. If VDN and QMIX have seeds 1 2, and 3, the cross-play evaluation might consider the seed pairings (VDN 1, QMIX 2), (VDN 2, QMIX 3), (VDN 3, QMIX 1). Given a pair of seeds (e.g. (VDN 1), (QMIX 2)), the \\(M-N\\) cross-play score is computed as the average return generated by sweeping \\(N \in \{1, \cdots, M-1\}\\) and evaluating the merged team that consists of selecting \\(N\\) agents from the first team, and \\(M - N\\) agents for \\(E\\) episodes. In our experiments, \\(E = 128\\), and \\(k=5\\).

#### Self-play scores.

The self-play scores reported in this paper are *model* self-play score, rather than *algorithm* self-play score `\citep{marl-book-albrecht24}`{=latex}. The reason for this is that we are interested in the performance of agents when paired with *known teammates*.

#### Measuring uncertainty

Means and 95% confidence intervals are computed over all team/seed pairings considered for any given scores, as we treat each \\(M-N\\) evaluation as an independent trial. For most experiments, five seed pairs are considered, where seeds are paired to ensure that all seeds participate in at least one evaluation. This ensures that the computational cost of the evaluation remains linear in \\(N\\), rather than quadratic. However, note that for the OOD experiments presented in Section <a href="#sec:ood_generalization" data-reference-type="ref" data-reference="sec:ood_generalization">6.3</a>, we consider all possible seed pairings for the most comprehensive evaluation.

### Algorithm Implementation [app:alg]

The experiments in this paper use algorithm implementations from the ePyMARL codebase `\citep{papoudakis_benchmarking_2021}`{=latex} (released under the Apache License). The value-based methods are used without modification (i.e. IQL, VDN, QMIX), but we implement our version of policy gradient methods (IPPO, MAPPO), based on the implementation of `\citet{yu2021surprising}`{=latex}.

#### Parameter Sharing.

All methods employ recurrent actors and critics, with full parameter sharing, i.e. all agents are controlled by the same policy, where the agent id is input to the actor and critic networks, to allow behavioral differentiation between agents. For POAM, which also maintains encoder-decoder networks for agent modeling, parameter sharing is used for the encoder and decoder networks, to improve training sample efficiency. Further, POAM’s decoder, which predicts observations and actions for all \\(-i\\) agents, employs parameter sharing across agent predictions, to prevent the target dimensionality from scaling with the number of teammates.

#### Optimizer and Neural Architecture.

The Adam optimizer is applied for all networks involved. For policy gradient methods, the policy architecture is two fully connected layers, followed by an RNN (GRU) layer, followed by an output layer. Each layer has 64 neurons with ReLU activation units, and employs layer normalization. The critic architecture is the same as the policy architecture. The value-based methods employ the same architecture, except that there is only a single fully connected layer before the RNN layers, and layer normalization is not used, following the ePyMARL implementation. Please consult the codebase for full implementation details.

<div id="tab:hyperparam_pg" markdown="1">

| Algorithm | Buffer size | Epochs | Minibatches | Entropy | Clip | Clip value loss |
|:--:|:--:|:--:|:--:|:---|:--:|:---|
| IPPO | 128, **256**, 512 | 1, **4**, 10 | 1, **3** | 0.01, 0.03, **0.05** | 0.01, 0.05, **0.1** | **no**, yes |
| MAPPO | 64, 128, **256** | 4, **10** | **1**, 3 | 0.01, **0.03**, 0.05, 0.07 | **0.05**, 0.1, 0.2 | no, **yes** |

Hyperparameters evaluated for the policy gradient algorithms. Selected values are bolded

</div>

#### Hyperparameters.

For the value-based methods, default hyperparameters are used. We tune the hyperparameters of the policy gradient methods on the `5v6` task, and apply those parameters directly to the remaining SMAC tasks. The hyperparameters considered for policy gradient algorithms are given in Table <a href="#tab:hyperparam_pg" data-reference-type="ref" data-reference="tab:hyperparam_pg">1</a>. POAM adopts the same hyperparameters as IPPO where applicable. We also tuned additional hyperparameters specific to POAM (Table <a href="#tab:poam_hyper" data-reference-type="ref" data-reference="tab:poam_hyper">2</a>).

<div id="tab:poam_hyper" markdown="1">

|   Task   | Algorithm |  ED epochs   | ED Minibatches | ED LR             |
|:--------:|:---------:|:------------:|:--------------:|:------------------|
|  `5v6`   |   POAM    | **1**, 5, 10 |    **1**, 2    | **0.0005**, 0.005 |
| `mpe-pp` |   POAM    |   **1**, 5   |    **1**, 2    | **0.0005**, 0.001 |

Additional hyperparameters evaluated for POAM; note that ED stands for encoder-decoder. Selected values are bolded.

</div>

## Supplemental Figures [app:supp_results]

This section contains additional figures referenced by the main paper. Please see Section <a href="#sec:exp" data-reference-type="ref" data-reference="sec:exp">6</a> for the corresponding analysis and discussion.

<figure id="fig:5v6_critic_masking">
<img src="./figures/5v6_poam-within-episode-loss.png"" style="width:85.0%" />
<img src="./figures/5v6_open-pqvmq_critic-masking-ablation.png"" style="width:100.0%" />
<figcaption>Learning curves of POAM and IPPO, where the value network is trained with and without uncontrolled agents’ data (UCD) on <code>5v6</code>. </figcaption>
</figure>

<figure id="fig:sc2_ood_gen">
<img src="./figures/sc2_open-pqvmq_ood_gen-id-baseline.png"" style="width:100.0%" />
<figcaption>Returns achieved by POAM and IPPO-NAHT, when paired with out-of-distribution teammates. POAM shows improved generalization to OOD teammates, as compared to IPPO-NAHT, across all StarCraft tasks. For each type of teammate, the performance of IPPO-NAHT/POAM against the exact teammate seen during training is shown as the in-distribution baseline.</figcaption>
</figure>

## Supplemental Analysis

The following subsections contains secondary analysis and results, intended to support the primary analysis of the main paper. For all results, the mean and 95% confidence interval over five trials is reported. All reported returns are *test* returns.

### The Need for Dedicated NAHT Algorithms - Empirical Evidence [app:supp_results:need_for_naht_exp]

Section <a href="#sec:motivation" data-reference-type="ref" data-reference="sec:motivation">4</a> argues theoretically that with a population of uncontrolled teammates that select one with probability 1/3 and zero otherwise, an optimal policy learned in the AHT setting would not be optimal in the NAHT setting. Here, we present experiments on the three agent bit matrix game that empirically verify the theory. Let \\(N\\) denote the number of controlled agents. The episode length is 25, and the observation for each agent consists of the agent index and the joint action at the previous time step. The team reward at each time step is \\(3*\mathbbm{1}_{\sum_i b_i = 1}\\).

The optimal expected return in the \\(N=1\\) (AHT) setting is 33.333 (derived from Lemma A.3), and in the \\(N=2\\) (NAHT) setting is 50.0 (derived from Lemma A.4). The methods compared are POAM-AHT and POAM. Table <a href="#tab:need_for_naht" data-reference-type="ref" data-reference="tab:need_for_naht">3</a> shows the expected returns achieved by POAM and POAM-AHT on the \\(N=1\\) and \\(N=2\\) scenarios. Since any policy is optimal in the \\(N=1\\) case, as expected, both methods achieve near-optimal returns. In the \\(N=2\\) case, as expected, POAM outperforms POAM-AHT by a large margin, achieving a near-optimal return of \\(48.858 \pm 1.092\\).

<div id="tab:need_for_naht" markdown="1">

|          |       N=1 (AHT)        |       N=2 (NAHT)       |
|:--------:|:----------------------:|:----------------------:|
| POAM-AHT | 33.441 \\(\pm\\) 0.511 |  22.5 \\(\pm\\) 8.250  |
|   POAM   | 33.483 \\(\pm\\) 0.485 | 48.858 \\(\pm\\) 1.092 |

Returns of POAM-AHT versus POAM on the three agent bit matrix game, in the N=1 (AHT) and N=2 (NAHT) setting. POAM and POAM-AHT both achieve the optimal return for the \\(N=1\\) case, but POAM has a much higher return on the \\(N=2\\) case.

</div>

### Validating the Existence of Coordination Conventions [app:supp:coord_conventions]

The experimental procedure detailed in Section <a href="#sec:ExpDesign" data-reference-type="ref" data-reference="sec:ExpDesign">6.1</a> generates diverse teammates in SMAC and MPE by using MARL algorithms to train multiple teams of agents. Two underlying assumptions of the procedure are that (1) teams trained by the same algorithm learn non-compatible coordination conventions, and (2) teams trained by different algorithms are not compatible. Both points are experimentally verified in this section.

#### Self-play with MARL algorithms.

For the tasks under consideration, teams trained using the same MARL algorithm, but different seeds, can converge to distinct coordination conventions. Figure <a href="#fig:sp_scores" data-reference-type="ref" data-reference="fig:sp_scores">10</a> demonstrates this by depicting the return of teams trained together (matched seeds) versus those of teams that were not trained together (mismatched seeds), across all naive MARL algorithms (IPPO, IQL, MAPPO, QMIX, VDN) and tasks considered. Overall, the returns of the teams that were trained together are higher than those not trained together.

The general phenomenon has been previously observed and exploited by prior works in ad hoc teamwork `\citep{strouse_fcp_2022}`{=latex}. This paper takes advantage of this to generate a set of diverse teammates for the StarCraft II experiments. We select tasks where the effect is significant, to ensure that there are distinct coordination behaviors for POAM to model. Tasks that were considered but subsequently ruled out for this reason include `3m vs 3m, 8m vs 8m`, `6h vs 8z`, and the MPE Spread task.

<figure id="fig:sp_scores">
<img src="./figures/mm-selfplay.png"" style="width:100.0%" />
<figcaption>Agent teams that were trained together using the same algorithm (<code>matched</code> seeds) have higher returns than teams that were not trained together but were trained with the same algorithm (<code>mismatched</code> seeds).</figcaption>
</figure>

#### Cross-play with MARL algorithms

<img src="./figures/mpe-pp_ts%3D100_shape%3D0.01_xp-matrix.png"" alt="image" />

<img src="./figures/5v6_xp-matrix.png"" alt="image" />

<img src="./figures/8v9_xp-matrix.png"" alt="image" />

<img src="./figures/3s5z_xp-matrix.png"" alt="image" />

<img src="./figures/10v11_xp-matrix.png"" alt="image" />

Tables <a href="#fig:mpe-pp-xp-matrix" data-reference-type="ref" data-reference="fig:mpe-pp-xp-matrix">[fig:mpe-pp-xp-matrix]</a>, <a href="#fig:5v6-xp-matrix" data-reference-type="ref" data-reference="fig:5v6-xp-matrix">[fig:5v6-xp-matrix]</a>, <a href="#fig:8v9-xp-matrix" data-reference-type="ref" data-reference="fig:8v9-xp-matrix">[fig:8v9-xp-matrix]</a>, <a href="#fig:3s5z-xp-matrix" data-reference-type="ref" data-reference="fig:3s5z-xp-matrix">[fig:3s5z-xp-matrix]</a>, and <a href="#fig:10v11-xp-matrix" data-reference-type="ref" data-reference="fig:10v11-xp-matrix">[fig:10v11-xp-matrix]</a> display the full cross-play results for all MARL algorithms and POAM-AHT, on all tasks. Note that the tables are reflected across the diagonal axis for viewing ease. The values on the off-diagonal (i.e., where the two algorithms are not the same) are the cross-play score, while the values shown on the diagonal are self-play scores, computed as described in App. <a href="#app:eval_details" data-reference-type="ref" data-reference="app:eval_details">9.3.3</a>. The cross-play and self-play scores displayed are means and 95% confidence intervals. The rightmost column reflects the row average and 95% confidence interval of the cross-play (XP) scores corresponding to the test set of VDN, QMIX, IQL, IPPO, and MAPPO, excluding the self-play score. Thus, the rightmost column reflects how well on average the row MARL/AHT algorithm can generalize to the test set used throughout this paper.

Overall, we find that MARL algorithms perform significantly better in self-play than cross-play. We note that there are a few exceptions (e.g., IPPO vs QMIX on `8m`), and that the cross-play and self-play scores are much closer on `10v11`, but overall the trend is consistent across tasks.

### Generalization to Unseen Teammate Types [app:supp:alt_ood_gen]

As discussed and experimentally verified in Section <a href="#app:supp:coord_conventions" data-reference-type="ref" data-reference="app:supp:coord_conventions">9.5.2</a>, diverse coordination conventions in StarCraft and MPE Predator Prey can be generated by (1) running the same MARL algorithm with various random seeds, or (2) running various MARL algorithms. In Section <a href="#sec:ood_generalization" data-reference-type="ref" data-reference="sec:ood_generalization">6.3</a>, the out-of-distribution (OOD) teammates considered were generated by running MARL algorithms with different random seeds than those used to generate train-time teammates—in other words, generating diverse and unseen teammates using the first procedure specified above.

<figure id="fig:alt_ood_gen">
<img src="./figures/open-qmq_ood.png"" style="width:100.0%" />
<figcaption>Returns achieved by POAM and IPPO-NAHT, when trained on MAPPO, QMIX, and IQL, and tested on IPPO, VDN.</figcaption>
</figure>

Here, we generate OOD teammates using the second procedure. More precisely, the five MARL algorithms used to generate teammates may be divided into train/test sets. Figure <a href="#fig:alt_ood_gen" data-reference-type="ref" data-reference="fig:alt_ood_gen">11</a> shows the results of such an experiment, where POAM and IPPO-NAHT are trained with MAPPO, QMIX, and IQL teammates, and tested with agents from IPPO and VDN. Similar to the results presented in Section <a href="#sec:ood_generalization" data-reference-type="ref" data-reference="sec:ood_generalization">6.3</a>, we find that POAM generally outperforms IPPO-NAHT for the unseen teammate types for all tasks.

### Modeling Controlled and Uncontrolled Agents [app:supp:ed_ctrl_unctrl]

<figure id="fig:ed_loss_ctrl_unctrl">
<img src="./figures/ed-loss-controlled-vs-uncontrolled.png"" style="width:60.0%" />
<figcaption>Loss of POAM’s encoder-decoder on the <code>mpe-pp</code> task, separated by uncontrolled (left) and controlled agents (right).</figcaption>
</figure>

POAM’s encoder-decoder (ED) models both controlled and uncontrolled agents. Since the controlled agents are updated during the training process, the encoder-decoder must deal with a “moving target". Thus, in principle, the problem of modeling the controlled agents is more challenging than modeling uncontrolled agents.

Fig. <a href="#fig:ed_loss_ctrl_unctrl" data-reference-type="ref" data-reference="fig:ed_loss_ctrl_unctrl">12</a> shows the probability of predicting the correct *action* for the uncontrolled agents and controlled agents separately, on the `mpe-pp` task. Note that the action probabilities shown in Fig. <a href="#fig:mpe-pp_poam_within_episode" data-reference-type="ref" data-reference="fig:mpe-pp_poam_within_episode">5</a> (right) of the main paper would be the average of the two plots shown in Fig. <a href="#fig:ed_loss_ctrl_unctrl" data-reference-type="ref" data-reference="fig:ed_loss_ctrl_unctrl">12</a>.

For uncontrolled agents (Fig. <a href="#fig:ed_loss_ctrl_unctrl" data-reference-type="ref" data-reference="fig:ed_loss_ctrl_unctrl">12</a>, left), we observe that the accuracy of action predictions for the uncontrolled agents increases much more consistently as training goes on, and is higher than that for the controlled agents. As expected, the ED is able to model the uncontrolled agents more easily than the nonstationary controlled agents.

### Performance as a Function of the Number of Controlled Agents

To provide more insight on how an NAHT team’s performance changes as the number of controlled agents increases, Figure <a href="#fig:varying-n-ctrl" data-reference-type="ref" data-reference="fig:varying-n-ctrl">13</a> displays the mean test returns of POAM and POAM-AHT as a function of the number of controlled agents, where the evaluation returns are averaged across five types of uncontrolled teammates: QMIX, VDN, IQL, MAPPO, and IPPO. Note that the self-play returns of POAM and POAM-AHT (i.e. where the number of controlled agents is maximal) are shown as horizontal lines, while the performances at \\(N=0\\) correspond to the averaged self-play returns of the five uncontrolled team types.

Recall that POAM-AHT was trained on the \\(N=1\\) scenario only, while POAM is trained on the full NAHT setting. As expected, POAM outperforms POAM-AHT for all values of \\(N>1\\), while for \\(N=1\\), the methods perform similarly. POAM-AHT’s performance declines as the number of controlled agents increases, which likely occurs because the evaluation setting becomes further from the training setting as \\(N\\) increases.

<figure id="fig:varying-n-ctrl">
<img src="./figures/nk_mean_curves.png"" style="width:100.0%" />
<figcaption>Comparing the performance of POAM versus POAM-AHT, as the number of controlled agents varies. POAM and POAM-AHT agents are evaluated with the following set of uncontrolled agents: QMIX, VDN, IQL, MAPPO, IPPO.</figcaption>
</figure>

## Computing Infrastructure [app:computing_infra]

All value-based algorithms (e.g. QMIX, VDN, IQL) were run without parallelizing environments, while policy gradient algorithms were run with parallelized environments. All methods were trained for 20M steps on all tasks. Each run took between 12-48 hours of compute, and used less than 2gB of GPU memory. Runs were parallelized to use computational resources efficiently. The servers used for our experiments ran Ubuntu 20.04 with the following configurations:

- Intel Xeon CPU E5-2630 v4; Nvidia Titan V GPU.

- Intel Xeon CPU E5-2698 v4; Nvidia Tesla V100-SXM2 GPU.

- Intel Xeon Gold 6342 CPU; Nvidia A40 GPU.

- Intel Xeon Gold 6342 CPU; Nvidia A100 Gpu.

[^1]: Work was done while at SparkCognition.

[^2]: \\(\Delta(S)\\) denotes the space of probability distributions over set \\(S\\).

[^3]: The original AHT problem statement used the terms “known” and “unknown” agents. However, it is common for modern AHT learning methods to assume some knowledge about the “unknown” teammates during training `\citep{mirsky2022survey}`{=latex}. Thus, we instead employ the terms controlled and uncontrolled.

[^4]: Lemma <a href="#lemma:optimal-p-static" data-reference-type="ref" data-reference="lemma:optimal-p-static">1</a> shows that \\(p=\frac{1}{M}\\) is the optimal value of \\(p\\) for a team composed of such agents.

[^5]: See the Appendix for tables providing the performances of all naive MARL methods, across all tasks.
