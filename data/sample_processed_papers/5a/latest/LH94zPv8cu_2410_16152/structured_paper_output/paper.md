# *Warped Diffusion:* Solving Video Inverse Problems with Image Diffusion Models

## Abstract

Using image models naively for solving inverse video problems often suffers from flickering, texture-sticking, and temporal inconsistency in generated videos. To tackle these problems, in this paper, we view frames as continuous functions in the 2D space, and videos as a sequence of continuous warping transformations between different frames. This perspective allows us to train function space diffusion models only on *images* and utilize them to solve temporally correlated inverse problems. The function space diffusion models need to be equivariant with respect to the underlying spatial transformations. To ensure temporal consistency, we introduce a simple post-hoc test-time guidance towards (self)-equivariant solutions. Our method allows us to deploy state-of-the-art latent diffusion models such as Stable Diffusion XL to solve video inverse problems. We demonstrate the effectiveness of our method for video inpainting and \\(8\times\\) video super-resolution, outperforming existing techniques based on noise transformations. We provide generated video results in the following URL: <https://giannisdaras.github.io/warped_diffusion.github.io/>.

<figure id="fig:teaser">
<img src="./figures/fig1_robot.png"" style="width:50.0%" />
<figcaption>Inpainting results for “a robot sitting on a bench”. As the input video shifts smoothly, our output frames stay consistent.</figcaption>
</figure>

# Introduction

Diffusion models (DMs) `\cite{sohl2015deep,ho2020ddpm,song2020score}`{=latex} can synthesize photorealistic imagery `\cite{saharia2022imagen,ramesh2022dalle2,rombach2021highresolution,balaji2022ediffi,podell2024sdxl,esser2024scaling}`{=latex}. They can be conditioned easily, through explicit training or guidance `\cite{dhariwal2021diffusion,ho2021classifierfree}`{=latex}, and have also been widely used to solve inverse problems `\cite{diffusion_survey, whang2021deblurring,chung2022diffusion,chung2022improving,song2022pseudoinverse,wang2022zero,kawar2022denoising,mardani2023variational}`{=latex}, in particular for image processing applications like inpainting and super-resolution `\cite{ho2021cascaded,saharia2021palette,saharia2021image,rombach2021highresolution}`{=latex}.

How do these methods extend to video processing and solving inverse problems on videos? Although video DMs are seeing rapid progress `\cite{ho2022imagen,singer2023makeavideo,blattmann2023videoldm,ge2023pyoco,blattmann2023stable,girdhar2023emu,bartal2024lumiere,videoworldsimulators2024}`{=latex}, general text-to-video synthesis has not yet reached the level of robustness and expressivity comparable to modern image models. Moreover, no state-of-the-art video generative models are publicly available `\cite{videoworldsimulators2024}`{=latex}, and most video DMs are computationally expensive. To circumvent these challenges, a natural research direction is to leverage existing, powerful *image* generative models to solve *video* inverse problems.

Naively applying image DMs to videos in a frame-wise manner violates temporal consistency. Previous works alleviate the problem by fine-tuning on video data or by warping the networks’ features, using, for instance, temporal or cross-frame attention layers `\cite{wu2023tune,liu2023video,ceylan2023pix2video,khachatryan2023text2video,qi2023fatezero,yang2023rerender,zhang2024towards,guo2023animatediff,guo2023sparsectrl}`{=latex}. However, these methods are usually designed specifically for high-level text-driven editing or stylization and are typically not directly applicable to general inverse problems. Moreover, without training on diverse video data they often cannot maintain high frequency information across frames. For a detailed discussion of the related works, we refer the reader to Section <a href="#sec:related_work" data-reference-type="ref" data-reference="sec:related_work">12</a> in the Appendix.

The recent novel work, “How I Warped Your Noise” `\cite{chang2023warped}`{=latex}, proposes *noise warping* to achieve temporal consistency in generated videos by changing appropriately the input noise to the diffusion model. Videos can be thought of as image frames subject to spatial transformations. An object may move according to a translation; complex and general transformations can be described by motion vectors on the pixels defined through optical flow `\cite{fortun2015opticalflow}`{=latex}. It is these transformations that define how the noise maps need to be warped and transformed. In `\citep{chang2023warped}`{=latex}, temporally consistent noise maps are given as input to the DM’s denoiser, with the underlying assumption that temporally consistent inputs induce temporally consistent network outputs. In this paper, we argue that this assumption only holds true if the utilized image DM is *equivariant* with respect to the spatial warping transformations. However, as we show in this work, the network is not necessarily equivariant because i) the conditional expectation modeled by the DM may not be equivariant, and, ii) more importantly, a free-form neural network, as used in typical DMs, will not learn a perfectly equivariant function. When the equivariance assumption is violated, the method proposed in `\citep{chang2023warped}`{=latex} achieves poor results. This is typically the case for challenging conditional tasks (see Figure <a href="#fig:teaser" data-reference-type="ref" data-reference="fig:teaser">1</a>) or when modeling complex distributions. Particularly, `\citep{chang2023warped}`{=latex} finds that the proposed method has “limited impact on temporal coherency” when applied to *latent* diffusion models and that “all the noise schemes produce temporally inconsistent results”.

We introduce a new framework, dubbed *Warped Diffusion*, for the rigorous application of image DMs to video inverse problems. We employ a continuous function space perspective to DMs `\cite{lim2023score, pidstrigach2023infinite, franzese2024continuous,hagemann2023multilevel}`{=latex} that naturally allows noise warping for arbitrarily complex spatial transformations. Our method generalizes the warping scheme of `\citep{chang2023warped}`{=latex} and does not require any auxiliary high-resolution noise maps. To achieve equivariance, we propose *equivariance self-guidance*, a novel sampling mechanism that enforces that the generated frames are consistent under the warping transformation. Our inference time approach elegantly circumvents the need for additional training. This unlocks the use of existing large DMs in a fully equivariant manner without further training, which may be prohibitive for a practitioner.

We extensively validate our method on video inpainting and super-resolution. Super-resolution represents a situation with strong conditioning, while inpainting requires large-scale, temporally coherent synthesis of new content. Warped Diffusion outperforms previous methods quantitatively and qualitatively, and shows reduced flickering and texture sticking artifacts. Due to our equivariance guidance, our method can also be used with *latent* DMs, which is not possible with previous approaches. Virtually all existing state-of-the-art text-to-image generation systems are indeed latent DMs, like Stable Diffusion `\cite{rombach2021highresolution}`{=latex}. Hence, any inverse problem solving method must be readily usable with latent DMs. In fact, all our experiments utilize the state-of-the-art text-to-image latent DM SDXL `\cite{podell2024sdxl}`{=latex}.

**Contributions:** *(a)* We propose Warped Diffusion, a novel framework for applying image DMs to video inverse problems. *(b)* We introduce a principled scheme for noise warping, based on Gaussian processes and a function space DM perspective. *(c)* We identify the equivariance of the DM as a critical requirement for the seamless application of image DMs to video inverse problems and propose an inference-time guidance method to enforce it. *(d)* We comprehensively test Warped Diffusion and achieve state-of-the-art video processing performance when considering the use of image DMs. Critically, Warped Diffusion can be used with any image DMs, including large-scale latent DMs.

<span id="sec:problem" label="sec:problem"></span>

<figure id="fig:fig2">
<img src="./figures/fig2.png"" style="width:90.0%" />
<figcaption>Visualization of Warped Diffusion applied to video super-resolution. (a) We develop a function space diffusion model that super-resolves images given samples from a Gaussian process (GP). To extend the image model to videos, (b) we extract warping transformations between consecutive input frames using optical flow. (c) We use the flow to warp the GP sample from the previous frame. (d) To ensure temporal consistency, we introduce equivariance self-guidance in the ODE sampler.</figcaption>
</figure>

# Functional Video Generation

The basis of our approach, summarized in Figure <a href="#fig:fig2" data-reference-type="ref" data-reference="fig:fig2">2</a>, is to structure the generative model so that it is equivariant with respect to spatial deformations and apply these deformations successively to the input noise. Each deformation effectively warps the noise and the equivariance guarantees that each output image will be similarly warped. By using an optical flow from a real video to define a sequence of such deformations, a new video can be generated. To introduce our method, we first conceptualize both images and noise as functions on a domain and the generator as a mapping between two function spaces.

## Functional Generative Modeling and Videos [subsec:generaitve_functions]

Each video frame can be seen as a single image, and an image as a discretization of a vector-valued function on a rectangular domain. Consider the domain as the \\(2\\)-D unit square \\(D = [0,1]^2\\), defining an image as a function \\(f : D \to \mathbb{R}^3\\). For each location \\(x \in D\\), the value \\(f(x) \in \mathbb{R}^3\\) represents an RGB color. We assume images have infinite resolution. To formulate a model that generates such images, we must have a notion of a space containing all possible images. We’ll use the separable Hilbert space \\(H = L^2(D; \mathbb{R}^3)\\), with pointwise formulas interpreted almost everywhere with respect to the Lebesgue measure.

We assume that there exists a probability measure \\(\mu\\) on \\(H\\) whose support is the set of photorealistic images and denote by \\(\eta\\) a known reference probability measure on \\(H\\). In our case, \\(\eta\\) will be a Gaussian measure on \\(H\\); for details, see Section <a href="#sec:grf" data-reference-type="ref" data-reference="sec:grf">3.1</a>. A *generative model*, or *transport map*, is then a mapping \\(G : H \to H\\) such that the pushforward of \\(\eta\\) under \\(G\\) is \\(\mu\\) which we denote as \\(G_\sharp \eta = \mu\\). In particular, this implies that any random variable \\(\xi \sim \eta\\) will satisfy \\(G(\xi) \sim \mu\\). For diffusion models, \\(G\\) can be defined by the probability flow ODE; see Section <a href="#sec:diffusion_guidance" data-reference-type="ref" data-reference="sec:diffusion_guidance">3.2</a>.

Given an image \\(f_0 \in H\\), a *video* with \\(n+1 \in \mathbb{N}\\) frames is the sequence of functions \\((f_0, f_1, \dots, f_n) \in H^{n+1}\\), where each subsequent function is obtained, at least partially, from the previous one by a deformation. Specifically, a sequence of bounded, injective maps \\((T_j : D \to D_j)_{j=1}^{n}\\) exists such that

\\[\label{eq:continuity_eq}
    \gdef\csname eqnannotate@data@f1\endcsname{NavyBlue}\tikzmarknode{f1}{\begingroup\colorlet{currentcolor}{.}%
% \colorbox sets the second argument in text mode, so for use within equations we wrap it in $ $ again
    \mathchoice% to get right font size in each mode:
        {\colorbox{NavyBlue!17}{$\displaystyle \mathstrut f$}}%
        {\colorbox{NavyBlue!17}{$\textstyle \mathstrut f$}}%
        {\colorbox{NavyBlue!17}{$\scriptstyle \mathstrut f$}}%
        {\colorbox{NavyBlue!17}{$\scriptscriptstyle \mathstrut f$}}%
\endgroup}_{\gdef\csname eqnannotate@data@j\endcsname{RoyalPurple}\tikzmarknode{j}{\begingroup\colorlet{currentcolor}{.}%
% \colorbox sets the second argument in text mode, so for use within equations we wrap it in $ $ again
    \mathchoice% to get right font size in each mode:
        {\colorbox{RoyalPurple!17}{$\displaystyle \mathstrut j$}}%
        {\colorbox{RoyalPurple!17}{$\textstyle \mathstrut j$}}%
        {\colorbox{RoyalPurple!17}{$\scriptstyle \mathstrut j$}}%
        {\colorbox{RoyalPurple!17}{$\scriptscriptstyle \mathstrut j$}}%
\endgroup}}(\gdef\csname eqnannotate@data@x1\endcsname{OliveGreen}\tikzmarknode{x1}{\begingroup\colorlet{currentcolor}{.}%
% \colorbox sets the second argument in text mode, so for use within equations we wrap it in $ $ again
    \mathchoice% to get right font size in each mode:
        {\colorbox{OliveGreen!17}{$\displaystyle \mathstrut x$}}%
        {\colorbox{OliveGreen!17}{$\textstyle \mathstrut x$}}%
        {\colorbox{OliveGreen!17}{$\scriptstyle \mathstrut x$}}%
        {\colorbox{OliveGreen!17}{$\scriptscriptstyle \mathstrut x$}}%
\endgroup}) = \gdef\csname eqnannotate@data@f2\endcsname{NavyBlue}\tikzmarknode{f2}{\begingroup\colorlet{currentcolor}{.}%
% \colorbox sets the second argument in text mode, so for use within equations we wrap it in $ $ again
    \mathchoice% to get right font size in each mode:
        {\colorbox{NavyBlue!17}{$\displaystyle \mathstrut f$}}%
        {\colorbox{NavyBlue!17}{$\textstyle \mathstrut f$}}%
        {\colorbox{NavyBlue!17}{$\scriptstyle \mathstrut f$}}%
        {\colorbox{NavyBlue!17}{$\scriptscriptstyle \mathstrut f$}}%
\endgroup}_{\gdef\csname eqnannotate@data@j2\endcsname{RoyalPurple}\tikzmarknode{j2}{\begingroup\colorlet{currentcolor}{.}%
% \colorbox sets the second argument in text mode, so for use within equations we wrap it in $ $ again
    \mathchoice% to get right font size in each mode:
        {\colorbox{RoyalPurple!17}{$\displaystyle \mathstrut j-1$}}%
        {\colorbox{RoyalPurple!17}{$\textstyle \mathstrut j-1$}}%
        {\colorbox{RoyalPurple!17}{$\scriptstyle \mathstrut j-1$}}%
        {\colorbox{RoyalPurple!17}{$\scriptscriptstyle \mathstrut j-1$}}%
\endgroup}} \big( \gdef\csname eqnannotate@data@T\endcsname{Plum}\tikzmarknode{T}{\begingroup\colorlet{currentcolor}{.}%
% \colorbox sets the second argument in text mode, so for use within equations we wrap it in $ $ again
    \mathchoice% to get right font size in each mode:
        {\colorbox{Plum!17}{$\displaystyle \mathstrut T$}}%
        {\colorbox{Plum!17}{$\textstyle \mathstrut T$}}%
        {\colorbox{Plum!17}{$\scriptstyle \mathstrut T$}}%
        {\colorbox{Plum!17}{$\scriptscriptstyle \mathstrut T$}}%
\endgroup}_{j}^{-1}(\gdef\csname eqnannotate@data@x2\endcsname{OliveGreen}\tikzmarknode{x2}{\begingroup\colorlet{currentcolor}{.}%
% \colorbox sets the second argument in text mode, so for use within equations we wrap it in $ $ again
    \mathchoice% to get right font size in each mode:
        {\colorbox{OliveGreen!17}{$\displaystyle \mathstrut x$}}%
        {\colorbox{OliveGreen!17}{$\textstyle \mathstrut x$}}%
        {\colorbox{OliveGreen!17}{$\scriptstyle \mathstrut x$}}%
        {\colorbox{OliveGreen!17}{$\scriptscriptstyle \mathstrut x$}}%
\endgroup}) \big ), \qquad \forall \: x \in \gdef\csname eqnannotate@data@D\endcsname{Cerulean}\tikzmarknode{D}{\begingroup\colorlet{currentcolor}{.}%
% \colorbox sets the second argument in text mode, so for use within equations we wrap it in $ $ again
    \mathchoice% to get right font size in each mode:
        {\colorbox{Cerulean!17}{$\displaystyle \mathstrut D$}}%
        {\colorbox{Cerulean!17}{$\textstyle \mathstrut D$}}%
        {\colorbox{Cerulean!17}{$\scriptstyle \mathstrut D$}}%
        {\colorbox{Cerulean!17}{$\scriptscriptstyle \mathstrut D$}}%
\endgroup} \cap \gdef\csname eqnannotate@data@dj\endcsname{RedOrange}\tikzmarknode{dj}{\begingroup\colorlet{currentcolor}{.}%
% \colorbox sets the second argument in text mode, so for use within equations we wrap it in $ $ again
    \mathchoice% to get right font size in each mode:
        {\colorbox{RedOrange!17}{$\displaystyle \mathstrut D_{j}$}}%
        {\colorbox{RedOrange!17}{$\textstyle \mathstrut D_{j}$}}%
        {\colorbox{RedOrange!17}{$\scriptstyle \mathstrut D_{j}$}}%
        {\colorbox{RedOrange!17}{$\scriptscriptstyle \mathstrut D_{j}$}}%
\endgroup}, \;\; j=1,\dots,n,\\]

where \\(D_{j} \coloneqq T_{j}(D)\\) and we assume that the sets \\(D \cap D_{j}\\) have positive Lebesgue measure. In video modeling, the sequence \\((T_j )_{j=1}^{n}\\) is usually referred to as the *optical flow* as it specifies how each pixel in the previous frame moves to the next frame. While the frames can also be conceptualized as a continuum in time, we work with a discrete set of frames for simplicity. We consider \\(D\\) to always represent our fixed frame of vision and we allow each \\(T_j\\) to move pixels outside of this frame. Therefore <a href="#eq:continuity_eq" data-reference-type="eqref" data-reference="eq:continuity_eq">[eq:continuity_eq]</a> determines \\(f_{j}\\) only on the set \\(D \cap D_j\\) which contains pixels that remain within our field of vision.

## Video Generation and Equivariance [subsec:equivariance]

Given our notion of a video and a generative model, we now describe how such a model can be used to generate new videos. Suppose we want to create a two-frame video given an initial frame \\(f_0 \in H\\) and a deformation map \\(T_1 : D \to D_1\\). Assume we have a generative model \\(G: H \to H\\) and an initial noise image \\(\xi_0 \in H\\) such that \\(G(\xi_0) = f_0\\). From definition, the new frame of our video is \\(f_1 = f_0 \circ T_1^{-1}\\) on \\(D \cap D_1\\). If \\(D \subseteq D_1\\), it might seem that our generative model is unnecessary. However, proceeding this way generates blurry and unrealistic videos.

The primary issue is that, in practice, we don’t have access to \\(f_0\\) at an infinite resolution but only at a fixed, finite set of grid points \\(E_k = \{x_1, \dots, x_k\} \subset D\\). To determine \\(f_1\\) on our grid points, we need the values of \\(f_0\\) at the points \\(T_1^{-1}(E_k) = \{T_1^{-1}(x_1), \dots, T_1^{-1}(x_k)\}\\). It’s highly unlikely that \\(E_k = T_1^{-1}(E_k)\\) for any realistic deformation.

Thus, we must interpolate \\(f_0\\) to \\(T_1^{-1}(E_k)\\), which usually leads to blurry results with standard methods. Furthermore, if \\(D \not \subseteq D_1\\), there will be regions where \\(f_1\\) is not determined by \\(f_0\\) and will need to be inpainted on the new visible domain. Therefore, for each frame, we must solve an interpolation and an inpainting problem: tasks for which generative models are well-suited.

Suppose we have access to the noise function \\(\xi_0\\) at infinite resolution, and its domain extends to all of \\(\mathbb{R}^2\\); we discuss both in Section <a href="#sec:grf" data-reference-type="ref" data-reference="sec:grf">3.1</a>. We can then define the new frame in our video by applying the generative model to the deformed noise: \\(f_1 = G (\xi_0 \circ T_1^{-1})\\). The deformed noise function \\(\xi_0 \circ T_1^{-1}\\) gets its values from \\(\xi_0|_D\\) for points in \\(D \cap D_1\\) and from the extension of \\(\xi_0\\) to \\(\mathbb{R}^2\\) for all other points where inpainting is needed. To ensure this definition is consistent with <a href="#eq:continuity_eq" data-reference-type="eqref" data-reference="eq:continuity_eq">[eq:continuity_eq]</a>, \\(G\\) must be equivariant with respect to \\(T_1^{-1}\\). Specifically, for all \\(\xi \in \text{supp}(\eta) \subseteq H\\), we must have \\[\label{eq:equivariance}
    G \big ( \xi \circ T^{-1}_1 \big ) (x) = G \big ( \xi \big ) \big ( T^{-1}_1 (x) \big ), \qquad \forall \; x \in D \cap D_1.\\] Assuming <a href="#eq:equivariance" data-reference-type="eqref" data-reference="eq:equivariance">[eq:equivariance]</a>, it follows from \\(G(\xi_0) = f_0\\), that \\(f_1(x) = G(\xi_1 |_D)(x) = (f_0 \circ T^{-1}_1)(x)\\) for all \\(x \in D \cap D_1\\) hence the pair \\((f_0,f_1)\\) is a valid 2 frame video according to the definition of Section <a href="#subsec:generaitve_functions" data-reference-type="ref" data-reference="subsec:generaitve_functions">2.1</a>. To generate a video with any number of frames, we simply iterate on this process with a given sequence of deformation maps. Enforcing <a href="#eq:equivariance" data-reference-type="eqref" data-reference="eq:equivariance">[eq:equivariance]</a> can be done directly by the architectural design, through training with various deformation maps, or, through a guidance process; see Section <a href="#sec:diffusion_guidance" data-reference-type="ref" data-reference="sec:diffusion_guidance">3.2</a>.

## White Noise [sec:white_noise]

It is common practice to train generative models assuming the reference measure \\(\eta\\) is Gaussian white noise. Specifically, a draw \\(\xi \sim \eta\\) on the grid points \\(E_k = \{x_1, \dots, x_k\} \subset D\\) is realized as \\(\xi(x_l) = \chi_l\\) for an i.i.d. sequence \\(\chi_l \sim \mathcal{N}(0,1)\\) for \\(l=1, \dots, k\\). However, this approach is incompatible with our goal of having the generative model perform interpolation. For most deformations \\(T\\) encountered in practice, none of the points in \\(T^{-1}(E_k)\\) will match those in \\(E_k\\). Consequently, each new evaluation \\(\xi \big(T^{-1}(x_l)\big)\\) will be independent of the sequence \\(\{\chi_l\}_{l=1}^k\\), making \\(\xi \big(T^{-1}(E_k)\big)\\) appear as a new noise realization unrelated to \\(\xi(E_k)\\). This incompatibility arises because white noise processes are distributions, not regular functions, meaning realizations are almost surely not members of \\(H\\) `\cite{da2014stochastic}`{=latex}. `\cite{chang2023warped}`{=latex} proposes a stochastic interpolation method to address this issue (see Appendix <a href="#app:brownian_bridge" data-reference-type="ref" data-reference="app:brownian_bridge">10</a> for details and comparison). We generalize this idea and propose using generic Gaussian processes on \\(H\\).

# Method: Warped Diffusion [sec:method]

In Section <a href="#sec:problem" data-reference-type="ref" data-reference="sec:problem">[sec:problem]</a>, we formulated the problem of video generation as the computation of a series of functions warped by an optical flow and proposed the use of a generative model for inpainting and interpolating the warped functions. The main challenges which remain are defining a functional noise process which can be evaluated continuously and a generative model which is equivariant with respect to warping. We propose to use Gaussian processes for our functional noise and a guidance procedure within the sampling step of a diffusion model to overcome these challenges.

## Gaussian Processes (GPs) [sec:grf]

A Gaussian Process (GP) \\(\eta\\) is a probability measure on \\(H\\) completely specified by its mean element and covariance operator. For a mathematical introduction, see Appendix <a href="#app:gp" data-reference-type="ref" data-reference="app:gp">9</a>. We identify Gaussian processes with positive-definite kernel functions \\(\kappa : \mathbb{R}^2 \times \mathbb{R}^2 \to \mathbb{R}\\). Recall that \\(E_k = \{x_1, \dots, x_k\}\\) denotes the grid points where we know the values of an image \\(f \in H\\). To realize a random function \\(\xi \sim \eta\\) on these points, we sample the finite-dimensional multivariate Gaussian \\(N(0, Q)\\), where \\(Q \in \mathbb{R}^{k \times k}\\) is the kernel matrix \\(Q_{ij} = \kappa(x_i, x_j)\\) for \\(i, j = 1, \dots, k\\).

Once sampled, given the fixed values \\(\xi(E_k)\\), \\(\xi\\) can be evaluated at any new point \\(x^* \in D\\) by computing the conditional distribution \\(\xi(x^*) \mid \xi(E_k)\\) `\cite{rasmussen2005gaussian}`{=latex}. This approach allows us to realize random functional samples at infinite resolution through conditioning, thus resolving the interpolation problem. Furthermore, by ensuring the kernel \\(\kappa\\) is positive definite on a domain larger than \\(D\\), we can consistently sample \\(\xi\\) outside of \\(D\\), addressing the inpainting problem described in Section <a href="#subsec:equivariance" data-reference-type="ref" data-reference="subsec:equivariance">2.2</a>.

For high-resolution images when \\(k\\) is large, working with the matrix \\(Q\\) can be computationally expensive. Instead, we propose using **R**andom **F**ourier **F**eatures (RFF) to sample \\(\eta\\), which amounts to a finite-dimensional projection of the function \\(\xi \sim \eta\\) that converges in the limit of infinite features `\cite{rahimi2007random, wilson2020efficiently}`{=latex}. We can approximate samples from a GP with a squared exponential kernel with length-scale parameter \\(\epsilon > 0\\) by \\(\xi (x) = \sqrt{\frac{2}{J}} \sum_{j=1}^J w_j \cos \big ( \langle z_j, x \rangle + b_j \big )\\) for i.i.d. sequences \\(w_j \sim N(0,1)\\), \\(z_j \sim N(0,\epsilon^{-2} I_2)\\), \\(b_j \sim U(0, 2 \pi)\\) where \\(J \in \mathbb{N}\\) is the number of features. RFF allows us access to \\(\xi\\) at infinite resolution on the entirety of the plane while also allowing for efficient computation.

## Function Space Diffusion Models and Equivariance Self-Guidance [sec:diffusion_guidance]

We will now focus on the generative model that needs to be equivariant to the noise transformations. Specifically, in this section, i) we introduce function space diffusion models, ii) we prove that if every prediction of the diffusion model is equivariant then the whole diffusion model sampling chain is equivariant to the underlying spatial transformations, and, iii) we describe *equivariance self-guidance*, our sampling technique for enforcing the equivariance assumption.

For ease of notation, we will present everything for the case of unconditional video generation. However, our method seamlessly incorporates any addition conditioning information that may be available. If \\(c_0,\dots,c_n \in \mathbb{R}^c\\) is a sequence of known conditioning vectors then these can simply be passed into a conditional score model at the appropriate frame without any other change to our method; see Algorithm <a href="#alg" data-reference-type="ref" data-reference="alg">3</a>. Conditioning vectors could be, for example, low resolutions versions of a video or an original video with regions masked. In Section <a href="#sec:experiments" data-reference-type="ref" data-reference="sec:experiments">4</a>, we focus on such conditional tasks.

**Function Space Diffusion Models.** Typically, diffusion models are trained with white noise. As explained in Section <a href="#sec:white_noise" data-reference-type="ref" data-reference="sec:white_noise">2.3</a>, a principled continuous evaluation of the noise requires a functional process. We briefly describe diffusion models in the context of sampling using the Gaussian processes of Section <a href="#sec:grf" data-reference-type="ref" data-reference="sec:grf">3.1</a>. We show in Section <a href="#sec:training_correlated" data-reference-type="ref" data-reference="sec:training_correlated">4.1</a> (Table <a href="#tab:single_frame_results" data-reference-type="ref" data-reference="tab:single_frame_results">1</a>) that a model trained with white noise can be fine-tuned to GP noise without any loss in performance.

While it is possible to formulate diffusion models on the infinite-dimensional space \\(H\\) e.g. `\cite{lim2023score}`{=latex}, we will proceed in the finite-dimensional case for ease of exposition. In particular, we will define the forward and backward process as a flow on a vector \\(u \in \mathbb{R}^k\\), thinking of the entries as the values of a scalar function evaluated on the grid \\(E_k\\) and recall that \\(Q\\) is the kernel matrix on \\(E_k\\).

We consider forward processes of the form, \\[\label{eq:forward_diffusion}
    \mathsf{d} u_t = \big ( 2 \sigma(t) \dot{\sigma} (t) Q \big )^{1/2} \mathsf{d} W_t, \quad u(0) = u_0 \sim \mu\\] where \\(W_t\\) is a standard Wiener process on \\(\mathbb{R}^k\\) and \\(\sigma\\) is a scalar-valued, once differentiable function. This process results in conditional distributions \\(p(u_t | u_0) = N(u_0, \sigma^2 (t) Q)\\), see `\cite{karras2022elucidating}`{=latex}. Let \\(p(u_t,t)\\) denote the density of \\(u_t\\) induced by <a href="#eq:forward_diffusion" data-reference-type="eqref" data-reference="eq:forward_diffusion">[eq:forward_diffusion]</a>. Then the following backward in time ODE, \\[\label{eq:probability_flow_ode}
    \frac{\mathsf{d} u_t}{\mathsf{d} t} = - \sigma(t) \dot{\sigma}(t) Q \nabla_u \log p(u_t, t)\\] started at \\(u(\tau)\\) distributed according to <a href="#eq:forward_diffusion" data-reference-type="eqref" data-reference="eq:forward_diffusion">[eq:forward_diffusion]</a> has the same marginal distributions \\(p(u_t,t)\\) as <a href="#eq:forward_diffusion" data-reference-type="eqref" data-reference="eq:forward_diffusion">[eq:forward_diffusion]</a> on the interval \\([0,\tau]\\); see `\cite{karras2022elucidating}`{=latex}. Approximating \\(N(u_0, \sigma^2 (\tau) Q)\\) by \\(N(0, \sigma^2 (\tau) Q)\\), we may then define the generative model \\(G\\) by the mapping \\(u(\tau) \mapsto u(0)\\) with reference measure \\(\eta = N(0, \sigma^2 (\tau) Q)\\).

Solving <a href="#eq:probability_flow_ode" data-reference-type="eqref" data-reference="eq:probability_flow_ode">[eq:probability_flow_ode]</a> requires knowledge of the score \\(\nabla_u \log p(u_t,t)\\). Instead of learning the score, we opt for directly learning the weighted score \\(Q \nabla_u \log p(u_t,t)\\). This design choice leads to faster sampling since we do not need to perform any expensive matrix multiplication with \\(Q\\) at inference time.

A generalized version of Tweedie’s formula (for proof see Appendix <a href="#app:tweedie" data-reference-type="ref" data-reference="app:tweedie">8.2</a>) implies: \\[\label{eq:tweedie}
    Q \nabla_u \log p(u_t,t) = \frac{{\mathbb{E}}[u_0 | u_t] - u_t}{\sigma^2 (t)}.\\] We approximate \\({\mathbb{E}}[u_0 | u_t]\\) with a neural network \\(h_\theta\\) by minimizing the denoising objective: \\[\label{eq:objective_func}
    {\mathbb{E}}_{t \sim U(0,\tau)} {\mathbb{E}}_{u_0 \sim \mu} {\mathbb{E}}_{u_t \sim N(u_0, \sigma^2 (t) Q)} |h_\theta(u_t,t) - u_0|^2.\\] Having a minimizer \\(h_\theta\\) of <a href="#eq:objective_func" data-reference-type="eqref" data-reference="eq:objective_func">[eq:objective_func]</a> gives us access to the weighted score \\(Q \nabla_u \log p(u_t,t)\\) via <a href="#eq:tweedie" data-reference-type="eqref" data-reference="eq:tweedie">[eq:tweedie]</a>. We may then obtain an approximate solution to the map \\(u(\tau) \mapsto u(0)\\) by discretizing <a href="#eq:probability_flow_ode" data-reference-type="eqref" data-reference="eq:probability_flow_ode">[eq:probability_flow_ode]</a> in time. We consider Euler scheme updates given by \\[\label{eq:euler_scheme}
    u_{t-\Delta t} = u_t - \Delta t \frac{\dot{\sigma} (t)}{\sigma (t)} \big (h_\theta(u_t,t) - u_t \big ).\\] started with \\(u_\tau \sim N(0, \sigma^2 (\tau) Q)\\) for some time step \\(\Delta t >0\\).

**Equivariance for the Probability Flow ODE.** <span id="sec:equiv_for_flow" label="sec:equiv_for_flow"></span> Since the diffusion model works with discrete inputs, we need to introduce a discretization of <a href="#eq:equivariance" data-reference-type="eqref" data-reference="eq:equivariance">[eq:equivariance]</a> for the network. For a deformation \\(T_1\\), we define equivariance as \\[\label{eq:discrete_score_equivariance}
    h_\theta (u_t \circ T^{-1}_1, t) \circ T_1 = h_\theta (u_t, t),\\] which is obtained from composing both sides of <a href="#eq:equivariance" data-reference-type="eqref" data-reference="eq:equivariance">[eq:equivariance]</a> with \\(T_1\\). Note that <a href="#eq:discrete_score_equivariance" data-reference-type="eqref" data-reference="eq:discrete_score_equivariance">[eq:discrete_score_equivariance]</a> is valid only for pixels which stay within frame and we compute the l.h.s. with bilinear interpolation on the network output. The input to the network on the l.h.s. is computed with RFFs without any interpolation. Given this discrete equivariance is satisfied for every prediction of the network, it is straightforward to show that the whole diffusion model sampling chain will be equivariant. Indeed, the whole approximation to \\(u(\tau) \mapsto u(0)\\) is equivariant by the linearity of composition – for a full derivation, see Appendix <a href="#app:flow_equivariance" data-reference-type="ref" data-reference="app:flow_equivariance">8.3</a>.

**Equivariance Self-Guidance.** The condition <a href="#eq:discrete_score_equivariance" data-reference-type="eqref" data-reference="eq:discrete_score_equivariance">[eq:discrete_score_equivariance]</a> is rarely satisfied for deformations \\(T_1\\) arising in practical settings. This is because either the conditional expectation \\({\mathbb{E}}[u_0 | u_t]\\) is not equivariant with respect to \\(T^{-1}_1\\) or the neural network approximation has not fully captured it. If the underlying equivariance assumption breaks, methods that rely solely on noise warping for temporal consistency, e.g. `\cite{chang2023warped}`{=latex}, will perform poorly. This is evident in challenging conditional tasks (see Figure <a href="#fig:teaser" data-reference-type="ref" data-reference="fig:teaser">1</a>).

A potential solution is to directly train the network by adding <a href="#eq:discrete_score_equivariance" data-reference-type="eqref" data-reference="eq:discrete_score_equivariance">[eq:discrete_score_equivariance]</a> as a regularizer. However, this requires large amounts of video data from which to extract optical flows. Furthermore, by satisfying <a href="#eq:discrete_score_equivariance" data-reference-type="eqref" data-reference="eq:discrete_score_equivariance">[eq:discrete_score_equivariance]</a> over a large class of \\(T_1\\)(s), the network may become less apt at satisfying <a href="#eq:objective_func" data-reference-type="eqref" data-reference="eq:objective_func">[eq:objective_func]</a> and lose its generative abilities. Therefore, we opt for *guiding the model towards equivariant solutions at inference time*.

We first sample noise \\(u_\tau^{(0)}\\) and generate the first frame following <a href="#eq:euler_scheme" data-reference-type="eqref" data-reference="eq:euler_scheme">[eq:euler_scheme]</a>, keeping the outputs of the network at each time step \\(\{h_\theta (u^{(0)}_t, t)\}\\). To generative the next frame, we warp our noise \\(u^{(1)}_\tau = u^{(0)}_\tau \circ T_1^{-1}\\) with RFFs and again follow <a href="#eq:euler_scheme" data-reference-type="eqref" data-reference="eq:euler_scheme">[eq:euler_scheme]</a> but this time using <a href="#eq:discrete_score_equivariance" data-reference-type="eqref" data-reference="eq:discrete_score_equivariance">[eq:discrete_score_equivariance]</a> as guidance. In particular, we take a gradient steps in the direction of the loss function \\(|h_\theta (u^{(1)}_t, t) \circ T_1 - h_\theta(u^{(0)}_t,t)|^2\\), computed on the pixels that stay within frame. All frames can be generated by iterating this procedure as summarized in Algorithm <a href="#alg" data-reference-type="ref" data-reference="alg">3</a> (and visualized in Figures <a href="#fig:fig2" data-reference-type="ref" data-reference="fig:fig2">2</a>, <a href="#fig:fig3" data-reference-type="ref" data-reference="fig:fig3">42</a>) which also shows how to use conditioning information. Guidance is typically used to solve inverse problems with diffusion models (e.g. see `\citep{chung2022diffusion}`{=latex}), but here the guidance is applied to align the model with its own past predictions. We emphasize that to compute the composition with \\(T_1\\) above, we use bilinear interpolation on the network outputs but we never need to interpolate the network inputs since we can compute the warping via RFFs. Furthermore, since we are matching interpolated outputs to ones that are not interpolated, our output images remain sharp. This in contrast to directly using a discrete version of <a href="#eq:equivariance" data-reference-type="eqref" data-reference="eq:equivariance">[eq:equivariance]</a> which would suggest that we match network outputs to interpolated images, producing blurry results.

<figure id="alg">
<div class="algorithmic">
<p>ALGORITHM BLOCK (caption below)</p>
<p><br />
Require Conditioning vectors <span class="math inline">{<em>c</em><sub><em>j</em></sub>}<sub><em>j</em> = 0</sub><sup><em>n</em></sup></span>, Step <span class="math inline"><em>Δ</em><em>t</em></span>, Time <span class="math inline"><em>τ</em></span>, Schedule <span class="math inline"><em>σ</em>(<em>t</em>)</span>, Model <span class="math inline"><em>h</em><sub><em>θ</em></sub></span>, Guidance Strength <span class="math inline"><em>λ</em></span>.<br />
<span class="math inline">{<em>T</em><sub><em>j</em></sub>, <em>T</em><sub><em>j</em></sub><sup>−1</sup>}<sub><em>j</em> = 1</sub><sup><em>n</em></sup> ← <code>compute_optical_flow</code>({<em>c</em><sub><em>j</em></sub>}<sub><em>j</em> = 0</sub><sup><em>n</em></sup>)</span><br />
<span class="math inline"><em>u</em><sub><em>τ</em></sub><sup>(0)</sup> ∼ GP</span> using RFFs in Section <a href="#sec:grf" data-reference-type="ref" data-reference="sec:grf">3.1</a> # comment: <span>Fresh noise sample for the first frame</span><br />
Compute trajectory <span class="math inline">{<em>u</em><sub><em>t</em></sub><sup>(0)</sup>}</span> using <a href="#eq:euler_scheme" data-reference-type="eqref" data-reference="eq:euler_scheme">[eq:euler_scheme]</a> # comment: <span>Sample first frame</span><br />
<strong>For</strong> <span><span class="math inline"><em>j</em> ← 1</span> to <span class="math inline"><em>n</em></span></span><br />
<span class="math inline"><em>u</em><sub><em>τ</em></sub><sup>(<em>j</em>)</sup> ← <em>u</em><sub><em>τ</em></sub><sup>(<em>j</em> − 1)</sup> ∘ <em>T</em><sub><em>j</em></sub><sup>−1</sup></span> using RFFs # comment: <span>Warp noise from previous frame</span><br />
<span class="math inline"><em>t</em> ← <em>τ</em></span><br />
<strong>While</strong> <span><span class="math inline"><em>t</em> &gt; 0</span></span><br />
<span class="math inline">$u_{t - \Delta t}^{(j)} \gets u_t^{(j)} - \Delta t \frac{\dot{\sigma} (t)}{\sigma (t)} \big (h_\theta(u_t^{(j)},t, c_j) - u_t^{(j)} \big )$</span> # comment: <span>Take Euler step</span><br />
<span class="math inline"><em>e</em><sub><em>t</em></sub><sup>(<em>j</em>)</sup> ← |<em>h</em><sub><em>θ</em></sub>(<em>u</em><sub><em>t</em></sub><sup>(<em>j</em>)</sup>, <em>t</em>, <em>c</em><sub><em>j</em></sub>) ∘ <em>T</em><sub><em>j</em></sub> − <em>h</em><sub><em>θ</em></sub>(<em>u</em><sub><em>t</em></sub><sup>(<em>j</em> − 1)</sup>, <em>t</em>, <em>c</em><sub><em>j</em> − 1</sub>)|<sup>2</sup></span> # comment: <span>Compute warping error</span><br />
<span class="math inline">$u_{t - \Delta t}^{(j)} \gets u_{t - \Delta t}^{(j)} - \frac{\lambda}{\sqrt{e_t}} \nabla_u e_t^{(j)}$</span> # comment: <span>Equivariance self guidance</span><br />
<span class="math inline"><em>t</em> ← <em>t</em> − <em>Δ</em><em>t</em></span><br />
EndWhile<br />
EndFor<br />
<span><strong>return</strong> video <span class="math inline">{<em>u</em><sub>0</sub><sup>(<em>j</em>)</sup>}<sub><em>j</em> = 0</sub><sup><em>n</em></sup></span></span></p>
</div>
<figcaption>Warped Diffusion – Temporal Consistency with Equivariance Self Guidance </figcaption>
</figure>

# Experimental Results [sec:experiments]

For all our experiments, we use Stable Diffusion XL `\citep{podell2024sdxl}`{=latex} (SDXL) as our base image diffusion model. We start by finetuning SDXL on conditional tasks. We choose super-resolution and inpainting as the tasks of interest since they are both commonly used in the inverse problems literature and they represent two distinct scenarios: in super-resolution, the input condition is strong and in inpainting, the model needs to generate new content. For super-resolution, we choose a downsampling factor of \\(8\\). For inpainting, we create masks of different shapes at random, following the work of `\citep{sdxl_inp}`{=latex}. During the finetuning, we train the model to predict the uncorrupted image given the following inputs: i) the encoding of the noised image, ii) the noise level, and, iii) the encoding of the corrupted (downsampled/masked) image. To condition on the corrupted observation, we concatenate the measurements across the channel dimension. We train models with and without correlated noise on the COYO dataset `\citep{kakaobrain2022coyo-700m}`{=latex} for \\(100\\)k steps. We show realizations of independent and correlated noise in Figure <a href="#fig:noise_visuals" data-reference-type="ref" data-reference="fig:noise_visuals">17</a>. Additional implementation details are in Section <a href="#sec:training_details" data-reference-type="ref" data-reference="sec:training_details">13.2</a>, including the parameters for the GP introduced in Section <a href="#sec:grf" data-reference-type="ref" data-reference="sec:grf">3.1</a>.

## Training with correlated noise [sec:training_correlated]

The first step is to assess the quality of the trained models. To do so, we take images from a test split of the COYO dataset, we corrupt them (either by masking or downsampling) and we measure the conditional performance of the trained models. We use a diverse set of metrics that are commonly used in the inverse problems literature: CLIP Text Score `\citep{radford2021learningclip}`{=latex}, CLIP Image Score `\citep{radford2021learningclip}`{=latex}, SSIM `\citep{wang2004imagessim}`{=latex}, LPIPS `\citep{zhang2018unreasonablelpips}`{=latex}, MSE, Inception Score `\citep{salimans2016improvedinception}`{=latex} and FID `\citep{heusel2017gansfid}`{=latex}. The first five metrics measure point-wise restoration performance. Inception Score measures the quality of the generated distribution (without an explicit reference distribution). Finally, FID measures restoration performance in a distributional sense, i.e. it measures how close is the distribution after restoration to the ground truth distribution.

We report our results for the super-resolution and inpainting models in Table <a href="#tab:single_frame_results" data-reference-type="ref" data-reference="tab:single_frame_results">1</a>. The main finding is that finetuning with correlated noise does not compromise performance, i.e. SDXL models finetuned with correlated noise perform on par with SDXL models that are trained with independent noise. Particularly for inpainting, the GP models slightly outperform models trained with independent noise across all metrics. We provide qualitative results for our all models in Figure <a href="#fig:teaser" data-reference-type="ref" data-reference="fig:teaser">1</a> and in Appendix Figures <a href="#fig:inp_visuals" data-reference-type="ref" data-reference="fig:inp_visuals">29</a>, <a href="#fig:super_res_visuals" data-reference-type="ref" data-reference="fig:super_res_visuals">41</a>.

We remark that the advantages of using an initial distribution other than white noise have been explored in prior work `\cite{daras2022soft, bansal2022cold, hoogeboom2022blurring}`{=latex}. Our new finding is that a model initially trained with white noise can be easily fine-tuned to work with correlated noise. To the best of our knowledge, ours is the first work that shows that Stable Diffusion XL can be fine-tuned to work with correlated noise.

We underline that prior to fine-tuning Stable Diffusion XL produces unrealistic images when the sampling chain is initialized with correlated noise. Our experiments show that post-finetuning, the model can handle spatially correlated noise in the input without compromising performance. Our GP Warping mechanism requires models that can handle correlated noise. Hence, these fine-tunings are essential for the rest of the paper.

<div id="tab:single_frame_results" markdown="1">

| Model | FID \\(\downarrow\\) | Inception \\(\uparrow\\) | CLIP Txt \\(\uparrow\\) | CLIP Img \\(\uparrow\\) | SSIM \\(\uparrow\\) | LPIPS \\(\downarrow\\) | MSE \\(\downarrow\\) |
|:---|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| Super-resolution GP | **37.514** | **11.917** | **0.272\\({\pm}\\)`<!-- -->`{=html}0.042** | 0.955\\({\pm}\\)0.029 | 0.770\\({\pm}\\)0.106 | 0.253\\({\pm}\\)0.076 | 0.004\\({\pm}\\)0.004 |
| Super-resolution Indep. | 40.843 | 11.679 | 0.271\\({\pm}\\)0.041 | **0.957\\({\pm}\\)`<!-- -->`{=html}0.027** | **0.785\\({\pm}\\)`<!-- -->`{=html}0.108** | **0.242\\({\pm}\\)`<!-- -->`{=html}0.078** | 0.004\\({\pm}\\)0.004 |
| Inpainting GP | **58.727** | **11.769** | **0.276\\({\pm}\\)`<!-- -->`{=html}0.042** | **0.929\\({\pm}\\)`<!-- -->`{=html}0.060** | **0.798\\({\pm}\\)`<!-- -->`{=html}0.134** | **0.181\\({\pm}\\)`<!-- -->`{=html}0.122** | **0.056\\({\pm}\\)`<!-- -->`{=html}0.084** |
| Inpainting Indep. | 61.380 | 11.707 | 0.275\\({\pm}\\)0.048 | 0.913\\({\pm}\\)0.089 | 0.778\\({\pm}\\)0.161 | 0.198\\({\pm}\\)0.134 | 0.057\\({\pm}\\)0.076 |

Single-frame evaluation of super-resolution and inpainting models.

</div>

<span id="tab:single_frame_results" label="tab:single_frame_results"></span>

## Noise Warping and Equivariance Self Guidance

In the previous experiments, we measured the restoration performance of the trained models for a single image and we established that models trained with correlated noise perform on par (or even outperform) models trained with independent noise. The next step is to measure the temporal behavior of the models, i.e. how well they work for videos.

**Noise Warping baselines.** As explained in Section <a href="#sec:problem" data-reference-type="ref" data-reference="sec:problem">[sec:problem]</a>, to apply image diffusion models to videos, we need to transform the noise as we move from one frame to the next. We consider the following noise-warping baselines that were used in `\citep{chang2023warped}`{=latex}: **Fixed noise** uses the same noise across all the frames. **Resample noise** samples a new noise for each new frame. **Nearest Neighbor** uses the noise of the nearest location in the grid to evaluate the noise at the location that is not on the regular grid \\(E_k\\). **Bilinear Interpolation** interpolates the values of the noise bilinearly in the neighboring locations that lie on the grid. **How I Warped Your Noise `\citep{chang2023warped}`{=latex}** is the state-of-the-art method for solving temporally correlated inverse problems with image diffusion models. It warps the noise by using auxiliary high-resolution noise maps (see our intro, related work section, and Section <a href="#app:brownian_bridge" data-reference-type="ref" data-reference="app:brownian_bridge">10</a>). **Our GP Noise Warping** warps the input noise by resampling the Gaussian process in the mapped locations. We note that the Fixed Noise, Resample Noise, and Nearest Neighbor noise warping methods can be applied to models that are trained with either independent noise or correlated noise coming from a GP. For all the experiments, we also include our proposed method, ***Warped Diffusion*** that uses GP Noise Warping and Equivariance Self-Guidance (see Algorithm <a href="#alg:alg1" data-reference-type="ref" data-reference="alg:alg1">[alg:alg1]</a> for a reference implementation).

**Video Evaluation Metrics.** We follow the evaluation methodology of the “How I Warped Your Noise“ paper `\citep{chang2023warped}`{=latex}. Specifically, we want to measure two different aspects of our method: i) average restoration performance across frames, ii) temporal consistency. For i), we measure the average of all the previously reported metrics (FID, Inception, CLIP Image/Text score, SSIM, LPIPS and MSE) across the frames. For ii), we measure the self-warping error, i.e. how consistent are the model’s predictions across time. The warping error can be computed in either pixel or latent space and also with respect to the first generated frame or the previously generated frame, totaling \\(4\\) warping errors.

To warm up, we start with videos that are synthetically generated by 2-D shifting of a single image, as in Figure <a href="#fig:teaser" data-reference-type="ref" data-reference="fig:teaser">1</a>. To further simplify the setup, we consider the easy case of shifting the current frame by an integer amount of pixels with each new frame. For 2-D translations by an integer amount of pixels, the Nearest Neighbor, Bilinear Interpolation, How I Warped Your Noise and GP Noise Warping methods they become essentially the same since we always evaluate the noise distribution on points in the grid \\(E_k\\). Hence, the only difference is whether we apply these methods to white noise or to GPs.

Figure <a href="#fig:teaser" data-reference-type="ref" data-reference="fig:teaser">1</a> (Row 2) shows that the How I Warped Your Noise baseline produces temporally inconsistent results as we shift the masked input image. Even though all the inpaintings are of high quality, the baseline results are temporally inconsistent. Instead, our *Warped Diffusion* method produces temporally consistent results since it enforces equivariance by design. Since the How I Warped Your Noise warping mechanism and GP coincide here, the benefit strictly comes from enforcing the equivariance property. In fact, one could get the same results for the How I Warped Your Noise method by penalizing for equivariance at inference time.

We present quantitative results regarding temporal consistency in Figure <a href="#fig:noise_wp_comparisons_inpainting" data-reference-type="ref" data-reference="fig:noise_wp_comparisons_inpainting">6</a> (and additional results in Figure <a href="#fig:noise_wp_comparisons_inpainting_part_2" data-reference-type="ref" data-reference="fig:noise_wp_comparisons_inpainting_part_2">9</a> in the Appendix). As shown in the Figure, the fixed noise and the resample noise baselines perform the worst w.r.t. the temporal consistency both in latent and pixel space. The warping error of the Resample baseline is almost constant across frames as expected, while the warping error of the Fixed Noise increases with time. Both the How I Warped Your Noise method and our GP warping framework significantly improve the baselines. Yet, they still have significant temporal inconsistencies as evidenced by the results in Figure <a href="#fig:teaser" data-reference-type="ref" data-reference="fig:teaser">1</a> and the supplemental videos. The two methods perform on par on this task since they are essentially the same when it comes to integer shifts: the only difference is that GP Noise Warping is applied to correlated noise coming from a GP. The remaining temporal errors are not an artifact of the noise warping mechanism but they are due to the fact that the model itself is not equivariant w.r.t. the underlying transformation. The warping errors essentially disappear when we apply Equivariance Self Guidance. As shown in Figure <a href="#fig:noise_wp_comparisons_inpainting" data-reference-type="ref" data-reference="fig:noise_wp_comparisons_inpainting">6</a>, our method, Warped Diffusion, achieves almost \\(0\\) warping error (1e-4 mean pixel error with respect to the first frame to be precise) since it is enforcing equivariance by design.

The only remaining question is whether Warped Diffusion maintains good restoration performance. To answer this, we measure mean restoration performance across frames for the aforementioned metrics. We report our results in Table <a href="#tab:comp_inpainting" data-reference-type="ref" data-reference="tab:comp_inpainting">2</a>, including the mean warping error with respect to the first frame. As shown, Warped Diffusion maintains high performance across all the considered metrics while being significantly superior in terms of temporal consistency. The conclusion is that all the other noise warping baselines, including the previous state-of-the-art How I Warped Your Noise paper `\citep{chang2023warped}`{=latex}, perform poorly in terms of temporal consistency since they rely on the assumption that the network is equivariant. Even for simple temporal correlations such as integer movement in the 2-D space, this assumption is false for the challenging inpainting task. Warped Diffusion is the only method that achieves temporal consistency while it still manages to maintain high reconstruction performance.

<div id="tab:comp_inpainting" markdown="1">

| Method | Warping Err \\(\downarrow\\) | FID \\(\downarrow\\) | Inception \\(\uparrow\\) | CLIP Txt \\(\uparrow\\) | CLIP Img \\(\uparrow\\) | SSIM \\(\uparrow\\) | LPIPS \\(\downarrow\\) | MSE \\(\downarrow\\) |
|:---|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| Fixed (gp) | 0.129\\({\pm}\\)`<!-- -->`{=html}0.022 | 60.853\\({\pm}\\)`<!-- -->`{=html}2.908 | **12.421\\({\pm}\\)`<!-- -->`{=html}0.761** | **0.280\\({\pm}\\)`<!-- -->`{=html}0.003** | 0.924\\({\pm}\\)`<!-- -->`{=html}0.005 | 0.800\\({\pm}\\)`<!-- -->`{=html}0.001 | **0.182\\({\pm}\\)`<!-- -->`{=html}0.001** | 0.060\\({\pm}\\)`<!-- -->`{=html}0.002 |
| Fixed (indep) | 0.080\\({\pm}\\)`<!-- -->`{=html}0.014 | 67.021\\({\pm}\\)`<!-- -->`{=html}2.696 | 10.301\\({\pm}\\)`<!-- -->`{=html}0.392 | 0.275\\({\pm}\\)`<!-- -->`{=html}0.002 | 0.919\\({\pm}\\)`<!-- -->`{=html}0.004 | 0.780\\({\pm}\\)`<!-- -->`{=html}0.001 | 0.195\\({\pm}\\)`<!-- -->`{=html}0.001 | 0.059\\({\pm}\\)`<!-- -->`{=html}0.002 |
| Resample (indep) | 0.101\\({\pm}\\)`<!-- -->`{=html}0.006 | 71.078\\({\pm}\\)`<!-- -->`{=html}4.185 | 11.740\\({\pm}\\)`<!-- -->`{=html}0.435 | 0.277\\({\pm}\\)`<!-- -->`{=html}0.002 | 0.921\\({\pm}\\)`<!-- -->`{=html}0.004 | 0.781\\({\pm}\\)`<!-- -->`{=html}0.006 | 0.196\\({\pm}\\)`<!-- -->`{=html}0.002 | 0.061\\({\pm}\\)`<!-- -->`{=html}0.003 |
| Resample (gp) | 0.141\\({\pm}\\)`<!-- -->`{=html}0.008 | 60.029\\({\pm}\\)`<!-- -->`{=html}4.389 | 11.318\\({\pm}\\)`<!-- -->`{=html}0.403 | 0.277\\({\pm}\\)`<!-- -->`{=html}0.002 | **0.925\\({\pm}\\)`<!-- -->`{=html}0.003** | **0.806\\({\pm}\\)`<!-- -->`{=html}0.005** | **0.182\\({\pm}\\)`<!-- -->`{=html}0.002** | **0.056\\({\pm}\\)`<!-- -->`{=html}0.003** |
| How I Warped (indep) | 0.046\\({\pm}\\)`<!-- -->`{=html}0.007 | 68.701\\({\pm}\\)`<!-- -->`{=html}2.938 | 10.877\\({\pm}\\)`<!-- -->`{=html}0.432 | 0.276\\({\pm}\\)`<!-- -->`{=html}0.001 | 0.910\\({\pm}\\)`<!-- -->`{=html}0.005 | 0.781\\({\pm}\\)`<!-- -->`{=html}0.001 | 0.197\\({\pm}\\)`<!-- -->`{=html}0.001 | 0.067\\({\pm}\\)`<!-- -->`{=html}0.001 |
| GP Warping | 0.061\\({\pm}\\)`<!-- -->`{=html}0.010 | **59.897\\({\pm}\\)`<!-- -->`{=html}3.718** | 11.727\\({\pm}\\)`<!-- -->`{=html}0.375 | 0.277\\({\pm}\\)`<!-- -->`{=html}0.002 | 0.924\\({\pm}\\)`<!-- -->`{=html}0.004 | 0.803\\({\pm}\\)`<!-- -->`{=html}0.002 | **0.182\\({\pm}\\)`<!-- -->`{=html}0.001** | 0.057\\({\pm}\\)`<!-- -->`{=html}0.002 |
| **Warped Diffusion** (Ours) | **0.001\\({\pm}\\)`<!-- -->`{=html}0.001** | 61.249\\({\pm}\\)`<!-- -->`{=html}2.499 | 11.802\\({\pm}\\)`<!-- -->`{=html}0.427 | 0.276\\({\pm}\\)`<!-- -->`{=html}0.001 | 0.917\\({\pm}\\)`<!-- -->`{=html}0.006 | 0.779\\({\pm}\\)`<!-- -->`{=html}0.011 | 0.188\\({\pm}\\)`<!-- -->`{=html}0.006 | 0.058\\({\pm}\\)`<!-- -->`{=html}0.001 |

Mean-frame evaluation of inpainting models for the translation task.

</div>

<span id="tab:comp_inpainting" label="tab:comp_inpainting"></span>

We finally remark that our sampling algorithm enforces equivariance in the latent space. Yet, the warping errors are negligible in the pixel space as well. Our finding is that improving latent space equivariance translates to improvements in pixel space equivariance. The authors of `\citep{chang2023warped}`{=latex} also find that “the VAE decoder is translationally equivariant in a discrete way”.

<figure id="fig:noise_wp_comparisons_inpainting">
<figure id="fig:inp_noise_wp_first_latent">
<img src="./figures/self_warping_latent_error_wrt_first_frame.png"" />
<figcaption>Self-warping error w.r.t. first frame in latent space.</figcaption>
</figure>
<figure id="fig:inp_noise_wp_first_pixel">
<img src="./figures/self_warping_pixel_error_wrt_first_frame.png"" />
<figcaption>Self-warping error w.r.t. first frame in pixel space.</figcaption>
</figure>
<figcaption>Self-warping error w.r.t. first frame for the inpainting task as we shift the input frame.</figcaption>
</figure>

## Effect of Sampling Guidance for more general transformations

We proceed to evaluate our method on realistic videos. We measure performance on 600 captioned videos from the FETV `\citep{liu2023fetv}`{=latex} dataset. Since baseline inpainting methods fail even for very simple temporal transformations, we focus on \\(8\times\\) super-resolution for our comparisons on FETV.

For our video results, we could not provide comparisons with the How I Warped Your Noise paper. At the time of this writing, there was no available reference implementation as we confirmed with the authors by direct communication. In any case, the authors acknowledge as a limitation of their work that their proposed method has “limited impact on temporal coherency” when applied to latent models and that “all the noise schemes produce temporally inconsistent results” `\citep{chang2023warped}`{=latex}. Once again, we attribute this to the non-equivariance of the denoiser, which we mitigate with our guidance algorithm.

We proceed to evaluate our method and the baselines with respect to temporal consistency and mean restoration performance across frames, as we did for our inpainting experiments. We present our results in Table <a href="#tab:noise_warping_comp_videos" data-reference-type="ref" data-reference="tab:noise_warping_comp_videos">3</a> and additional results in Figures <a href="#fig:noise_wp_comparisons_video" data-reference-type="ref" data-reference="fig:noise_wp_comparisons_video">14</a>, <a href="#fig:super_res_visuals" data-reference-type="ref" data-reference="fig:super_res_visuals">41</a> of the Appendix and in the following URL as videos: <https://giannisdaras.github.io/warped_diffusion.github.io/>. As shown in Table <a href="#tab:noise_warping_comp_videos" data-reference-type="ref" data-reference="tab:noise_warping_comp_videos">3</a>, there is a trade-off between temporal consistency and restoration performance. Methods that perform better in terms of temporal consistency often have significantly worse performance across the other metrics. Our Warped Diffusion achieves a sweet spot: it has the lowest warping error by a large margin and it still maintains competitive performance across all the other metrics. On the contrary, methods that are based solely on noise warping, such as GP Warping and the simple interpolation methods, lead to significant performance deterioration for a small improvement in temporal consistency.

<div id="tab:noise_warping_comp_videos" markdown="1">

| Method | Warping Err \\(\downarrow\\) | FID \\(\downarrow\\) | Inception \\(\uparrow\\) | CLIP Txt \\(\uparrow\\) | CLIP Img \\(\uparrow\\) | SSIM \\(\uparrow\\) | LPIPS \\(\downarrow\\) | MSE \\(\downarrow\\) |  |
|:---|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| Fixed (indep) | 0.940\\({\pm}\\)0.312 | 48.764\\({\pm}\\)2.592 | 8.746\\({\pm}\\)1.325 | 0.227\\({\pm}\\)0.002 | 0.948\\({\pm}\\)0.013 | **0.716\\({\pm}\\)`<!-- -->`{=html}0.023** | 0.188\\({\pm}\\)0.018 | 0.005\\({\pm}\\)0.001 |  |
| Resample (indep) | 0.934\\({\pm}\\)0.341 | **47.550\\({\pm}\\)`<!-- -->`{=html}2.434** | 8.879\\({\pm}\\)1.337 | 0.229\\({\pm}\\)0.002 | 0.948\\({\pm}\\)0.011 | 0.708\\({\pm}\\)0.021 | **0.183\\({\pm}\\)`<!-- -->`{=html}0.018** | **0.005\\({\pm}\\)`<!-- -->`{=html}0.001** |  |
| Nearest (indep) | 1.048\\({\pm}\\)0.381 | 67.078\\({\pm}\\)6.359 | 8.608\\({\pm}\\)1.339 | 0.228\\({\pm}\\)0.002 | 0.943\\({\pm}\\)0.009 | 0.683\\({\pm}\\)0.022 | 0.227\\({\pm}\\)0.031 | 0.007\\({\pm}\\)0.002 |  |
| Bilinear (indep) | 0.990\\({\pm}\\)0.372 | 66.330\\({\pm}\\)6.394 | 8.832\\({\pm}\\)1.480 | 0.228\\({\pm}\\)0.002 | 0.942\\({\pm}\\)0.012 | 0.684\\({\pm}\\)0.019 | 0.216\\({\pm}\\)0.029 | 0.008\\({\pm}\\)0.002 |  |
| Fixed (gp) | 1.006\\({\pm}\\)0.362 | 54.058\\({\pm}\\)4.299 | 8.045\\({\pm}\\)1.205 | 0.222\\({\pm}\\)0.002 | 0.954\\({\pm}\\)0.007 | 0.666\\({\pm}\\)0.019 | 0.198\\({\pm}\\)0.021 | 0.007\\({\pm}\\)0.001 |  |
| Resample (gp) | 0.974\\({\pm}\\)0.308 | 54.778\\({\pm}\\)3.942 | **9.471\\({\pm}\\)`<!-- -->`{=html}1.566** | 0.225\\({\pm}\\)0.004 | **0.954\\({\pm}\\)`<!-- -->`{=html}0.007** | 0.661\\({\pm}\\)0.025 | 0.209\\({\pm}\\)0.020 | 0.006\\({\pm}\\)0.001 |  |
| Nearest (gp) | 0.975\\({\pm}\\)0.383 | 79.743\\({\pm}\\)10.835 | 8.896\\({\pm}\\)1.462 | 0.224\\({\pm}\\)0.002 | 0.939\\({\pm}\\)0.004 | 0.637\\({\pm}\\)0.015 | 0.243\\({\pm}\\)0.044 | 0.009\\({\pm}\\)0.003 |  |
| Bilinear (gp) | 0.953\\({\pm}\\)0.390 | 78.866\\({\pm}\\)11.960 | 8.565\\({\pm}\\)1.537 | 0.228\\({\pm}\\)0.001 | 0.942\\({\pm}\\)0.005 | 0.635\\({\pm}\\)0.014 | 0.247\\({\pm}\\)0.046 | 0.009\\({\pm}\\)0.003 |  |
| GP Warping | 0.812\\({\pm}\\)0.337 | 75.763\\({\pm}\\)11.555 | 8.291\\({\pm}\\)1.168 | 0.225\\({\pm}\\)0.002 | 0.941\\({\pm}\\)0.006 | 0.653\\({\pm}\\)0.016 | 0.226\\({\pm}\\)0.043 | 0.008\\({\pm}\\)0.003 |  |
| **Warped Diffusion** (Ours) | **0.649\\({\pm}\\)`<!-- -->`{=html}0.363** | 58.189\\({\pm}\\)6.322 | 8.882\\({\pm}\\)1.704 | **0.235\\({\pm}\\)`<!-- -->`{=html}0.003** | 0.943\\({\pm}\\)0.005 | 0.654\\({\pm}\\)0.024 | 0.221\\({\pm}\\)0.041 | 0.008\\({\pm}\\)0.002 |  |

Mean-frame evaluation of super-resolution models for real videos.

</div>

<span id="tab:noise_warping_comp_videos" label="tab:noise_warping_comp_videos"></span>

#### Noise Warping Speed.

We measure the time needed for a single noise warping. Our GP Warping mechanism takes \\(39\\)ms per frame Wall Clock time, to produce the warping at \\(1024\times 1024\\) resolution. This is \\(16\times\\) faster than the reported \\(629\\)ms number in `\cite{chang2023warped}`{=latex}. If we use batch parallelization, our method generates \\(1000\\) noise warpings in just \\(46\\)ms (at the expense of extra memory).

#### No Warping?

A natural question is whether we can omit completely the noise warping scheme since equivariance is forced at inference time. We ran some preliminary experiments for super-resolution on real-videos and we found that omitting the warping significantly deteriorates the results when the number of sampling steps is low. We found that increasing the number of sampling steps makes the effect of the initial noise warping less significant, at the cost of increased sampling time.

# Limitations [sec:limitations]

Our method has several limitations. First, the guidance term increases the sampling time, as detailed in the Appendix, Section <a href="#sec:speed" data-reference-type="ref" data-reference="sec:speed">13.3</a>. For reference, processing a 2-second video takes roughly 5 minutes on a single A-100 GPU. Second, even though in our experiments we observed a monotonic relation between the warping error in latent space and warping error in pixel space, it is possible that for some transformations the decoder of a Latent Diffusion Model might not be equivariant. We noticed that this is a common failure for text rendering, e.g. in [this](https://giannisdaras.github.io/warped_diffusion.github.io/assets/videos/3_output_latent_video.mp4) latent video the model seems to be equivariant, but in the [pixel video](https://giannisdaras.github.io/warped_diffusion.github.io/assets/videos/3_output_video.mp4) it is not. Third, the success of our method depends on the quality of the flow estimation – inconsistent flow estimation between frames will lead to flickering artifacts. For real videos, there might be occlusions and the estimation of the flow map can be noisy. We observed that in such cases our method fails, especially for challenging tasks such as video inpainting. The correlations obtained by following the optical flow field obtained from real videos might lead to a distribution shift compared to the training distribution. For such extreme deformations, our method produces correlation artifacts. This has been observed in prior work (see [this video](https://warpyournoise.github.io/docs/assets/videos/SuperRes/Bear/adv.mp4)), but it also appears in our setting (e.g. see [this video](https://giannisdaras.github.io/warped_diffusion.github.io/assets/videos/216_output_video.mp4)). Finally, our method cannot work in a zero-shot manner since it requires a model that is trained with correlated noise.

# Conclusions [sec:conclusion]

Warped Diffusion is a novel framework for solving temporally correlated inverse problems with image diffusion models. It leverages a noise warping scheme based on Gaussian processes to propagate noise maps and it ensures equivariant generation through an efficient equivariance self-guidance technique. We extensively validated Warped Diffusion on temporally coherent inpainting and superresolution, where our approach outperforms relevant baselines both quantitatively and qualitatively. Importantly, in contrast to previous work `\cite{chang2023warped}`{=latex}, our method can be applied seamlessly also to *latent* diffusion models, including state-of-the-art text-to-image models like SDXL `\cite{podell2024sdxl}`{=latex}.

# Acknowledgements [sec:ack]

This research has been partially supported by NSF Grants AF 1901292, CNS 2148141, Tripods CCF 1934932, IFML CCF 2019844 and research gifts by Western Digital, Amazon, WNCG IAP, UT Austin Machine Learning Lab (MLL), Cisco and the Stanly P. Finch Centennial Professorship in Engineering. Giannis Daras has been partially supported by the Onassis Fellowship (Scholarship ID: F ZS 012-1/2022-2023), the Bodossaki Fellowship and the Leventis Fellowship.

# References [references]

<div class="thebibliography" markdown="1">

Asad Aali, Marius Arvinte, Sidharth Kumar, and Jonathan I Tamir Solving inverse problems with score-based generative priors learned from noisy data , 2023. **Abstract:** We present SURE-Score: an approach for learning score-based generative models using training samples corrupted by additive Gaussian noise. When a large training set of clean samples is available, solving inverse problems via score-based (diffusion) generative models trained on the underlying fully-sampled data distribution has recently been shown to outperform end-to-end supervised deep learning. In practice, such a large collection of training data may be prohibitively expensive to acquire in the first place. In this work, we present an approach for approximately learning a score-based generative model of the clean distribution, from noisy training data. We formulate and justify a novel loss function that leverages Stein’s unbiased risk estimate to jointly denoise the data and learn the score function via denoising score matching, while using only the noisy samples. We demonstrate the generality of SURE-Score by learning priors and applying posterior sampling to ill-posed inverse problems in two practical applications from different domains: compressive wireless multiple-input multiple-output channel estimation and accelerated 2D multi-coil magnetic resonance imaging reconstruction, where we demonstrate competitive reconstruction performance when learning at signal-to-noise ratio values of 0 and 10 dB, respectively. (@aali2023solving)

Asad Aali, Giannis Daras, Brett Levac, Sidharth Kumar, Alexandros G Dimakis, and Jonathan I Tamir Ambient diffusion posterior sampling: Solving inverse problems with diffusion models trained on corrupted data , 2024. **Abstract:** We provide a framework for solving inverse problems with diffusion models learned from linearly corrupted data. Firstly, we extend the Ambient Diffusion framework to enable training directly from measurements corrupted in the Fourier domain. Subsequently, we train diffusion models for MRI with access only to Fourier subsampled multi-coil measurements at acceleration factors R= 2,4,6,8. Secondly, we propose Ambient Diffusion Posterior Sampling (A-DPS), a reconstruction algorithm that leverages generative models pre-trained on one type of corruption (e.g. image inpainting) to perform posterior sampling on measurements from a different forward process (e.g. image blurring). For MRI reconstruction in high acceleration regimes, we observe that A-DPS models trained on subsampled data are better suited to solving inverse problems than models trained on fully sampled data. We also test the efficacy of A-DPS on natural image datasets (CelebA, FFHQ, and AFHQ) and show that A-DPS can sometimes outperform models trained on clean data for several image restoration tasks in both speed and performance. (@aali2024ambient)

Namrata Anand and Tudor Achim Protein structure and sequence generation with equivariant denoising diffusion probabilistic models , 2022. **Abstract:** Proteins are macromolecules that mediate a significant fraction of the cellular processes that underlie life. An important task in bioengineering is designing proteins with specific 3D structures and chemical properties which enable targeted functions. To this end, we introduce a generative model of both protein structure and sequence that can operate at significantly larger scales than previous molecular generative modeling approaches. The model is learned entirely from experimental data and conditions its generation on a compact specification of protein topology to produce a full-atom backbone configuration as well as sequence and side-chain predictions. We demonstrate the quality of the model via qualitative and quantitative analysis of its samples. Videos of sampling trajectories are available at https://nanand2.github.io/proteins . (@anand2022protein)

Nachman Aronszajn Theory of reproducing kernels , 68, 1950. **Abstract:** May 7. Difference of reproducing kernels.354 8. Product of reproducing kernels.357 9. Limits of reproducing kernels.362 10.Construction of a r.k. by resolution of identity.368 11.Operators in spaces with reproducing kernels.371 12.The reproducing kernel of a sum of two closed subspaces.375 13.Final remarks in the general theory.380 Part II.Examples.384 1.I ntroductory remarks.384 (1) Bergman’s kernels.384 (2) Harmonic kernels.386 2. Comparison domains.387 3. The difference of kernels.388 4. The square of a kernel introduced by Szegö.391 5.The kernel H{z, zi) for an ellipse.393 6. Construction of H(z, z¡) for a strip.394 7. Limits of increasing sequences of kernels.396 8. Construction of reproducing kernels by the projection-formula of §12, I. (@aronszajn1950theory)

Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu ediff-i: Text-to-image diffusion models with ensemble of expert denoisers , 2022. **Abstract:** Large-scale diffusion-based generative models have led to breakthroughs in text-conditioned high-resolution image synthesis. Starting from random noise, such text-to-image diffusion models gradually synthesize images in an iterative fashion while conditioning on text prompts. We find that their synthesis behavior qualitatively changes throughout this process: Early in sampling, generation strongly relies on the text prompt to generate text-aligned content, while later, the text conditioning is almost entirely ignored. This suggests that sharing model parameters throughout the entire generation process may not be ideal. Therefore, in contrast to existing works, we propose to train an ensemble of text-to-image diffusion models specialized for different synthesis stages. To maintain training efficiency, we initially train a single model, which is then split into specialized models that are trained for the specific stages of the iterative generation process. Our ensemble of diffusion models, called eDiff-I, results in improved text alignment while maintaining the same inference computation cost and preserving high visual quality, outperforming previous large-scale text-to-image diffusion models on the standard benchmark. In addition, we train our model to exploit a variety of embeddings for conditioning, including the T5 text, CLIP text, and CLIP image embeddings. We show that these different embeddings lead to different behaviors. Notably, the CLIP image embedding allows an intuitive way of transferring the style of a reference image to the target text-to-image output. Lastly, we show a technique that enables eDiff-I’s "paint-with-words" capability. A user can select the word in the input text and paint it in a canvas to control the output, which is very handy for crafting the desired image in mind. The project page is available at https://deepimagination.cc/eDiff-I/ (@balaji2022ediffi)

Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S. Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein Cold diffusion: Inverting arbitrary image transforms without noise , 2022. **Abstract:** Standard diffusion models involve an image transform – adding Gaussian noise – and an image restoration operator that inverts this degradation. We observe that the generative behavior of diffusion models is not strongly dependent on the choice of image degradation, and in fact an entire family of generative models can be constructed by varying this choice. Even when using completely deterministic degradations (e.g., blur, masking, and more), the training and test-time update rules that underlie diffusion models can be easily generalized to create generative models. The success of these fully deterministic models calls into question the community’s understanding of diffusion models, which relies on noise in either gradient Langevin dynamics or variational inference, and paves the way for generalized diffusion models that invert arbitrary processes. Our code is available at https://github.com/arpitbansal297/Cold-Diffusion-Models (@bansal2022cold)

Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, Yuanzhen Li, Michael Rubinstein, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, and Inbar Mosseri Lumiere: A space-time diffusion model for video generation , 2024. **Abstract:** We introduce Lumiere – a text-to-video diffusion model designed for synthesizing videos that portray realistic, diverse and coherent motion – a pivotal challenge in video synthesis. To this end, we introduce a Space-Time U-Net architecture that generates the entire temporal duration of the video at once, through a single pass in the model. This is in contrast to existing video models which synthesize distant keyframes followed by temporal super-resolution – an approach that inherently makes global temporal consistency difficult to achieve. By deploying both spatial and (importantly) temporal down- and up-sampling and leveraging a pre-trained text-to-image diffusion model, our model learns to directly generate a full-frame-rate, low-resolution video by processing it in multiple space-time scales. We demonstrate state-of-the-art text-to-video generation results, and show that our design easily facilitates a wide range of content creation tasks and video editing applications, including image-to-video, video inpainting, and stylized generation. (@bartal2024lumiere)

Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach Stable video diffusion: Scaling latent video diffusion models to large datasets , 2023. **Abstract:** We present Stable Video Diffusion - a latent video diffusion model for high-resolution, state-of-the-art text-to-video and image-to-video generation. Recently, latent diffusion models trained for 2D image synthesis have been turned into generative video models by inserting temporal layers and finetuning them on small, high-quality video datasets. However, training methods in the literature vary widely, and the field has yet to agree on a unified strategy for curating video data. In this paper, we identify and evaluate three different stages for successful training of video LDMs: text-to-image pretraining, video pretraining, and high-quality video finetuning. Furthermore, we demonstrate the necessity of a well-curated pretraining dataset for generating high-quality videos and present a systematic curation process to train a strong base model, including captioning and filtering strategies. We then explore the impact of finetuning our base model on high-quality data and train a text-to-video model that is competitive with closed-source video generation. We also show that our base model provides a powerful motion representation for downstream tasks such as image-to-video generation and adaptability to camera motion-specific LoRA modules. Finally, we demonstrate that our model provides a strong multi-view 3D-prior and can serve as a base to finetune a multi-view diffusion model that jointly generates multiple views of objects in a feedforward fashion, outperforming image-based methods at a fraction of their compute budget. We release code and model weights at https://github.com/Stability-AI/generative-models . (@blattmann2023stable)

Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis Align your latents: High-resolution video synthesis with latent diffusion models In *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2023. **Abstract:** Latent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space. Here, we apply the LDM paradigm to high-resolution video generation, a particularly resource-intensive task. We first pre-train an LDM on images only; then, we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and finetuning on encoded image sequences, i.e., videos. Similarly, we temporally align diffusion model upsamplers, turning them into temporally consistent video super resolution models. We focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video modeling. In particular, we validate our Video LDM on real driving videos of resolution \<tex xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"\>$512 \\}times 1024$\</tex\> , achieving state-of-the-art performance. Furthermore, our approach can easily leverage off-the-shelf pretrained image LDMs, as we only need to train a temporal alignment model in that case. Doing so, we turn the publicly available, state-of-the-art text-to-image LDM Stable Diffusion into an efficient and expressive text-to-video model with resolution up to \<tex xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"\>$1280 \\}times 2048$\</tex\> . We show that the temporal layers trained in this way generalize to different finetuned text-to-image LDMs. Utilizing this property, we show the first results for personalized text-to-video generation, opening exciting directions for future content creation. Project page: https://nv-tlabs.github.io/VideoLDM/ (@blattmann2023videoldm)

Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet Diffusion schrödinger bridge with applications to score-based generative modeling In *Advances in Neural Information Processing Systems*, 2021. **Abstract:** Progressively applying Gaussian noise transforms complex data distributions to approximately Gaussian. Reversing this dynamic defines a generative model. When the forward noising process is given by a Stochastic Differential Equation (SDE), Song et al. (2021) demonstrate how the time inhomogeneous drift of the associated reverse-time SDE may be estimated using score-matching. A limitation of this approach is that the forward-time SDE must be run for a sufficiently long time for the final distribution to be approximately Gaussian. In contrast, solving the Schr\\}"odinger Bridge problem (SB), i.e. an entropy-regularized optimal transport problem on path spaces, yields diffusions which generate samples from the data distribution in finite time. We present Diffusion SB (DSB), an original approximation of the Iterative Proportional Fitting (IPF) procedure to solve the SB problem, and provide theoretical analysis along with generative modeling experiments. The first DSB iteration recovers the methodology proposed by Song et al. (2021), with the flexibility of using shorter time intervals, as subsequent DSB iterations reduce the discrepancy between the final-time marginal of the forward (resp. backward) SDE with respect to the prior (resp. data) distribution. Beyond generative modeling, DSB offers a widely applicable computational optimal transport tool as the continuous state-space analogue of the popular Sinkhorn algorithm (Cuturi, 2013). (@bortoli2021diffusion)

Tim Brooks, Bill Peebles, Connor Homes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Wing Yin Ng, Ricky Wang, and Aditya Ramesh Video generation models as world simulators . **Abstract:** Recent advancements in predictive models have demonstrated exceptional capabilities in predicting the future state of objects and scenes. However, the lack of categorization based on inherent characteristics continues to hinder the progress of predictive model development. Additionally, existing benchmarks are unable to effectively evaluate higher-capability, highly embodied predictive models from an embodied perspective. In this work, we classify the functionalities of predictive models into a hierarchy and take the first step in evaluating World Simulators by proposing a dual evaluation framework called WorldSimBench. WorldSimBench includes Explicit Perceptual Evaluation and Implicit Manipulative Evaluation, encompassing human preference assessments from the visual perspective and action-level evaluations in embodied tasks, covering three representative embodied scenarios: Open-Ended Embodied Environment, Autonomous, Driving, and Robot Manipulation. In the Explicit Perceptual Evaluation, we introduce the HF-Embodied Dataset, a video assessment dataset based on fine-grained human feedback, which we use to train a Human Preference Evaluator that aligns with human perception and explicitly assesses the visual fidelity of World Simulators. In the Implicit Manipulative Evaluation, we assess the video-action consistency of World Simulators by evaluating whether the generated situation-aware video can be accurately translated into the correct control signals in dynamic environments. Our comprehensive evaluation offers key insights that can drive further innovation in video generation models, positioning World Simulators as a pivotal advancement toward embodied artificial intelligence. (@videoworldsimulators2024)

Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim Coyo-700m: Image-text pair dataset <https://github.com/kakaobrain/coyo-dataset>, 2022. **Abstract:** Digital archiving is becoming widespread owing to its effectiveness in protecting valuable books and providing knowledge to many people electronically. In this paper, we propose a novel approach to leverage digital archives for machine learning. If we can fully utilize such digitized data, machine learning has the potential to uncover unknown insights and ultimately acquire knowledge autonomously, just like humans read books. As a first step, we design a dataset construction pipeline comprising an optical character reader (OCR), an object detector, and a layout analyzer for the autonomous extraction of image-text pairs. In our experiments, we apply our pipeline on old photo books to construct an image-text pair dataset, showing its effectiveness in image-text retrieval and insight extraction. (@kakaobrain2022coyo-700m)

Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra Pix2video: Video editing using image diffusion In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 23206–23217, 2023. **Abstract:** Image diffusion models, trained on massive image collections, have emerged as the most versatile image generator model in terms of quality and diversity. They support inverting real images and conditional (e.g., text) generation, making them attractive for high-quality image editing applications. We investigate how to use such pre-trained image models for text-guided video editing. The critical challenge is to achieve the target edits while still preserving the content of the source video. Our method works in two simple steps: first, we use a pre-trained structure-guided (e.g., depth) image diffusion model to perform text-guided edits on an anchor frame; then, in the key step, we progressively propagate the changes to the future frames via self-attention feature injection to adapt the core denoising step of the diffusion model. We then consolidate the changes by adjusting the latent code for the frame before continuing the process. Our approach is training-free and generalizes to a wide range of edits. We demonstrate the effectiveness of the approach by extensive experimentation and compare it against four different prior and parallel efforts (on ArXiv). We demonstrate that realistic text-guided video edits are possible, without any compute-intensive preprocessing or video-specific finetuning. https://duyguceylan.github.io/pix2video.github.io/. (@ceylan2023pix2video)

Pascal Chang, Jingwei Tang, Markus Gross, and Vinicius C Azevedo How i warped your noise: a temporally-correlated noise prior for diffusion models In *The Twelfth International Conference on Learning Representations*, 2023. **Abstract:** Video editing and generation methods often rely on pre-trained image-based diffusion models. During the diffusion process, however, the reliance on rudimentary noise sampling techniques that do not preserve correlations present in subsequent frames of a video is detrimental to the quality of the results. This either produces high-frequency flickering, or texture-sticking artifacts that are not amenable to post-processing. With this in mind, we propose a novel method for preserving temporal correlations in a sequence of noise samples. This approach is materialized by a novel noise representation, dubbed $\\}int$-noise (integral noise), that reinterprets individual noise samples as a continuously integrated noise field: pixel values do not represent discrete values, but are rather the integral of an underlying infinite-resolution noise over the pixel area. Additionally, we propose a carefully tailored transport method that uses $\\}int$-noise to accurately advect noise samples over a sequence of frames, maximizing the correlation between different frames while also preserving the noise properties. Our results demonstrate that the proposed $\\}int$-noise can be used for a variety of tasks, such as video restoration, surrogate rendering, and conditional video generation. See https://warpyournoise.github.io/ for video results. (@chang2023warped)

Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye Diffusion posterior sampling for general noisy inverse problems , 2022. **Abstract:** Diffusion models have been recently studied as powerful generative inverse problem solvers, owing to their high quality reconstructions and the ease of combining existing iterative solvers. However, most works focus on solving simple linear inverse problems in noiseless settings, which significantly under-represents the complexity of real-world problems. In this work, we extend diffusion solvers to efficiently handle general noisy (non)linear inverse problems via approximation of the posterior sampling. Interestingly, the resulting posterior sampling scheme is a blended version of diffusion sampling with the manifold constrained gradient without a strict measurement consistency projection step, yielding a more desirable generative path in noisy settings compared to the previous studies. Our method demonstrates that diffusion models can incorporate various measurement noise statistics such as Gaussian and Poisson, and also efficiently handle noisy nonlinear inverse problems such as Fourier phase retrieval and non-uniform deblurring. Code available at https://github.com/DPS2022/diffusion-posterior-sampling (@chung2022diffusion)

Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye Improving diffusion models for inverse problems using manifold constraints , 2022. **Abstract:** Recently, diffusion models have been used to solve various inverse problems in an unsupervised manner with appropriate modifications to the sampling process. However, the current solvers, which recursively apply a reverse diffusion step followed by a projection-based measurement consistency step, often produce suboptimal results. By studying the generative sampling path, here we show that current solvers throw the sample path off the data manifold, and hence the error accumulates. To address this, we propose an additional correction term inspired by the manifold constraint, which can be used synergistically with the previous solvers to make the iterations close to the manifold. The proposed manifold constraint is straightforward to implement within a few lines of code, yet boosts the performance by a surprisingly large margin. With extensive experiments, we show that our method is superior to the previous methods both theoretically and empirically, producing promising results in many applications such as image inpainting, colorization, and sparse-view computed tomography. Code available https://github.com/HJ-harry/MCG_diffusion (@chung2022improving)

Gabriele Corso, Hannes Stärk, Bowen Jing, Regina Barzilay, and Tommi Jaakkola Diffdock: Diffusion steps, twists, and turns for molecular docking , 2022. **Abstract:** Predicting the binding structure of a small molecule ligand to a protein – a task known as molecular docking – is critical to drug design. Recent deep learning methods that treat docking as a regression problem have decreased runtime compared to traditional search-based methods but have yet to offer substantial improvements in accuracy. We instead frame molecular docking as a generative modeling problem and develop DiffDock, a diffusion generative model over the non-Euclidean manifold of ligand poses. To do so, we map this manifold to the product space of the degrees of freedom (translational, rotational, and torsional) involved in docking and develop an efficient diffusion process on this space. Empirically, DiffDock obtains a 38% top-1 success rate (RMSD\<2A) on PDBBind, significantly outperforming the previous state-of-the-art of traditional docking (23%) and deep learning (20%) methods. Moreover, while previous methods are not able to dock on computationally folded structures (maximum accuracy 10.4%), DiffDock maintains significantly higher precision (21.7%). Finally, DiffDock has fast inference times and provides confidence estimates with high selective accuracy. (@corso2022diffdock)

G. Da Prato and J. Zabczyk Encyclopedia of Mathematics and its Applications. Cambridge University Press, 2014. **Abstract:** We prove that distribution dependent (also called McKean–Vlasov) stochastic delay equations of the form \\}begin{equation\*} \\}mathrm{d}X(t)= b(t,X_t,\\}mathcal{L}\_{X_t})\\}mathrm{d}t+ \\}sigma(t,X_t,\\}mathcal{L}\_{X_t})\\}mathrm{d}W(t) \\}end{equation\*} have unique (strong) solutions in finite as well as infinite dimensional state spaces if the coefficients fulfill certain monotonicity assumptions. (@da2014stochastic)

Giannis Daras, Hyungjin Chung, Chieh-Hsin Lai, Yuki Mitsufuji, Peyman Milanfar, Alexandros G. Dimakis, Chul Ye, and Mauricio Delbracio A survey on diffusion models for inverse problems . **Abstract:** Diffusion models have become increasingly popular for generative modeling due to their ability to generate high-quality samples. This has unlocked exciting new possibilities for solving inverse problems, especially in image restoration and reconstruction, by treating diffusion models as unsupervised priors. This survey provides a comprehensive overview of methods that utilize pre-trained diffusion models to solve inverse problems without requiring further training. We introduce taxonomies to categorize these methods based on both the problems they address and the techniques they employ. We analyze the connections between different approaches, offering insights into their practical implementation and highlighting important considerations. We further discuss specific challenges and potential solutions associated with using latent diffusion models for inverse problems. This work aims to be a valuable resource for those interested in learning about the intersection of diffusion models and inverse problems. (@diffusion_survey)

Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alexandros G. Dimakis, and Peyman Milanfar Soft diffusion: Score matching for general corruptions , 2022. **Abstract:** We define a broader family of corruption processes that generalizes previously known diffusion models. To reverse these general diffusions, we propose a new objective called Soft Score Matching that provably learns the score function for any linear corruption process and yields state of the art results for CelebA. Soft Score Matching incorporates the degradation process in the network. Our new loss trains the model to predict a clean image, \\}textit{that after corruption}, matches the diffused observation. We show that our objective learns the gradient of the likelihood under suitable regularity conditions for a family of corruption processes. We further develop a principled way to select the corruption levels for general diffusion processes and a novel sampling method that we call Momentum Sampler. We show experimentally that our framework works for general linear corruption processes, such as Gaussian blur and masking. We achieve state-of-the-art FID score $1.85$ on CelebA-64, outperforming all previous linear diffusion models. We also show significant computational benefits compared to vanilla denoising diffusion. (@daras2022soft)

Giannis Daras and Alex Dimakis Solving inverse problems with ambient diffusion In *NeurIPS 2023 Workshop on Deep Learning and Inverse Problems*, 2023. **Abstract:** We provide a framework for solving inverse problems with diffusion models learned from linearly corrupted data. Our method, Ambient Diffusion Posterior Sampling (A-DPS), leverages a generative model pre-trained on one type of corruption (e.g. image inpainting) to perform posterior sampling conditioned on measurements from a potentially different forward process (e.g. image blurring). We test the efficacy of our approach on standard natural image datasets (CelebA, FFHQ, and AFHQ) and we show that A-DPS can sometimes outperform models trained on clean data for several image restoration tasks in both speed and performance. We further extend the Ambient Diffusion framework to train MRI models with access only to Fourier subsampled multi-coil MRI measurements at various acceleration factors (R=2, 4, 6, 8). We again observe that models trained on highly subsampled data are better priors for solving inverse problems in the high acceleration regime than models trained on fully sampled data. We open-source our code and the trained Ambient Diffusion MRI models: https://github.com/utcsilab/ambient-diffusion-mri . (@daras2023solving)

Giannis Daras, Alexandros G Dimakis, and Constantinos Daskalakis Consistent diffusion meets tweedie: Training exact ambient diffusion models with noisy data , 2024. **Abstract:** Ambient diffusion is a recently proposed framework for training diffusion models using corrupted data. Both Ambient Diffusion and alternative SURE-based approaches for learning diffusion models from corrupted data resort to approximations which deteriorate performance. We present the first framework for training diffusion models that provably sample from the uncorrupted distribution given only noisy training data, solving an open problem in this space. Our key technical contribution is a method that uses a double application of Tweedie’s formula and a consistency loss function that allows us to extend sampling at noise levels below the observed data noise. We also provide further evidence that diffusion models memorize from their training sets by identifying extremely corrupted images that are almost perfectly reconstructed, raising copyright and privacy concerns. Our method for training using corrupted samples can be used to mitigate this problem. We demonstrate this by fine-tuning Stable Diffusion XL to generate samples from a distribution using only noisy samples. Our framework reduces the amount of memorization of the fine-tuning dataset, while maintaining competitive performance. (@daras2024consistent)

Giannis Daras, Kulin Shah, Yuval Dagan, Aravind Gollakota, Alex Dimakis, and Adam Klivans Ambient diffusion: Learning clean distributions from corrupted data , 36, 2024. **Abstract:** We present the first diffusion-based framework that can learn an unknown distribution using only highly-corrupted samples. This problem arises in scientific applications where access to uncorrupted samples is impossible or expensive to acquire. Another benefit of our approach is the ability to train generative models that are less likely to memorize individual training samples since they never observe clean training data. Our main idea is to introduce additional measurement distortion during the diffusion process and require the model to predict the original corrupted image from the further corrupted image. We prove that our method leads to models that learn the conditional expectation of the full uncorrupted image given this additional measurement corruption. This holds for any corruption process that satisfies some technical conditions (and in particular includes inpainting and compressed sensing). We train models on standard benchmarks (CelebA, CIFAR-10 and AFHQ) and show that we can learn the distribution even when all the training samples have $90\\}%$ of their pixels missing. We also show that we can finetune foundation models on small corrupted datasets (e.g. MRI scans with block corruptions) and learn the clean distribution without memorizing the training set. (@daras2024ambient)

Mauricio Delbracio and Peyman Milanfar Inversion by direct iteration: An alternative to denoising diffusion for image restoration , 2023. **Abstract:** Inversion by Direct Iteration (InDI) is a new formulation for supervised image restoration that avoids the so-called "regression to the mean" effect and produces more realistic and detailed images than existing regression-based methods. It does this by gradually improving image quality in small steps, similar to generative denoising diffusion models. Image restoration is an ill-posed problem where multiple high-quality images are plausible reconstructions of a given low-quality input. Therefore, the outcome of a single step regression model is typically an aggregate of all possible explanations, therefore lacking details and realism. The main advantage of InDI is that it does not try to predict the clean target image in a single step but instead gradually improves the image in small steps, resulting in better perceptual quality. While generative denoising diffusion models also work in small steps, our formulation is distinct in that it does not require knowledge of any analytic form of the degradation process. Instead, we directly learn an iterative restoration process from low-quality and high-quality paired examples. InDI can be applied to virtually any image degradation, given paired training data. In conditional denoising diffusion image restoration the denoising network generates the restored image by repeatedly denoising an initial image of pure noise, conditioned on the degraded input. Contrary to conditional denoising formulations, InDI directly proceeds by iteratively restoring the input low-quality image, producing high-quality results on a variety of image restoration tasks, including motion and out-of-focus deblurring, super-resolution, compression artifact removal, and denoising. (@delbracio2023inversion)

Prafulla Dhariwal and Alexander Quinn Nichol Diffusion models beat GANs on image synthesis In *Advances in Neural Information Processing Systems*, 2021. **Abstract:** We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\}times$128, 4.59 on ImageNet 256$\\}times$256, and 7.72 on ImageNet 512$\\}times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\}times$256 and 3.85 on ImageNet 512$\\}times$512. We release our code at https://github.com/openai/guided-diffusion (@dhariwal2021diffusion)

Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach Scaling rectified flow transformers for high-resolution image synthesis , 2024. **Abstract:** Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available. (@esser2024scaling)

Denis Fortun, Patrick Bouthemy, and Charles Kervrann Optical flow modeling and computation: A survey , 134:1–21, 2015. **Abstract:** Optical ﬂow estimation is one of the oldest and still most active research domains in computer vision. In 35 years, many methodological concepts have been introduced and have progressively improved performances, while opening the way to new challenges. In the last decade, the growing interest in evaluation benchmarks has stimulated a great amount of work. In this paper, we propose a survey of optical ﬂow estimation classifying the main principles elaborated during this evolution, with a particular concern given to recent developments. It is conceived as a tutorial organizing in a comprehensive framework current approaches and practices. We give insights on the motivations, interests and limitations of modeling and optimization techniques, and we highlight similarities between methods to allow for a clear understanding of their behavior. (@fortun2015opticalflow)

Giulio Franzese, Giulio Corallo, Simone Rossi, Markus Heinonen, Maurizio Filippone, and Pietro Michiardi Continuous-time functional diffusion processes , 36, 2024. **Abstract:** We introduce Functional Diffusion Processes (FDPs), which generalize score-based diffusion models to infinite-dimensional function spaces. FDPs require a new mathematical framework to describe the forward and backward dynamics, and several extensions to derive practical training objectives. These include infinite-dimensional versions of Girsanov theorem, in order to be able to compute an ELBO, and of the sampling theorem, in order to guarantee that functional evaluations in a countable set of points are equivalent to infinite-dimensional functions. We use FDPs to build a new breed of generative models in function spaces, which do not require specialized network architectures, and that can work with any kind of continuous data. Our results on real data show that FDPs achieve high-quality image generation, using a simple MLP architecture with orders of magnitude fewer parameters than existing diffusion models. (@franzese2024continuous)

Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, 2023. **Abstract:** Despite tremendous progress in generating high-quality images using diffusion models, synthesizing a sequence of animated frames that are both photorealistic and temporally coherent is still in its infancy. While off-the-shelf billion-scale datasets for image generation are available, collecting similar video data of the same scale is still challenging. Also, training a video diffusion model is computationally much more expensive than its image counterpart. In this work, we explore finetuning a pretrained image diffusion model with video data as a practical solution for the video synthesis task. We find that naively extending the image noise prior to video noise prior in video diffusion leads to sub-optimal performance. Our carefully designed video noise prior leads to substantially better performance. Extensive experimental validation shows that our model, Preserve Your Own COrrelation (PYoCo), attains SOTA zero-shot text-to-video results on the UCF-101 and MSR-VTT benchmarks. It also achieves SOTA video generation quality on the small-scale UCF-101 benchmark with a 10× smaller model using significantly less computation than the prior art. The project page is available at https://research.nvidia.com/labs/dir/pyoco/. (@ge2023pyoco)

Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra Emu video: Factorizing text-to-video generation by explicit image conditioning , 2023. **Abstract:** We present Emu Video, a text-to-video generation model that factorizes the generation into two steps: first generating an image conditioned on the text, and then generating a video conditioned on the text and the generated image. We identify critical design decisions–adjusted noise schedules for diffusion, and multi-stage training that enable us to directly generate high quality and high resolution videos, without requiring a deep cascade of models as in prior work. In human evaluations, our generated videos are strongly preferred in quality compared to all prior work–81% vs. Google’s Imagen Video, 90% vs. Nvidia’s PYOCO, and 96% vs. Meta’s Make-A-Video. Our model outperforms commercial solutions such as RunwayML’s Gen2 and Pika Labs. Finally, our factorizing approach naturally lends itself to animating images based on a user’s text prompt, where our generations are preferred 96% over prior work. (@girdhar2023emu)

Ramesh Girish and Gopal Krishna A survey on video diffusion models , 2023. **Abstract:** The recent wave of AI-generated content (AIGC) has witnessed substantial success in computer vision, with the diffusion model playing a crucial role in this achievement. Due to their impressive generative capabilities, diffusion models are gradually superseding methods based on GANs and auto-regressive Transformers, demonstrating exceptional performance not only in image generation and editing, but also in the realm of video-related research. However, existing surveys mainly focus on diffusion models in the context of image generation, with few up-to-date reviews on their application in the video domain. To address this gap, this paper presents a comprehensive review of video diffusion models in the AIGC era. Specifically, we begin with a concise introduction to the fundamentals and evolution of diffusion models. Subsequently, we present an overview of research on diffusion models in the video domain, categorizing the work into three key areas: video generation, video editing, and other video understanding tasks. We conduct a thorough review of the literature in these three key areas, including further categorization and practical contributions in the field. Finally, we discuss the challenges faced by research in this domain and outline potential future developmental trends. A comprehensive list of video diffusion models studied in this survey is available at https://github.com/ChenHsing/Awesome-Video-Diffusion-Models. (@girish2023survey)

Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and Dimitris Samaras Diffusion models as plug-and-play priors , 2022. **Abstract:** We consider the problem of inferring high-dimensional data $\\}mathbf{x}$ in a model that consists of a prior $p(\\}mathbf{x})$ and an auxiliary differentiable constraint $c(\\}mathbf{x},\\}mathbf{y})$ on $x$ given some additional information $\\}mathbf{y}$. In this paper, the prior is an independently trained denoising diffusion generative model. The auxiliary constraint is expected to have a differentiable form, but can come from diverse sources. The possibility of such inference turns diffusion models into plug-and-play modules, thereby allowing a range of potential applications in adapting models to new domains and tasks, such as conditional generation or image segmentation. The structure of diffusion models allows us to perform approximate inference by iterating differentiation through the fixed denoising network enriched with different amounts of noise at each step. Considering many noised versions of $\\}mathbf{x}$ in evaluation of its fitness is a novel search mechanism that may lead to new algorithms for solving combinatorial optimization problems. (@graikos2022diffusion)

Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai Sparsectrl: Adding sparse controls to text-to-video diffusion models , 2023. **Abstract:** The development of text-to-video (T2V), i.e., generating videos with a given text prompt, has been significantly advanced in recent years. However, relying solely on text prompts often results in ambiguous frame composition due to spatial uncertainty. The research community thus leverages the dense structure signals, e.g., per-frame depth/edge sequences, to enhance controllability, whose collection accordingly increases the burden of inference. In this work, we present SparseCtrl to enable flexible structure control with temporally sparse signals, requiring only one or a few inputs, as shown in Figure 1. It incorporates an additional condition encoder to process these sparse signals while leaving the pre-trained T2V model untouched. The proposed approach is compatible with various modalities, including sketches, depth maps, and RGB images, providing more practical control for video generation and promoting applications such as storyboarding, depth rendering, keyframe animation, and interpolation. Extensive experiments demonstrate the generalization of SparseCtrl on both original and personalized T2V generators. Codes and models will be publicly available at https://guoyww.github.io/projects/SparseCtrl . (@guo2023sparsectrl)

Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai Animatediff: Animate your personalized text-to-image diffusion models without specific tuning , 2024. **Abstract:** With the advance of text-to-image (T2I) diffusion models (e.g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost. However, adding motion dynamics to existing high-quality personalized T2Is and enabling them to generate animations remains an open challenge. In this paper, we present AnimateDiff, a practical framework for animating personalized T2I models without requiring model-specific tuning. At the core of our framework is a plug-and-play motion module that can be trained once and seamlessly integrated into any personalized T2Is originating from the same base T2I. Through our proposed training strategy, the motion module effectively learns transferable motion priors from real-world videos. Once trained, the motion module can be inserted into a personalized T2I model to form a personalized animation generator. We further propose MotionLoRA, a lightweight fine-tuning technique for AnimateDiff that enables a pre-trained motion module to adapt to new motion patterns, such as different shot types, at a low training and data collection cost. We evaluate AnimateDiff and MotionLoRA on several public representative personalized T2I models collected from the community. The results demonstrate that our approaches help these models generate temporally smooth animation clips while preserving the visual quality and motion diversity. Codes and pre-trained weights are available at https://github.com/guoyww/AnimateDiff. (@guo2023animatediff)

Paul Hagemann, Sophie Mildenberger, Lars Ruthotto, Gabriele Steidl, and Nicole Tianjiao Yang Multilevel diffusion: Infinite dimensional score-based diffusion models for image generation , 2023. **Abstract:** Score-based diffusion models (SBDM) have recently emerged as state-of-the-art approaches for image generation. Existing SBDMs are typically formulated in a finite-dimensional setting, where images are considered as tensors of finite size. This paper develops SBDMs in the infinite-dimensional setting, that is, we model the training data as functions supported on a rectangular domain. In addition to the quest for generating images at ever-higher resolutions, our primary motivation is to create a well-posed infinite-dimensional learning problem that we can discretize consistently on multiple resolution levels. We thereby intend to obtain diffusion models that generalize across different resolution levels and improve the efficiency of the training process. We demonstrate how to overcome two shortcomings of current SBDM approaches in the infinite-dimensional setting. First, we modify the forward process using trace class operators to ensure that the latent distribution is well-defined in the infinite-dimensional setting and derive the reverse processes for finite-dimensional approximations. Second, we illustrate that approximating the score function with an operator network is beneficial for multilevel training. After deriving the convergence of the discretization and the approximation of multilevel training, we demonstrate some practical benefits of our infinite-dimensional SBDM approach on a synthetic Gaussian mixture example, the MNIST dataset, and a dataset generated from a nonlinear 2D reaction-diffusion equation. (@hagemann2023multilevel)

Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and Arash Vahdat Diffit: Diffusion vision transformers for image generation , 2023. **Abstract:** Diffusion models with their powerful expressivity and high sample quality have achieved State-Of-The-Art (SOTA) performance in the generative domain. The pioneering Vision Transformer (ViT) has also demonstrated strong modeling capabilities and scalability, especially for recognition tasks. In this paper, we study the effectiveness of ViTs in diffusion-based generative learning and propose a new model denoted as Diffusion Vision Transformers (DiffiT). Specifically, we propose a methodology for finegrained control of the denoising process and introduce the Time-dependant Multihead Self Attention (TMSA) mechanism. DiffiT is surprisingly effective in generating high-fidelity images with significantly better parameter efficiency. We also propose latent and image space DiffiT models and show SOTA performance on a variety of class-conditional and unconditional synthesis tasks at different resolutions. The Latent DiffiT model achieves a new SOTA FID score of 1.73 on ImageNet256 dataset while having 19.85%, 16.88% less parameters than other Transformer-based diffusion models such as MDT and DiT,respectively. Code: https://github.com/NVlabs/DiffiT (@hatamizadeh2023diffit)

Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter Gans trained by a two time-scale update rule converge to a local nash equilibrium , 30, 2017. **Abstract:** Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr\\}’echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark. (@heusel2017gansfid)

Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al Imagen video: High definition video generation with diffusion models , 2022. **Abstract:** We present Imagen Video, a text-conditional video generation system based on a cascade of video diffusion models. Given a text prompt, Imagen Video generates high definition videos using a base video generation model and a sequence of interleaved spatial and temporal video super-resolution models. We describe how we scale up the system as a high definition text-to-video model including design decisions such as the choice of fully-convolutional temporal and spatial super-resolution models at certain resolutions, and the choice of the v-parameterization of diffusion models. In addition, we confirm and transfer findings from previous work on diffusion-based image generation to the video generation setting. Finally, we apply progressive distillation to our video models with classifier-free guidance for fast, high quality sampling. We find Imagen Video not only capable of generating videos of high fidelity, but also having a high degree of controllability and world knowledge, including the ability to generate diverse videos and text animations in various artistic styles and with 3D object understanding. See https://imagen.research.google/video/ for samples. (@ho2022imagen)

Jonathan Ho, Ajay Jain, and Pieter Abbeel Denoising diffusion probabilistic models In *Advances in Neural Information Processing Systems*, 2020. **Abstract:** We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion (@ho2020ddpm)

Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans Cascaded diffusion models for high fidelity image generation , 2021. **Abstract:** We show that cascaded diffusion models are capable of generating high fidelity images on the class-conditional ImageNet generation benchmark, without any assistance from auxiliary image classifiers to boost sample quality. A cascaded diffusion model comprises a pipeline of multiple diffusion models that generate images of increasing resolution, beginning with a standard diffusion model at the lowest resolution, followed by one or more super-resolution diffusion models that successively upsample the image and add higher resolution details. We find that the sample quality of a cascading pipeline relies crucially on conditioning augmentation, our proposed method of data augmentation of the lower resolution conditioning inputs to the super-resolution models. Our experiments show that conditioning augmentation prevents compounding error during sampling in a cascaded model, helping us to train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at 128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and classification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256, outperforming VQ-VAE-2. (@ho2021cascaded)

Jonathan Ho and Tim Salimans Classifier-free diffusion guidance In *NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications*, 2021. **Abstract:** Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance. (@ho2021classifierfree)

Emiel Hoogeboom and Tim Salimans Blurring diffusion models , 2022. **Abstract:** Recently, Rissanen et al., (2022) have presented a new type of diffusion process for generative modeling based on heat dissipation, or blurring, as an alternative to isotropic Gaussian diffusion. Here, we show that blurring can equivalently be defined through a Gaussian diffusion process with non-isotropic noise. In making this connection, we bridge the gap between inverse heat dissipation and denoising diffusion, and we shed light on the inductive bias that results from this modeling choice. Finally, we propose a generalized class of diffusion models that offers the best of both standard Gaussian denoising diffusion and inverse heat dissipation, which we call Blurring Diffusion Models. (@hoogeboom2022blurring)

Emiel Hoogeboom, Victor Garcia Satorras, Clément Vignac, and Max Welling Equivariant diffusion for molecule generation in 3d In *International Conference on Machine Learning (ICML)*, 2022. **Abstract:** This work introduces a diffusion model for molecule generation in 3D that is equivariant to Euclidean transformations. Our E(3) Equivariant Diffusion Model (EDM) learns to denoise a diffusion process with an equivariant network that jointly operates on both continuous (atom coordinates) and categorical features (atom types). In addition, we provide a probabilistic analysis which admits likelihood computation of molecules using our model. Experimentally, the proposed method significantly outperforms previous 3D molecular generative methods regarding the quality of generated samples and efficiency at training time. (@hoogeboom2022equivariant)

Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jon Tamir Robust compressed sensing mri with deep generative priors , 34:14938–14954, 2021. **Abstract:** The CSGM framework (Bora-Jalal-Price-Dimakis’17) has shown that deep generative priors can be powerful tools for solving inverse problems. However, to date this framework has been empirically successful only on certain datasets (for example, human faces and MNIST digits), and it is known to perform poorly on out-of-distribution samples. In this paper, we present the first successful application of the CSGM framework on clinical MRI data. We train a generative prior on brain scans from the fastMRI dataset, and show that posterior sampling via Langevin dynamics achieves high quality reconstructions. Furthermore, our experiments and theory show that posterior sampling is robust to changes in the ground-truth distribution and measurement process. Our code and models are available at: \\}url{https://github.com/utcsilab/csgm-mri-langevin}. (@jalal2021robust)

Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi Jaakkola Torsional diffusion for molecular conformer generation , 35:24240–24253, 2022. **Abstract:** Molecular conformer generation is a fundamental task in computational chemistry. Several machine learning approaches have been developed, but none have outperformed state-of-the-art cheminformatics methods. We propose torsional diffusion, a novel diffusion framework that operates on the space of torsion angles via a diffusion process on the hypertorus and an extrinsic-to-intrinsic score model. On a standard benchmark of drug-like molecules, torsional diffusion generates superior conformer ensembles compared to machine learning and cheminformatics methods in terms of both RMSD and chemical properties, and is orders of magnitude faster than previous diffusion-based models. Moreover, our model provides exact likelihoods, which we employ to build the first generalizable Boltzmann generator. Code is available at https://github.com/gcorso/torsional-diffusion. (@jing2022torsional)

Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine Elucidating the design space of diffusion-based generative models , 2022. **Abstract:** We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36. (@karras2022elucidating)

Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song Denoising diffusion restoration models , 35:23593–23606, 2022. **Abstract:** Many interesting tasks in image restoration can be cast as linear inverse problems. A recent family of approaches for solving these problems uses stochastic algorithms that sample from the posterior distribution of natural images given the measurements. However, efficient solutions often require problem-specific supervised training to model the posterior, whereas unsupervised methods that are not problem-specific typically rely on inefficient iterative methods. This work addresses these issues by introducing Denoising Diffusion Restoration Models (DDRM), an efficient, unsupervised posterior sampling method. Motivated by variational inference, DDRM takes advantage of a pre-trained denoising diffusion generative model for solving any linear inverse problem. We demonstrate DDRM’s versatility on several image datasets for super-resolution, deblurring, inpainting, and colorization under various amounts of measurement noise. DDRM outperforms the current leading unsupervised methods on the diverse ImageNet dataset in reconstruction quality, perceptual quality, and runtime, being 5x faster than the nearest competitor. DDRM also generalizes well for natural images out of the distribution of the observed ImageNet training set. (@kawar2022denoising)

Bahjat Kawar, Noam Elata, Tomer Michaeli, and Michael Elad Gsure-based diffusion model training with corrupted data , 2023. **Abstract:** Diffusion models have demonstrated impressive results in both data generation and downstream tasks such as inverse problems, text-based editing, classification, and more. However, training such models usually requires large amounts of clean signals which are often difficult or impossible to obtain. In this work, we propose a novel training technique for generative diffusion models based only on corrupted data. We introduce a loss function based on the Generalized Stein’s Unbiased Risk Estimator (GSURE), and prove that under some conditions, it is equivalent to the training objective used in fully supervised diffusion models. We demonstrate our technique on face images as well as Magnetic Resonance Imaging (MRI), where the use of undersampled data significantly alleviates data collection costs. Our approach achieves generative performance comparable to its fully supervised counterpart without training on any clean signals. In addition, we deploy the resulting diffusion model in various downstream tasks beyond the degradation present in the training set, showcasing promising results. (@kawar2023gsure)

Gavin Kerrigan, Justin Ley, and Padhraic Smyth Diffusion generative models in infinite dimensions , 2022. **Abstract:** Diffusion generative models have recently been applied to domains where the available data can be seen as a discretization of an underlying function, such as audio signals or time series. However, these models operate directly on the discretized data, and there are no semantics in the modeling process that relate the observed data to the underlying functional forms. We generalize diffusion models to operate directly in function space by developing the foundational theory for such models in terms of Gaussian measures on Hilbert spaces. A significant benefit of our function space point of view is that it allows us to explicitly specify the space of functions we are working in, leading us to develop methods for diffusion generative modeling in Sobolev spaces. Our approach allows us to perform both unconditional and conditional generation of function-valued data. We demonstrate our methods on several synthetic and real-world benchmarks. (@kerrigan2022diffusion)

Gavin Kerrigan, Giosue Migliorini, and Padhraic Smyth Functional flow matching , 2023. **Abstract:** We propose Functional Flow Matching (FFM), a function-space generative model that generalizes the recently-introduced Flow Matching model to operate in infinite-dimensional spaces. Our approach works by first defining a path of probability measures that interpolates between a fixed Gaussian measure and the data distribution, followed by learning a vector field on the underlying space of functions that generates this path of measures. Our method does not rely on likelihoods or simulations, making it well-suited to the function space setting. We provide both a theoretical framework for building such models and an empirical evaluation of our techniques. We demonstrate through experiments on several real-world benchmarks that our proposed FFM method outperforms several recently proposed function-space generative models. (@kerrigan2023functional)

Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi Text2video-zero: Text-to-image diffusion models are zero-shot video generators In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 15954–15964, 2023. **Abstract:** Recent text-to-video generation approaches rely on computationally heavy training and require large-scale video datasets. In this paper, we introduce a new task, zero-shot text-to-video generation, and propose a low-cost approach (without any training or optimization) by leveraging the power of existing text-to-image synthesis methods (e.g. Stable Diffusion), making them suitable for the video domain. Our key modifications include (i) enriching the latent codes of the generated frames with motion dynamics to keep the global scene and the background time consistent; and (ii) reprogramming frame-level self-attention using a new cross-frame attention of each frame on the first frame, to preserve the context, appearance, and identity of the foreground object. Experiments show that this leads to low overhead, yet high-quality and remarkably consistent video generation. Moreover, our approach is not limited to text-to-video synthesis but is also applicable to other tasks such as conditional and content-specialized video generation, and Video Instruct-Pix2Pix, i.e., instruction-guided video editing. As experiments show, our method performs comparably or sometimes better than recent approaches, despite not being trained on additional video data. Our code is publicly available at: https://github.com/Picsart-AI-Research/Text2Video-Zero. (@khachatryan2023text2video)

Jae Hyun Lim, Nikola B Kovachki, Ricardo Baptista, Christopher Beckham, Kamyar Azizzadenesheli, Jean Kossaifi, Vikram Voleti, Jiaming Song, Karsten Kreis, Jan Kautz, et al Score-based diffusion models in function space , 2023. **Abstract:** Diffusion models have recently emerged as a powerful framework for generative modeling. They consist of a forward process that perturbs input data with Gaussian white noise and a reverse process that learns a score function to generate samples by denoising. Despite their tremendous success, they are mostly formulated on finite-dimensional spaces, e.g., Euclidean, limiting their applications to many domains where the data has a functional form, such as in scientific computing and 3D geometric data analysis. This work introduces a mathematically rigorous framework called Denoising Diffusion Operators (DDOs) for training diffusion models in function space. In DDOs, the forward process perturbs input functions gradually using a Gaussian process. The generative process is formulated by a function-valued annealed Langevin dynamic. Our approach requires an appropriate notion of the score for the perturbed data distribution, which we obtain by generalizing denoising score matching to function spaces that can be infinite-dimensional. We show that the corresponding discretized algorithm generates accurate samples at a fixed cost independent of the data resolution. We theoretically and numerically verify the applicability of our approach on a set of function-valued problems, including generating solutions to the Navier-Stokes equation viewed as the push-forward distribution of forcings from a Gaussian Random Field (GRF), as well as volcano InSAR and MNIST-SDF. (@lim2023score)

Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A Theodorou, Weili Nie, and Anima Anandkumar I\\(^2\\)sb: Image-to-image schrödinger bridge , 2023. **Abstract:** We propose Image-to-Image Schr\\}"odinger Bridge (I$\^2$SB), a new class of conditional diffusion models that directly learn the nonlinear diffusion processes between two given distributions. These diffusion bridges are particularly useful for image restoration, as the degraded images are structurally informative priors for reconstructing the clean images. I$\^2$SB belongs to a tractable class of Schr\\}"odinger bridge, the nonlinear extension to score-based models, whose marginal distributions can be computed analytically given boundary pairs. This results in a simulation-free framework for nonlinear diffusions, where the I$\^2$SB training becomes scalable by adopting practical techniques used in standard diffusion models. We validate I$\^2$SB in solving various image restoration tasks, including inpainting, super-resolution, deblurring, and JPEG restoration on ImageNet 256x256 and show that I$\^2$SB surpasses standard conditional diffusion models with more interpretable generative processes. Moreover, I$\^2$SB matches the performance of inverse methods that additionally require the knowledge of the corruption operators. Our work opens up new algorithmic opportunities for developing efficient nonlinear diffusion models on a large scale. scale. Project page and codes: https://i2sb.github.io/ (@liu2023i2sb)

Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia Video-p2p: Video editing with cross-attention control , 2023. **Abstract:** This paper presents Video-P2P, a novel framework for real-world video editing with cross-attention control. While attention control has proven effective for image editing with pre-trained image generation models, there are currently no large-scale video generation models publicly available. Video-P2P addresses this limitation by adapting an image generation diffusion model to complete various video editing tasks. Specifically, we propose to first tune a Text-to-Set (T2S) model to complete an approximate inversion and then optimize a shared unconditional embedding to achieve accurate video inversion with a small memory cost. For attention control, we introduce a novel decoupled-guidance strategy, which uses different guidance strategies for the source and target prompts. The optimized unconditional embedding for the source prompt improves reconstruction ability, while an initialized unconditional embedding for the target prompt enhances editability. Incorporating the attention maps of these two branches enables detailed editing. These technical designs enable various text-driven editing applications, including word swap, prompt refinement, and attention re-weighting. Video-P2P works well on real-world videos for generating new characters while optimally preserving their original poses and scenes. It significantly outperforms previous approaches. (@liu2023video)

Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, and Lu Hou Fetv: A benchmark for fine-grained evaluation of open-domain text-to-video generation , 2023. **Abstract:** Recently, open-domain text-to-video (T2V) generation models have made remarkable progress. However, the promising results are mainly shown by the qualitative cases of generated videos, while the quantitative evaluation of T2V models still faces two critical problems. Firstly, existing studies lack fine-grained evaluation of T2V models on different categories of text prompts. Although some benchmarks have categorized the prompts, their categorization either only focuses on a single aspect or fails to consider the temporal information in video generation. Secondly, it is unclear whether the automatic evaluation metrics are consistent with human standards. To address these problems, we propose FETV, a benchmark for Fine-grained Evaluation of Text-to-Video generation. FETV is multi-aspect, categorizing the prompts based on three orthogonal aspects: the major content, the attributes to control and the prompt complexity. FETV is also temporal-aware, which introduces several temporal categories tailored for video generation. Based on FETV, we conduct comprehensive manual evaluations of four representative T2V models, revealing their pros and cons on different categories of prompts from different aspects. We also extend FETV as a testbed to evaluate the reliability of automatic T2V metrics. The multi-aspect categorization of FETV enables fine-grained analysis of the metrics’ reliability in different scenarios. We find that existing automatic metrics (e.g., CLIPScore and FVD) correlate poorly with human evaluation. To address this problem, we explore several solutions to improve CLIPScore and FVD, and develop two automatic metrics that exhibit significant higher correlation with humans than existing metrics. Benchmark page: https://github.com/llyx97/FETV. (@liu2023fetv)

Morteza Mardani, Jiaming Song, Jan Kautz, and Arash Vahdat A variational perspective on solving inverse problems with diffusion models In *The Twelfth International Conference on Learning Representations*, 2023. **Abstract:** Diffusion models have emerged as a key pillar of foundation models in visual domains. One of their critical applications is to universally solve different downstream inverse tasks via a single diffusion prior without re-training for each task. Most inverse tasks can be formulated as inferring a posterior distribution over data (e.g., a full image) given a measurement (e.g., a masked image). This is however challenging in diffusion models since the nonlinear and iterative nature of the diffusion process renders the posterior intractable. To cope with this challenge, we propose a variational approach that by design seeks to approximate the true posterior distribution. We show that our approach naturally leads to regularization by denoising diffusion process (RED-Diff) where denoisers at different timesteps concurrently impose different structural constraints over the image. To gauge the contribution of denoisers from different timesteps, we propose a weighting mechanism based on signal-to-noise-ratio (SNR). Our approach provides a new variational perspective for solving inverse problems with diffusion models, allowing us to formulate sampling as stochastic optimization, where one can simply apply off-the-shelf solvers with lightweight iterates. Our experiments for image restoration tasks such as inpainting and superresolution demonstrate the strengths of our method compared with state-of-the-art sampling-based diffusion models. (@mardani2023variational)

Suraj Patil Sdxl inpainting model 2024. Accessed: 2024-05-21. **Abstract:** Digital image manipulation has become increasingly accessible and realistic with the advent of generative AI technologies. Recent developments allow for text-guided inpainting, making sophisticated image edits possible with minimal effort. This poses new challenges for digital media forensics. For example, diffusion model-based approaches could either splice the inpainted region into the original image, or regenerate the entire image. In the latter case, traditional image forgery localization (IFL) methods typically fail. This paper introduces the Text-Guided Inpainting Forgery (TGIF) dataset, a comprehensive collection of images designed to support the training and evaluation of image forgery localization and synthetic image detection (SID) methods. The TGIF dataset includes approximately 75k forged images, originating from popular open-source and commercial methods, namely SD2, SDXL, and Adobe Firefly. We benchmark several state-of-the-art IFL and SID methods on TGIF. Whereas traditional IFL methods can detect spliced images, they fail to detect regenerated inpainted images. Moreover, traditional SID may detect the regenerated inpainted images to be fake, but cannot localize the inpainted area. Finally, both IFL and SID methods fail when exposed to stronger compression, while they are less robust to modern compression algorithms, such as WEBP. In conclusion, this work demonstrates the inefficiency of state-of-the-art detectors on local manipulations performed by modern generative approaches, and aspires to help with the development of more capable IFL and SID methods. The dataset and code can be downloaded at https://github.com/IDLabMedia/tgif-dataset. (@sdxl_inp)

William Peebles and Saining Xie Scalable diffusion models with transformers , 2022. **Abstract:** We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops – through increased transformer depth/width or increased number of input tokens – consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512x512 and 256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter. (@peebles2022scalable)

Jakiw Pidstrigach, Youssef Marzouk, Sebastian Reich, and Sven Wang Infinite-dimensional diffusion models , 2023. **Abstract:** Diffusion models have had a profound impact on many application areas, including those where data are intrinsically infinite-dimensional, such as images or time series. The standard approach is first to discretize and then to apply diffusion models to the discretized data. While such approaches are practically appealing, the performance of the resulting algorithms typically deteriorates as discretization parameters are refined. In this paper, we instead directly formulate diffusion-based generative models in infinite dimensions and apply them to the generative modelling of functions. We prove that our formulations are well posed in the infinite-dimensional setting and provide dimension-independent distance bounds from the sample to the target measure. Using our theory, we also develop guidelines for the design of infinite-dimensional diffusion models. For image distributions, these guidelines are in line with current canonical choices. For other distributions, however, we can improve upon these canonical choices. We demonstrate these results both theoretically and empirically, by applying the algorithms to data distributions on manifolds and to distributions arising in Bayesian inverse problems or simulation-based inference. (@pidstrigach2023infinite)

Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach : Improving latent diffusion models for high-resolution image synthesis In *The Twelfth International Conference on Learning Representations (ICLR)*, 2024. **Abstract:** We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at https://github.com/Stability-AI/generative-models (@podell2024sdxl)

Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen Fatezero: Fusing attentions for zero-shot text-based video editing In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 15932–15942, 2023. **Abstract:** The diffusion-based generative models have achieved remarkable success in text-based image generation. However, since it contains enormous randomness in generation progress, it is still challenging to apply such models for real-world visual content editing, especially in videos. In this paper, we propose FateZero, a zero-shot text-based editing method on real-world videos without per-prompt training or use-specific mask. To edit videos consistently, we propose several techniques based on the pre-trained models. Firstly, in contrast to the straightforward DDIM inversion technique, our approach captures intermediate attention maps during inversion, which effectively retain both structural and motion information. These maps are directly fused in the editing process rather than generated during denoising. To further minimize semantic leakage of the source video, we then fuse self-attentions with a blending mask obtained by cross-attention features from the source prompt. Furthermore, we have implemented a reform of the self-attention mechanism in denoising UNet by introducing spatial-temporal attention to ensure frame consistency. Yet succinct, our method is the first one to show the ability of zero-shot text-driven video style and local attribute editing from the trained text-to-image model. We also have a better zero-shot shape-aware editing ability based on the text-to-video model \[52\]. Extensive experiments demonstrate our superior temporal consistency and editing capability than previous works. (@qi2023fatezero)

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al Learning transferable visual models from natural language supervision In *International conference on machine learning*, pages 8748–8763. PMLR, 2021. **Abstract:** State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP. (@radford2021learningclip)

Ali Rahimi and Benjamin Recht Random features for large-scale kernel machines , 20, 2007. **Abstract:** To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. The features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms applied to these features outperform state-of-the-art large-scale kernel machines. (@rahimi2007random)

Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen Hierarchical text-conditional image generation with clip latents , 2022. **Abstract:** Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples. (@ramesh2022dalle2)

C.E. Rasmussen and C.K.I. Williams Adaptive Computation and Machine Learning series. MIT Press, 2005. **Abstract:** The research interest on indoor Location-Based Services (LBS) has increased during the last years, especially using LED lighting, since they can deal with the dual functionality of lighting and localization with centimetric accuracy. There are several positioning approaches using lateration and angular methods. These methods typically rely on the physical model to deal with the multipath effect, environmental fluctuations, calibration of the optical setup, etc. A recent approach is the use of Machine Learning (ML) techniques. ML techniques provide accurate location estimates based on observed data without requiring the underlying physical model to be described. This work proposes an optical indoor local positioning system based on multiple LEDs and a quadrant photodiode plus an aperture. Different frequencies are used to allow the simultaneous emission of all transmitted signals and their processing at the receiver. For that purpose, two algorithms are developed. First, a triangulation algorithm based on Angle of Arrival (AoA) measurements, which uses the Received Signal Strength (RSS) values from every LED on each quadrant to determine the image points projected from each emitter on the receiver and, then, implements a Least Squares Estimator (LSE) and trigonometric considerations to estimate the receiver’s position. Secondly, the performance of a data-driven approach using Gaussian Processes is evaluated. The proposals have been experimentally validated in an area of 3 × 3 m$\^{2}$ and a height of 1.3 m (distance from transmitters to receiver). The experimental tests achieve p50 and p95 2D absolute errors below 9.38 cm and 21.94 cm for the AoA-based triangulation algorithm, and 3.62 cm and 16.65 cm for the Gaussian Processes. (@rasmussen2005gaussian)

Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer High-resolution image synthesis with latent diffusion models , 2021. **Abstract:** By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion . (@rombach2021highresolution)

Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer High-resolution image synthesis with latent diffusion models In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 10684–10695, 2022. **Abstract:** By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. (@rombach2022high)

Olaf Ronneberger, Philipp Fischer, and Thomas Brox U-net: Convolutional networks for biomedical image segmentation In *Medical Image Computing and Computer-Assisted Intervention (MICCAI)*, 2015. **Abstract:** There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net . (@ronneberger2015unet)

Claudio Rota, Marco Buzzelli, and Joost van de Weijer Enhancing perceptual quality in video super-resolution through temporally-consistent detail synthesis using diffusion models , 2023. **Abstract:** In this paper, we address the problem of enhancing perceptual quality in video super-resolution (VSR) using Diffusion Models (DMs) while ensuring temporal consistency among frames. We present StableVSR, a VSR method based on DMs that can significantly enhance the perceptual quality of upscaled videos by synthesizing realistic and temporally-consistent details. We introduce the Temporal Conditioning Module (TCM) into a pre-trained DM for single image super-resolution to turn it into a VSR method. TCM uses the novel Temporal Texture Guidance, which provides it with spatially-aligned and detail-rich texture information synthesized in adjacent frames. This guides the generative process of the current frame toward high-quality and temporally-consistent results. In addition, we introduce the novel Frame-wise Bidirectional Sampling strategy to encourage the use of information from past to future and vice-versa. This strategy improves the perceptual quality of the results and the temporal consistency across frames. We demonstrate the effectiveness of StableVSR in enhancing the perceptual quality of upscaled videos while achieving better temporal consistency compared to existing state-of-the-art methods for VSR. The project page is available at https://github.com/claudiom4sir/StableVSR. (@rota2023enhancing)

Litu Rout, Negin Raoof, Giannis Daras, Constantine Caramanis, Alex Dimakis, and Sanjay Shakkottai Solving linear inverse problems provably via posterior sampling with latent diffusion models , 36, 2024. **Abstract:** We present the first framework to solve linear inverse problems leveraging pre-trained latent diffusion models. Previously proposed algorithms (such as DPS and DDRM) only apply to pixel-space diffusion models. We theoretically analyze our algorithm showing provable sample recovery in a linear model setting. The algorithmic insight obtained from our analysis extends to more general settings often considered in practice. Experimentally, we outperform previously proposed posterior sampling algorithms in a wide variety of problems including random inpainting, block inpainting, denoising, deblurring, destriping, and super-resolution. (@rout2024solving)

Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi Palette: Image-to-image diffusion models In *ACM SIGGRAPH 2022 conference proceedings*, pages 1–10, 2022. **Abstract:** This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out https://diffusion-palette.github.io/ for an overview of the results and code. (@saharia2022palette)

Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans, David J. Fleet, and Mohammad Norouzi Palette: Image-to-image diffusion models , 2021. **Abstract:** This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out https://diffusion-palette.github.io for an overview of the results. (@saharia2021palette)

Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi Photorealistic text-to-image diffusion models with deep language understanding , 2022. **Abstract:** We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results. (@saharia2022imagen)

Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi Image super-resolution via iterative refinement , 2021. **Abstract:** We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models to conditional image generation and performs super-resolution through a stochastic denoising process. Inference starts with pure Gaussian noise and iteratively refines the noisy output using a U-Net model trained on denoising at various noise levels. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA GAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic outputs, while GANs do not exceed a fool rate of 34%. We further show the effectiveness of SR3 in cascaded image generation, where generative models are chained with super-resolution models, yielding a competitive FID score of 11.3 on ImageNet. (@saharia2021image)

Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi Image super-resolution via iterative refinement , 45(4):4713–4726, 2022. **Abstract:** We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models (Ho et al. 2020), (Sohl-Dickstein et al. 2015) to image-to-image translation, and performs super-resolution through a stochastic iterative denoising process. Output images are initialized with pure Gaussian noise and iteratively refined using a U-Net architecture that is trained on denoising at various noise levels, conditioned on a low-resolution input image. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8× face super-resolution task on CelebA-HQ for which SR3 achieves a fool rate close to 50%, suggesting photo-realistic outputs, while GAN baselines do not exceed a fool rate of 34%. We evaluate SR3 on a 4× super-resolution task on ImageNet, where SR3 outperforms baselines in human evaluation and classification accuracy of a ResNet-50 classifier trained on high-resolution images. We further show the effectiveness of SR3 in cascaded image generation, where a generative model is chained with super-resolution models to synthesize high-resolution images with competitive FID scores on the class-conditional 256×256 ImageNet generation challenge. (@saharia2022image)

Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen Improved techniques for training gans , 29, 2016. **Abstract:** We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes. (@salimans2016improvedinception)

Yuyang Shi, Valentin De Bortoli, George Deligiannidis, and Arnaud Doucet Conditional simulation using diffusion schrödinger bridges , 2022. **Abstract:** Denoising diffusion models have recently emerged as a powerful class of generative models. They provide state-of-the-art results, not only for unconditional simulation, but also when used to solve conditional simulation problems arising in a wide range of inverse problems. A limitation of these models is that they are computationally intensive at generation time as they require simulating a diffusion process over a long time horizon. When performing unconditional simulation, a Schr\\}"odinger bridge formulation of generative modeling leads to a theoretically grounded algorithm shortening generation time which is complementary to other proposed acceleration techniques. We extend the Schr\\}"odinger bridge framework to conditional simulation. We demonstrate this novel methodology on various applications including image super-resolution, optimal filtering for state-space models and the refinement of pre-trained networks. Our code can be found at https://github.com/vdeborto/cdsb. (@shi2022conditional)

Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman In *The Eleventh International Conference on Learning Representations (ICLR)*, 2023. **Abstract:** We propose Make-A-Video – an approach for directly translating the tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our intuition is simple: learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage. Make-A-Video has three advantages: (1) it accelerates training of the T2V model (it does not need to learn visual and multimodal representations from scratch), (2) it does not require paired text-video data, and (3) the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today’s image generation models. We design a simple yet effective way to build on T2I models with novel and effective spatial-temporal modules. First, we decompose the full temporal U-Net and attention tensors and approximate them in space and time. Second, we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder, interpolation model and two super resolution models that can enable various applications besides T2V. In all aspects, spatial and temporal resolution, faithfulness to text, and quality, Make-A-Video sets the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures. (@singer2023makeavideo)

Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli Deep unsupervised learning using nonequilibrium thermodynamics In *International Conference on Machine Learning*, 2015. **Abstract:** A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm. (@sohl2015deep)

Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz Pseudoinverse-guided diffusion models for inverse problems In *International Conference on Learning Representations*, 2022. (@song2022pseudoinverse)

Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz Pseudoinverse-guided diffusion models for inverse problems In *International Conference on Learning Representations*, 2023. (@song2023PiGDM)

Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole Score-based generative modeling through stochastic differential equations In *International Conference on Learning Representations*, 2021. **Abstract:** Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\\}aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model. (@song2020score)

Zachary Teed and Jia Deng Raft: Recurrent all-pairs field transforms for optical flow In *Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16*, pages 402–419. Springer, 2020. **Abstract:** We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical ow. RAFT extracts per- pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a ow eld through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves state- of-the-art performance. On KITTI, RAFT achieves an F1-all error of 5.10%, a 16% error reduction from the best published result (6.10%). On Sintel ( nal pass), RAFT obtains an end-point-error of 2.855 pixels, a 30% error reduction from the best published result (4.098 pixels). In addition, RAFT has strong cross-dataset generalization as well as high eciency in inference time, training speed, and parameter count. Code is available at https://github.com/princeton-vl/RAFT . 1 Introduction Optical ow is the task of estimating per-pixel motion between video frames. It is a long-standing vision problem that remains unsolved. The best systems are limited by diculties including fast-moving objects, occlusions, motion blur, and textureless surfaces. Optical ow has traditionally been approached as a hand-crafted optimiza- tion problem over the space of dense displacement elds between a pair of im- ages \[21,51,13\]. Generally, the optimization objective de nes a trade-o between adata term which encourages the alignment of visually similar image regions and a regularization term which imposes priors on the plausibility of motion. Such an approach has achieved considerable success, but further progress has appeared challenging, due to the diculties in hand-designing an optimization objective that is robust to a variety of corner cases. Recently, deep learning has been shown as a promising alternative to tradi- tional methods. Deep learning can side-step formulating an optimization prob- lem and train a network to directly predict ow. Current deep learning meth- ods \[25,42,22,49,20\] have achieved performance comparable to the best tradi- tional methods while being signi cantly faster at inference time. A key question for further research is designing e ective architectures that perform better, train more easily and generalize well to novel scenes. We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical ow. RAFT enjoys the following strengths: arXiv:2003.12039v3 \[cs.CV\] 25 Aug 20202 Z. Teed and J. Deng ⟨⋅,⋅⟩       0 Frame 1 Frame 1Frame 2 Feature Encoder Context EncoderOptical Flow10 (@teed2020raft)

Yinhuai Wang, Jiwen Yu, and Jian Zhang Zero-shot image restoration using denoising diffusion null-space model , 2022. **Abstract:** Most existing Image Restoration (IR) models are task-specific, which can not be generalized to different degradation operators. In this work, we propose the Denoising Diffusion Null-Space Model (DDNM), a novel zero-shot framework for arbitrary linear IR problems, including but not limited to image super-resolution, colorization, inpainting, compressed sensing, and deblurring. DDNM only needs a pre-trained off-the-shelf diffusion model as the generative prior, without any extra training or network modifications. By refining only the null-space contents during the reverse diffusion process, we can yield diverse results satisfying both data consistency and realness. We further propose an enhanced and robust version, dubbed DDNM+, to support noisy restoration and improve restoration quality for hard tasks. Our experiments on several IR tasks reveal that DDNM outperforms other state-of-the-art zero-shot IR methods. We also demonstrate that DDNM+ can solve complex real-world applications, e.g., old photo restoration. (@wang2022zero)

Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli Image quality assessment: from error visibility to structural similarity , 13(4):600–612, 2004. **Abstract:** Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/. (@wang2004imagessim)

Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan Saharia, Alexandros G. Dimakis, and Peyman Milanfar Deblurring via stochastic refinement , 2021. **Abstract:** Image deblurring is an ill-posed problem with multiple plausible solutions for a given input image. However, most existing methods produce a deterministic estimate of the clean image and are trained to minimize pixel-level distortion. These metrics are known to be poorly correlated with human perception, and often lead to unrealistic reconstructions. We present an alternative framework for blind deblurring based on conditional diffusion models. Unlike existing techniques, we train a stochastic sampler that refines the output of a deterministic predictor and is capable of producing a diverse set of plausible reconstructions for a given input. This leads to a significant improvement in perceptual quality over existing state-of-the-art methods across multiple standard benchmarks. Our predict-and-refine approach also enables much more efficient sampling compared to typical diffusion models. Combined with a carefully tuned network architecture and inference procedure, our method is competitive in terms of distortion metrics such as PSNR. These results show clear benefits of our diffusion-based method for deblurring and challenge the widely used strategy of producing a single, deterministic reconstruction. (@whang2021deblurring)

James Wilson, Viacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky, and Marc Deisenroth Efficiently sampling functions from gaussian process posteriors In *International Conference on Machine Learning*, pages 10292–10302. PMLR, 2020. **Abstract:** Gaussian processes are the gold standard for many real-world modeling problems, especially in cases where a model’s success hinges upon its ability to faithfully represent predictive uncertainty. These problems typically exist as parts of larger frameworks, wherein quantities of interest are ultimately defined by integrating over posterior distributions. These quantities are frequently intractable, motivating the use of Monte Carlo methods. Despite substantial progress in scaling up Gaussian processes to large training sets, methods for accurately generating draws from their posterior distributions still scale cubically in the number of test locations. We identify a decomposition of Gaussian processes that naturally lends itself to scalable sampling by separating out the prior from the data. Building off of this factorization, we propose an easy-to-use and general-purpose approach for fast posterior sampling, which seamlessly pairs with sparse approximations to afford scalability both during training and at test time. In a series of experiments designed to test competing sampling schemes’ statistical properties and practical ramifications, we demonstrate how decoupled sample paths accurately represent Gaussian process posteriors at a fraction of the usual cost. (@wilson2020efficiently)

Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 7623–7633, 2023. **Abstract:** To replicate the success of text-to-image (T2I) generation, recent works employ large-scale video datasets to train a text-to-video (T2V) generator. Despite their promising results, such paradigm is computationally expensive. In this work, we propose a new T2V generation setting—One-Shot Video Tuning, where only one text-video pair is presented. Our model is built on state-of-the-art T2I diffusion models pre-trained on massive image data. We make two key observations: 1) T2I models can generate still images that represent verb terms; 2) extending T2I models to generate multiple images concurrently exhibits surprisingly good content consistency. To further learn continuous motion, we introduce Tune-A-Video, which involves a tailored spatio-temporal attention mechanism and an efficient one-shot tuning strategy. At inference, we employ DDIM inversion to provide structure guidance for sampling. Extensive qualitative and numerical experiments demonstrate the remarkable ability of our method across various applications. (@wu2023tune)

Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang Geodiff: A geometric diffusion model for molecular conformation generation In *International Conference on Learning Representations (ICLR)*, 2022. **Abstract:** Predicting molecular conformations from molecular graphs is a fundamental problem in cheminformatics and drug discovery. Recently, significant progress has been achieved with machine learning approaches, especially with deep generative models. Inspired by the diffusion process in classical non-equilibrium thermodynamics where heated particles will diffuse from original states to a noise distribution, in this paper, we propose a novel generative model named GeoDiff for molecular conformation prediction. GeoDiff treats each atom as a particle and learns to directly reverse the diffusion process (i.e., transforming from a noise distribution to stable conformations) as a Markov chain. Modeling such a generation process is however very challenging as the likelihood of conformations should be roto-translational invariant. We theoretically show that Markov chains evolving with equivariant Markov kernels can induce an invariant distribution by design, and further propose building blocks for the Markov kernels to preserve the desirable equivariance property. The whole framework can be efficiently trained in an end-to-end fashion by optimizing a weighted variational lower bound to the (conditional) likelihood. Experiments on multiple benchmarks show that GeoDiff is superior or comparable to existing state-of-the-art approaches, especially on large molecules. (@xu2022geodiff)

Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy Rerender a video: Zero-shot text-guided video-to-video translation In *SIGGRAPH Asia 2023 Conference Papers*, pages 1–11, 2023. **Abstract:** Large text-to-image diffusion models have exhibited impressive proficiency in generating high-quality images. However, when applying these models to video domain, ensuring temporal consistency across video frames remains a formidable challenge. This paper proposes a novel zero-shot text-guided video-to-video translation framework to adapt image models to videos. The framework includes two parts: key frame translation and full video translation. The first part uses an adapted diffusion model to generate key frames, with hierarchical cross-frame constraints applied to enforce coherence in shapes, textures and colors. The second part propagates the key frames to other frames with temporal-aware patch matching and frame blending. Our framework achieves global style and local texture temporal consistency at a low cost (without re-training or optimization). The adaptation is compatible with existing image diffusion techniques, allowing our framework to take advantage of them, such as customizing a specific subject with LoRA, and introducing extra spatial guidance with ControlNet. Extensive experimental results demonstrate the effectiveness of our proposed framework over existing methods in rendering high-quality and temporally-coherent videos. Code is available at our project page: https://www.mmlab-ntu.com/project/rerender/ (@yang2023rerender)

Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang The unreasonable effectiveness of deep features as a perceptual metric In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 586–595, 2018. **Abstract:** While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations. (@zhang2018unreasonablelpips)

Zicheng Zhang, Bonan Li, Xuecheng Nie, Congying Han, Tiande Guo, and Luoqi Liu Towards consistent video editing with text-to-image diffusion models , 36, 2024. **Abstract:** Existing works have advanced Text-to-Image (TTI) diffusion models for video editing in a one-shot learning manner. Despite their low requirements of data and computation, these methods might produce results of unsatisfied consistency with text prompt as well as temporal sequence, limiting their applications in the real world. In this paper, we propose to address the above issues with a novel EI$\^2$ model towards \\}textbf{E}nhancing v\\}textbf{I}deo \\}textbf{E}diting cons\\}textbf{I}stency of TTI-based frameworks. Specifically, we analyze and find that the inconsistent problem is caused by newly added modules into TTI models for learning temporal information. These modules lead to covariate shift in the feature space, which harms the editing capability. Thus, we design EI$\^2$ to tackle the above drawbacks with two classical modules: Shift-restricted Temporal Attention Module (STAM) and Fine-coarse Frame Attention Module (FFAM). First, through theoretical analysis, we demonstrate that covariate shift is highly related to Layer Normalization, thus STAM employs a \\}textit{Instance Centering} layer replacing it to preserve the distribution of temporal features. In addition, {STAM} employs an attention layer with normalized mapping to transform temporal features while constraining the variance shift. As the second part, we incorporate {STAM} with a novel {FFAM}, which efficiently leverages fine-coarse spatial information of overall frames to further enhance temporal consistency. Extensive experiments demonstrate the superiority of the proposed EI$\^2$ model for text-driven video editing. (@zhang2024towards)

Shangchen Zhou et al Flow-guided diffusion for video inpainting , 2023. **Abstract:** The increasing use of medical imaging in healthcare settings presents a significant challenge due to the increasing workload for radiologists, yet it also offers opportunity for enhancing healthcare outcomes if effectively leveraged. 3D image retrieval holds potential to reduce radiologist workloads by enabling clinicians to efficiently search through diagnostically similar or otherwise relevant cases, resulting in faster and more precise diagnoses. However, the field of 3D medical image retrieval is still emerging, lacking established evaluation benchmarks, comprehensive datasets, and thorough studies. This paper attempts to bridge this gap by introducing a novel benchmark for 3D Medical Image Retrieval (3D-MIR) that encompasses four different anatomies imaged with computed tomography. Using this benchmark, we explore a diverse set of search strategies that use aggregated 2D slices, 3D volumes, and multi-modal embeddings from popular multi-modal foundation models as queries. Quantitative and qualitative assessments of each approach are provided alongside an in-depth discussion that offers insight for future research. To promote the advancement of this field, our benchmark, dataset, and code are made publicly available. (@zhou2023flow)

</div>

# Theoretical Results [app:theory]

## Convolutions and Equivariance [app:conv_equi]

To better understand <a href="#eq:equivariance" data-reference-type="eqref" data-reference="eq:equivariance">[eq:equivariance]</a>, we consider the following example. We will assume that \\(\xi : \mathbb{R}^2 \to \mathbb{R}\\) is a scalar-valued field (grayscale image) defined on the whole plane. Furthermore, we let \\(G\\) be given by a continuous convolution and \\(T_1^{-1}\\) be a translation. In particular, for all \\(x \in \mathbb{R}^2\\), \\[G(\xi)(x) = \int_{\mathbb{R}^2} \kappa(x - y) \xi(y) \: \mathsf{d}y, \qquad T^{-1}_1(x) = x - a\\] for some compactly supported kernel \\(\kappa: \mathbb{R}^2 \to \mathbb{R}\\) and a direction \\(a \in \mathbb{R}^2\\). We then have \\[G \big ( \xi \circ T^{-1}_1 \big ) \big ( x \big ) = \int_{\mathbb{R}^2} \kappa(x-y) \xi(y - a) \: \mathsf{d}y = \int_{\mathbb{R}^2} \kappa \big ( (x-a) -y \big ) \xi \big ( y \big ) \: \mathsf{d}y = G \big ( \xi \big ) \big ( T_1^{-1}(x) \big )\\] by the change of variables formula. This shows that \\(G\\) is equivariant to all translations. This is a well-known property of the convolution and, in particular, it shows that convolutional neural networks are translation equivariant, noting that pointwise non-linearities will preserve this property. This example shows that a model can be equivariant with respect to certain deformations by architectural design. However, in realstic video modeling, optical flows are not know explicitly and can only be approximated numerically. It is therefore natural to instead build-in approximate equivariance into a model instead of enforcing it directly in the architecture. Our guidance procedure in Section <a href="#sec:diffusion_guidance" data-reference-type="ref" data-reference="sec:diffusion_guidance">3.2</a> is an example such an approximate form of equivariance.

## Tweedie’s Formula [app:tweedie]

In Section <a href="#sec:diffusion_guidance" data-reference-type="ref" data-reference="sec:diffusion_guidance">3.2</a>, we show a diffusion model can be trained and sampled from using Gaussian process noise instead of white noise. Our result depends on the following lemma which is a simple generalization of Tweedie’s formula.

<div class="lemma" markdown="1">

**Lemma 1**. *Let \\(x\\) be a random variable with positive density \\(p_x \in C^1 (\mathbb{R}^k)\\). Let \\(\sigma > 0\\) and \\(z \sim \mathcal{N}(0,Q)\\) for some positive definite matrix \\(Q \in \mathbb{R}^{k \times k}\\) and assume that \\(x \perp z\\). Define the random variable \\[y = x + \sigma z\\] and let \\(p_y \in C^\infty (\mathbb{R}^k)\\) be the density of \\(y\\). It holds that \\[\nabla_y \log p_y(y) = \frac{1}{\sigma^2} Q^{-1} \big ( {\mathbb{E}}[x|y] - y \big ).\\]*

</div>

<div class="proof" markdown="1">

*Proof.* First note that by the chain rule, \\[\nabla_y \log p_y (y) = \frac{1}{p_y(y)} \nabla_y p_y (y) = \frac{1}{p_y(y)} \nabla_y \int_{\mathbb{R}^k} p(y,x) \: \mathsf{d}x\\] where \\(p(y,x)\\) denotes the joint density of \\((y,x)\\). Let \\(p(y|x)\\) denote the Gaussian density of the conditional \\(y|x\\). Since \\(p_x \in C^1(\mathbb{R}^k)\\), \\[\nabla_y \int_{\mathbb{R}^k} p(y,x) \: \mathsf{d}x = \int_{\mathbb{R}^k} \nabla_y p(y|x) p_x(x) \: \mathsf{d}x.\\] Therefore, by the chain rule, \\[\nabla_y \log p_y (y) = \frac{1}{p_y(y)} \int_{\mathbb{R}^k} p(y|x) p_x(x) \nabla_y \log p(y|x) \: \mathsf{d}x.\\] Since \\(p(y|x)\\) is the density of \\(\mathcal{N}(x,\sigma^2 Q)\\), a direct calculations shows that \\[\nabla_y \log p(y|x) = \frac{1}{\sigma^2} Q^{-1} \big ( x - y \big ).\\] Furthermore, Bayes’ theorem implies \\[p(y|x) p_x(x) = p(x|y) p_y(y).\\] Therefore, \\[\nabla_y \log p_y (y) = \frac{1}{\sigma^2} \int_{\mathbb{R}^k} Q^{-1} \big ( x - y \big ) p(x|y) \: \mathsf{d}x =  \frac{1}{\sigma^2} Q^{-1} \big ( {\mathbb{E}}[x|y] - y \big )\\] as desired. ◻

</div>

## Flow Equivariance [app:flow_equivariance]

In Section <a href="#sec:diffusion_guidance" data-reference-type="ref" data-reference="sec:diffusion_guidance">3.2</a>, we claim that if the score network \\(h_\theta\\) is equivarient with respect to a deformation \\(T^{-1}\\), then the Euler scheme approximation of the map \\(u(\tau) \mapsto u(0)\\) is equivarient with respect to \\(T^{-1}\\). It is easy to see that this results holds so long as it holds for the single step \\(u_t \mapsto u_{t - \Delta t}\\) defined by <a href="#eq:euler_scheme" data-reference-type="eqref" data-reference="eq:euler_scheme">[eq:euler_scheme]</a>. We will assume that \\(h_\theta\\) safisfies <a href="#eq:discrete_score_equivariance" data-reference-type="eqref" data-reference="eq:discrete_score_equivariance">[eq:discrete_score_equivariance]</a> written as \\[h_\theta (u_t \circ T^{-1}, t) = h_\theta (u_t, t) \circ T^{-1}.\\] We make sense of this equation by using RFF to define \\(u_t\\) as a function on the plane and similarly bilinear interpolation to define \\(h_\theta (u_t,t)\\) as a function. It follows by linearity of composition that \\[\begin{aligned}
    u_{t-\Delta t} \circ T^{-1} &= u_t \circ T^{-1} - \Delta t \frac{\dot{\sigma} (t)}{\sigma (t)} \big (h_\theta(u_t,t) \circ T^{-1} - u_t \circ T^{-1} \big ) \\
    &= u_t \circ T^{-1} - \Delta t \frac{\dot{\sigma} (t)}{\sigma (t)} \big (h_\theta(u_t \circ T^{-1},t) - u_t \circ T^{-1} \big )
\end{aligned}\\] which is the requisite equivariance of the map \\(u_t \mapsto u_{t - \Delta t}\\).

# Gaussian Processes [app:gp]

A probability measure \\(\eta\\) on \\(H\\) is called Gaussian if there exists an element \\(m \in H\\) and a self-adjoint, non-negative, trace-class operator \\(Q : H \to H\\) such that, for all \\(h, h' \in H\\), \\[\langle h, m \rangle  = \int_H \langle h, f \rangle \; \mathsf{d}\eta (f), \quad \langle Qh, h' \rangle = \int_H \langle h, f-m \rangle \langle h', f-m \rangle \; \mathsf{d}\eta (f),\\] where \\(\langle \cdot, \cdot \rangle\\) denotes the inner product on \\(H\\). The element \\(m\\) is called the *mean* while the operator \\(Q\\) is called the *covariance*. It is immediate from this definition that white noise is not included since the identity operator is not trace-class on any infinite dimensional space. This definition ensures that any realization of a random variable \\(\xi \sim \eta\\) is almost surely an element of \\(H\\). When the domain \\(D\\) of the elements of \\(H\\) is a subset of the real line, \\(\eta\\) is often called a *Gaussian process*. We continue to use this terminology even when \\(D\\) is a subset of a higher dimensional space i.e. \\(\mathbb{R}^2\\) but remark that the nomenclature *Gaussian random field* is sometimes preferred.

Since we working on a separable space, each such field on \\(H\\) has associated to it a unique reproducing kernel Hilbert space `\cite[Theorem 2.9]{da2014stochastic}`{=latex} which is associated to a unique positive definite kernel `\cite{aronszajn1950theory}`{=latex}. In particular, there exists a positive definite function \\(\kappa : D \times D \to \mathbb{R}\\) for which \\(Q\\) is its associated integral operator. It follows that a Gaussian process can be uniquely identified with a positive definite kernel. Sampling and conditioning this process can then be accomplished via the kernel matrix.

To make this explicit, suppose that \\(X = \{x_1,\dots,x_n\} \subset D\\) and \\(Y = \{y_1,\dots,y_m\} \subset D\\) are two sets of points in \\(D\\). We will slightly abuse notation and write \\[Q(X,Y)_{ij} \coloneqq \kappa (x_i, y_j), \qquad i=1,\dots,n \text{ and } j=1,\dots, m\\] for the kernel matrix between \\(X\\) and \\(Y\\) and similarly \\(Q(Y,X), Q(X,X), Q(Y,Y)\\). Suppose that \\(\xi \sim \eta\\) is a random variable from the Gaussian process with kernel \\(\kappa\\) and mean zero. To sample a realization of \\(\xi\\) on the points \\(X\\), we sample the finite dimensional Gaussian \\(\mathcal{N}\big( 0, Q(X,X) \big )\\). This can be written as \\[\xi(X) = Q(X,X)^{1/2} Z\\] where \\(Z \sim \mathcal{N}(0, I_n)\\). Suppose now that the points in \\(Y\\) are distinct from those in \\(X\\) and we want to sample \\(\xi\\) on \\(Y\\) given the realization \\(\xi(X)\\). This can be done by conditioning `\cite{rasmussen2005gaussian}`{=latex} \\[\xi(Y) | \xi(X) \sim \mathcal{N}\big ( Q(Y,X) Q(X,X)^{-1} \xi(X), Q(Y,Y) - Q(Y,X) Q(X,X)^{-1} Q(X,Y)  \big ).\\]

While the above formulas fully characterize sampling \\(\xi\\), working with them can be computationally burdensome. It is therefore of interest to consider a different viewpoint on Gaussian processes, in particular, through the Karhunen–Loève expansion. The spectral theorem implies that \\(Q\\) possesses a full set of eigenfunctions \\(Q_j \phi_j = \lambda_j \phi_j\\) for \\(j=1,2,\dots\\) with some decaying sequence of eigenvalues \\(\lambda_1 \geq \lambda_2 \geq \dots\\). The random variable \\(\xi \sim \mathcal{N}(0, Q)\\) can be written as \\[\xi = \sum_{j=1}^\infty \sqrt{\lambda_j} \chi_j \phi_j\\] where \\(\chi_j \sim \mathcal{N}(0,1)\\) is an i.i.d. sequence and the right hand side sum converges almost surely in the norm of \\(H\\) `\cite{da2014stochastic}`{=latex}. By truncating this sum to a finite number of terms, computing realizations of \\(\xi\\) becomes much more computationally manageable. This inspires the random features approach to Gaussian processes which is the basis of our computational method; for precise details, see `\cite{rasmussen2005gaussian, rahimi2007random, wilson2020efficiently}`{=latex}.

# Brownian Bridge Interpolation [app:brownian_bridge]

We show in Section <a href="#sec:white_noise" data-reference-type="ref" data-reference="sec:white_noise">2.3</a> that a white noise process is not compatible with the idea of using a generative model to interpolate deformed functions. A potential way of dealing with this issue is to treat the original realizations of the white noise \\(\xi(E_k)\\) as the fixed nodal points of a function \\(\xi\\) and obtain the rest of the values via interpolation. It is shown in `\cite{chang2023warped}`{=latex} that common forms of interpolation yield a conditional distribution \\(\xi \big ( T^{-1}(E_k) \big ) | \xi(E_k)\\) that is too dissimilar from the training distribution \\(\mathcal \mathcal{N}(0,I_k)\\) and thus the generative model produces blurry or disfigured images.

Therefore `\cite{chang2023warped}`{=latex} proposes a stochastic interpolation method which has the property that, for a new point \\(x^* \not \in E_k\\), the distribution of \\(\xi (x^*)\\) marginalized over the joint distribution \\(\big ( \xi(E_k), \xi (x^*) \big )\\) follows \\(\mathcal \mathcal{N}(0,1)\\). This is most easily seen in one spatial dimension with \\(k = 2\\) points. Suppose that \\(D = [0,1]\\) and let \\(a, b \sim \mathcal{N}(0,1)\\) be two independent random variables. Consider a Gaussian process on \\(D\\) with kernel function \\(\kappa (x,y) = 1 - |x-y|\\) and suppose that \\(\xi\\) is distributed according to this GP conditioned on \\(\xi (0) = a\\) and \\(\xi(1) = b\\). A straightforward calculation shows that, for any \\(x^* \in (0,1)\\), \\[\xi(x^*) = (1-x^*) a + x^* b + \sqrt{2 x^*(1 - x^*)}z\\] for \\(z \sim \mathcal{N}(0,1)\\) independent of \\((a,b)\\). This is simply the Brownian bridge connecting \\(a\\) to \\(b\\). Remarkably, the marginal distribution of \\(\xi (x^*)\\) over the joint \\((\xi(x^*),a,b)\\) is \\(\mathcal{N}(0,1)\\) independently of \\(x^*\\). However, the conditional distribution is \\[\xi(x^*)|a,b = \mathcal{N}\big ( (1-x^*) a + x^* b,2 x^*(1 - x^*) \big )\\] which is not \\(\mathcal{N}(0,1)\\) for all \\(x^* \in (0,1)\\). In `\cite[Section 2.2]{chang2023warped}`{=latex}, it is proposed that such Brownian bridges are used between any two pair of pixels, yielding a stochastic interpolation method given by a sequence of such independent GPs. However, from the point of view of using a generative model that is pre-trained on \\(\mathcal{N}(0,I_k)\\), it is not of interest that the marginal distribution of \\(\xi (x^*)\\) is \\(\mathcal{N}(0,1)\\) but rather that the conditional \\(\xi(x^*)|a,b\\) is \\(\mathcal{N}(0,1)\\). As we have seen, this is not the case for the method of `\cite{chang2023warped}`{=latex} and, in fact, it will only ever be the case for white noise processes as discussed in Section <a href="#sec:white_noise" data-reference-type="ref" data-reference="sec:white_noise">2.3</a>. Therefore, no matter what method is used, there will always be a distribution shift to the model input induced by the deformation \\(T^{-1}\\). A well chosen noise process will simply try to minimize this shift as much a possible.

The work `\cite{chang2023warped}`{=latex} proposes to use diffusion models trained on discrete inputs distributed according to \\(\mathcal{N}(0,I_k)\\) and computes conditional distributions \\(\xi \big ( T^{-1}(E_k) \big ) | \xi (E_k)\\) using the stochastic interpolation method described above, generalized to two dimensions. We, instead, propose to use a Gaussian process \\(\mathcal{N}(0, Q)\\), as described in Section <a href="#sec:grf" data-reference-type="ref" data-reference="sec:grf">3.1</a> and compute \\(\xi \big ( T^{-1}(E_k) \big ) | \xi (E_k)\\) by conditioning this process which amounts to simply evaluating the RFF projection. It is our numerical experience that this better preservers the qualitative properties of the input distribution for large deformations. We leave the exploration of a process best suited for this task as important future work.

# Additional Results

In this section, we provide additional results that did not fit in the main paper. We visualize the difference between independent noise and noise from our GP in Figure <a href="#fig:noise_visuals" data-reference-type="ref" data-reference="fig:noise_visuals">17</a>. We present inpainting results from our SDXL inpainting model fine-tuned with GP noise in Figure <a href="#fig:inp_visuals" data-reference-type="ref" data-reference="fig:inp_visuals">29</a>. We present super-resolution results from our SDXL super-resolution model fine-tuned with GP noise in Figure <a href="#fig:super_res_visuals" data-reference-type="ref" data-reference="fig:super_res_visuals">41</a>. We further present warping errors with respect to the previous frame in Figure <a href="#fig:noise_wp_comparisons_inpainting_part_2" data-reference-type="ref" data-reference="fig:noise_wp_comparisons_inpainting_part_2">9</a> for the inpainting results and warping errors for super-resolution for real videos in Figure <a href="#fig:noise_wp_comparisons_video" data-reference-type="ref" data-reference="fig:noise_wp_comparisons_video">14</a>. Finally, we present additional comparisons for super-resolution in Figure <a href="#fig:super_res_integer_translation_comparions" data-reference-type="ref" data-reference="fig:super_res_integer_translation_comparions">43</a>.

<figure id="fig:noise_wp_comparisons_inpainting_part_2">
<figure id="fig:inp_noise_wp_prev_latent">
<img src="./figures/self_warping_latent_error_wrt_prev_frame.png"" />
<figcaption>Warping error w.r.t. previously generated frame in latent space.</figcaption>
</figure>
<figure id="fig:inp_noise_wp_prev_pixel">
<img src="./figures/self_warping_pixel_error_wrt_prev_frame.png"" />
<figcaption>Warping error w.r.t. previously generated frame in pixel space.</figcaption>
</figure>
<figcaption>Warping errors w.r.t. previously generated frame in latent and pixel space for the inpainting task as we shift the input frame.</figcaption>
</figure>

<figure id="fig:noise_wp_comparisons_video">
<figure id="fig:video_noise_wp_first_latent">
<img src="./figures/self_warping_latent_error_wrt_first_frame.png"" />
<figcaption>Warping error w.r.t. first generated frame in latent space.</figcaption>
</figure>
<figure id="fig:video_noise_wp_first_pixel">
<img src="./figures/self_warping_pixel_error_wrt_first_frame.png"" />
<figcaption>Warping error w.r.t. first generated frame in pixel space.</figcaption>
</figure>
<figure id="fig:video_noise_wp_prev_latent">
<img src="./figures/self_warping_latent_error_wrt_prev_frame.png"" />
<figcaption>Warping error w.r.t. previously generated frame in latent space.</figcaption>
</figure>
<figure id="fig:video_noise_wp_prev_pixel">
<img src="./figures/self_warping_pixel_error_wrt_prev_frame.png"" />
<figcaption>Warping error w.r.t. previously generated frame in pixel space.</figcaption>
</figure>
<figcaption>Warping errors w.r.t. first generated frame (top-row) and prev. generated frame (bottom row) for the <span class="math inline">8×</span> super-resolution task for real videos.</figcaption>
</figure>

<figure id="fig:noise_visuals">
<figure id="fig:indep_noise">
<img src="./figures/noise_indep.png"" style="width:80.0%" />
<figcaption>Independent noise realization.</figcaption>
</figure>
<figure id="fig:gp_noise">
<img src="./figures/noise_gp.png"" style="width:80.0%" />
<figcaption>Gaussian Process noise realization.</figcaption>
</figure>
<figcaption>Visualization of independent noise and noise from a Gaussian Process.</figcaption>
</figure>

<figure id="fig:inp_visual_output_3">
<figure id="fig:inp_visual_input_1">
<p><img src="./figures/13_input_0.png"" alt="image" /> <span id="fig:inp_visual_input_1" data-label="fig:inp_visual_input_1"></span></p>
</figure>
<figure id="fig:inp_visual_output_1">
<p><img src="./figures/13_output_0.png"" alt="image" /> <span id="fig:inp_visual_output_1" data-label="fig:inp_visual_output_1"></span></p>
</figure>
<figure id="fig:inp_visual_input_2">
<p><img src="./figures/53_input_0.png"" alt="image" /> <span id="fig:inp_visual_input_2" data-label="fig:inp_visual_input_2"></span></p>
</figure>
<figure id="fig:inp_visual_output_2">
<p><img src="./figures/53_output_0.png"" alt="image" /> <span id="fig:inp_visual_output_2" data-label="fig:inp_visual_output_2"></span></p>
</figure>
<figure id="fig:inp_visual_input_3">
<p><img src="./figures/55_input_0.png"" alt="image" /> <span id="fig:inp_visual_input_3" data-label="fig:inp_visual_input_3"></span></p>
</figure>
<figure id="fig:inp_visual_output_3">
<p><img src="./figures/55_output_0.png"" alt="image" /> <span id="fig:inp_visual_output_3" data-label="fig:inp_visual_output_3"></span></p>
</figure>
</figure>

<figure id="fig:inp_visuals">
<figure id="fig:inp_visual_input_4">
<p><img src="./figures/17_input_0.png"" alt="image" /> <span id="fig:inp_visual_input_4" data-label="fig:inp_visual_input_4"></span></p>
</figure>
<figure id="fig:inp_visual_output_4">
<p><img src="./figures/17_output_0.png"" alt="image" /> <span id="fig:inp_visual_output_4" data-label="fig:inp_visual_output_4"></span></p>
</figure>
<figure id="fig:inp_visual_input_5">
<p><img src="./figures/45_input_0.png"" alt="image" /> <span id="fig:inp_visual_input_5" data-label="fig:inp_visual_input_5"></span></p>
</figure>
<figure id="fig:inp_visual_output_5">
<p><img src="./figures/45_output_0.png"" alt="image" /> <span id="fig:inp_visual_output_5" data-label="fig:inp_visual_output_5"></span></p>
</figure>
<figcaption>Inpainting examples. Left column: inputs by randomly masking images from the COYO dataset. Right column: inpainting outputs from our SDXL fine-tuned model with correlated noise.</figcaption>
</figure>

<figure id="fig:super_res_visual_output_3">
<figure id="fig:super_res_visual_input_1">
<p><img src="./figures/32_input_0.png"" alt="image" /> <span id="fig:super_res_visual_input_1" data-label="fig:super_res_visual_input_1"></span></p>
</figure>
<figure id="fig:super_res_visual_output_1">
<p><img src="./figures/32_output_0.png"" alt="image" /> <span id="fig:super_res_visual_output_1" data-label="fig:super_res_visual_output_1"></span></p>
</figure>
<figure id="fig:super_res_visual_input_2">
<p><img src="./figures/33_input_0.png"" alt="image" /> <span id="fig:super_res_visual_input_2" data-label="fig:super_res_visual_input_2"></span></p>
</figure>
<figure id="fig:super_res_visual_output_2">
<p><img src="./figures/33_output_0.png"" alt="image" /> <span id="fig:super_res_visual_output_2" data-label="fig:super_res_visual_output_2"></span></p>
</figure>
<figure id="fig:super_res_visual_input_3">
<p><img src="./figures/45_input_0.png"" alt="image" /> <span id="fig:super_res_visual_input_3" data-label="fig:super_res_visual_input_3"></span></p>
</figure>
<figure id="fig:super_res_visual_output_3">
<p><img src="./figures/45_output_0.png"" alt="image" /> <span id="fig:super_res_visual_output_3" data-label="fig:super_res_visual_output_3"></span></p>
</figure>
</figure>

<figure id="fig:super_res_visuals">
<figure id="fig:super_res_visual_input_4">
<p><img src="./figures/46_input_0.png"" alt="image" /> <span id="fig:super_res_visual_input_4" data-label="fig:super_res_visual_input_4"></span></p>
</figure>
<figure id="fig:super_res_visual_output_4">
<p><img src="./figures/46_output_0.png"" alt="image" /> <span id="fig:super_res_visual_output_4" data-label="fig:super_res_visual_output_4"></span></p>
</figure>
<figure id="fig:super_res_visual_input_5">
<p><img src="./figures/7_input_0.png"" alt="image" /> <span id="fig:super_res_visual_input_5" data-label="fig:super_res_visual_input_5"></span></p>
</figure>
<figure id="fig:super_res_visual_output_5">
<p><img src="./figures/7_output_0.png"" alt="image" /> <span id="fig:super_res_visual_output_5" data-label="fig:super_res_visual_output_5"></span></p>
</figure>
<figcaption>Super-resolution examples. Left column: downsampled inputs from the COYO dataset. Right column: super-resolution outputs from our SDXL fine-tuned model with correlated noise.</figcaption>
</figure>

<figure id="fig:fig3">
<img src="./figures/fig3.png"" style="width:80.0%" />
<figcaption>Schematic visualization of Equivariance Self Guidance (see Algorithm <a href="#alg:alg1" data-reference-type="ref" data-reference="alg:alg1">[alg:alg1]</a>).</figcaption>
</figure>

<figure id="fig:super_res_integer_translation_comparions">
<figure>
<img src="./figures/self_warping_pixel_error_wrt_prev_frame.png"" />
<figcaption>Warping error w.r.t. previously generated frame in pixel space.</figcaption>
</figure>
<figure>
<img src="./figures/self_warping_pixel_error_wrt_first_frame.png"" />
<figcaption>Warping error w.r.t. first generated frame in pixel space.</figcaption>
</figure>
<figcaption>Warping errors in pixel space for the super-resolution task as we shift the input frame.</figcaption>
</figure>

# Related Works [sec:related_work]

Our work is primarily related to three recent lines of research about the utility of diffusion models in inverse problems, video editing, and equivariance in function space diffusion models as elaborated below.

**Diffusion Models for Inverse Problems.** Diffusion models have been recently received widespread adoption for solving inverse problems in various domains. Diffusion models can solve inverse problems in a few different ways. A simple way is to train or finetune a *conditional* diffusion model for each specific task to learn the conditional distribution from the degraded data distribution to the clean data distribution `\citep{saharia2022palette,liu2023i2sb,shi2022conditional}`{=latex}. Some popular examples include SR3 `\cite{saharia2022image}`{=latex} and inpainting stable diffusion `\cite{rombach2022high}`{=latex}. We leverage stable diffusion inpainting in the present work. While successful, they however need to be trained (or finetuned) separately for each individual task that is computationally complex. Also, they are not robust to out of distribution data. To mitigate these challenges, *plug-and-play* methods have been introduced that utilize a single foundation diffusion model (e.g., stable diffusion) as a (rich) prior to solve many inverse problems at once `\citep{jalal2021robust, rout2024solving, chung2022diffusion, kawar2022denoising}`{=latex}. The crux of this approach is to modify the sampling post-hoc by either: \\((i)\\) add guidance to the score function of diffusion models as in `\cite{chung2022diffusion, song2023PiGDM}`{=latex}; \\((ii)\\) approximated projection onto the measurement subspace at each diffusion step `\cite{chung2022improving,kawar2022denoising}`{=latex} or, \\((iii)\\) use regularization by denoising via optimization `\cite{mardani2023variational, graikos2022diffusion}`{=latex}. In this work we adopt the guidance-based approach to impose equivariance for the score function. All these methods have been applied for 2D images. For *video* inverse problems, the problem is more challenging due to temporal consistency. There are some efforts to leverage diffusion models for example for text-to-video superresolution or inpainting; see e.g., `\cite{rota2023enhancing,girish2023survey, zhou2023flow}`{=latex}. However, there is no systematic framework yet based on 2D diffusion models to solve generic video inverse problems in a temporally consistent manner. This is essentially the focus of our work. Finally, we remark that recent work `\citep{daras2024consistent, daras2023solving, daras2024ambient, aali2024ambient, kawar2023gsure, aali2023solving}`{=latex} has shown that it is even possible to train diffusion models to solve inverse problems without ever seeing clean images from the distribution of interest.

**Video Editing with Image Diffusion Models.** Due to the lack of full-fledged pre-trained text-to-video diffusion models, many works focus on video editing (or video-to-video translation) using text-to-image diffusion models. One line of research has proposed to fine-tune the image diffusion model on a single text-video pair and generate novel videos that represent the edits at inference `\cite{wu2023tune,liu2023video,zhang2024towards}`{=latex}. Specifically, Tune-A-Video `\cite{wu2023tune}`{=latex} proposed a cross-frame attention mechanism and an efficient one-shot tuning strategy. Video-P2P `\cite{liu2023video}`{=latex} further improved the video inversion performance by optimizing a shared unconditional embedding for all frames. EI\\(^2\\) `\cite{zhang2024towards}`{=latex} refined the temporal modules to resolve semantic disparity and temporal inconsistency of video editing. However, the fine-tuning process over the input video makes the editing less efficient. Another line of research has developed various training-free methods for efficient video editing, which mostly rely on the cross-frame attention and latent fusion for maintaining temporal consistency `\cite{ceylan2023pix2video,khachatryan2023text2video,qi2023fatezero,yang2023rerender}`{=latex}. In particular, Text2Video-Zero `\cite{khachatryan2023text2video}`{=latex} encoded the motion dynamics in latent noises through a noise wrapping. FateZero `\cite{qi2023fatezero}`{=latex} fused the attention features with a blending mask obtained by the source prompt’s cross-attention map. Pix2Video `\cite{ceylan2023pix2video}`{=latex} proposed to progressively propagate the changes to the future frames via self-attention feature injection. Rerender-A-Video `\cite{yang2023rerender}`{=latex} proposed hierarchical cross-frame constraints with the optical flow for improved temporal consistency.

**Function Space Diffusion Models and Equivariance** Recently, several works `\cite{lim2023score, kerrigan2022diffusion, kerrigan2023functional}`{=latex} have extended diffusion models to function data. However, these methods primarily focus on theoretical developments and have been examined on simplistic datasets such as time series, Navier-Stokes solutions, or hand-written digits. This paper can be considered one of the first successful applications of function-space diffusion models to natural image datasets. Our work is also related to the equivariant diffusion models which have been extensively explored in scientific applications such as molecule and protein interaction and generation applications `\cite{hoogeboom2022equivariant, anand2022protein, xu2022geodiff, corso2022diffdock, jing2022torsional}`{=latex}. However, equivariant diffusion models for image generation are less explored, primarily because guaranteeing equivariance (for example with respect to translation, rotation, or rescaling) in commonly used diffusion architectures such as U-Net `\cite{ronneberger2015unet, ho2020ddpm}`{=latex} or Transformer `\cite{peebles2022scalable, hatamizadeh2023diffit}`{=latex} models is challenging.

**Diffusion models trained with correlated noise.** Ours is not the first work to train diffusion models with a prior other than white noise. The authors of `\citep{daras2022soft, hoogeboom2022blurring}`{=latex} show how to train diffusion models with blurring corruption, leading to a blurred terminal distribution. Several other works have shown how to generalize diffusion models to find mappings between arbitrary input-output distributions, including `\citep{bansal2022cold, bortoli2021diffusion, delbracio2023inversion}`{=latex}. One new finding in our work is that it is possible to start with a state-of-the-art model trained with white noise and fine-tune it easily to handle correlated noise. This allows us to convert vanilla diffusion models to Function Space Diffusion models by training them with noise sampled from Gaussian Processes. For more details, we refer the reader to Section <a href="#sec:grf" data-reference-type="ref" data-reference="sec:grf">3.1</a>.

# Experimental Details [app:exp_details]

## Dealing with Optical Flows [sec:optical_flows]

We use the RAFT model to predict the optical flows `\citep{teed2020raft}`{=latex}. The optical flows can be computed with respect to the first frame or between subsequent frames. We find that the optical flow estimation is much better between subsequent frames and we use subsequent transformations to find the position in the original frame, whenever possible.

Since we are working with Latent Diffusion Models, all the warping happens in a lower-dimensional space. Fortunately, as observed in numerous prior works, including `\citep{sdxl_inp}`{=latex}, there is a geometric correspondence between pixel blocks and latent locations, i.e. pixel blocks are mapped to specific locations in latent space. This allows us to extract the flows from the input frames and convert them to optical flows for our latent vectors. Alternatively, one nat first map to latent space and then compute the optical flow there. We did not pursue this approach since we rely on a deep learning method for the flow-estimation and the underlying model has been trained on natural images.

## Stable Diffusion XL Finetuning [sec:training_details]

To fine-tune SDXL in conditional tasks, we use the reference implementation found in the following link: <https://github.com/huggingface/diffusers/pull/6592>. The reference implementation finetunes SDXL on the inpainting task, however, it is straightforward to adapt it to other conditional tasks, such as super-resolution. As mentioned in the paper, we train all our models for \\(100,000\\) steps. We use the following training hyperparameters:

- Training resolution: \\(1024\times 1024\\).

- Batch size: \\(64\\).

- Latent resolution: \\(128\\).

- Optimizer Adam with Weight Decay. Optimizer parameters:

  - Learning rate: \\(5e-6\\)

  - \\(\beta_1=0.9\\)

  - \\(\beta_2=0.999\\)

  - Weight Decay: \\(1e-2\\)

  - \\(\epsilon=1e-08\\)

  - Max Gradient Norm (Gradient clipping): \\(1.0\\)

- Gaussian Process parameters:

  1.  Truncation parameter: \\(2.0\\)

  2.  Number of random features: \\(3000\\)

  3.  Length scale: \\(0.004977\\).

The parameter length scale controls the amount of correlation in the noise from the GP. Recall that RFFs are generated by sampling \\(z_j \sim N(0,\epsilon^{-2} I_2)\\). To avoid aliasing artifacts when generating GP, we truncated the Normal distribution at \\(2 \epsilon^{-1}\\) (i.e., 2\\(\times\\) its standard deviation) and we made sure that \\(2 \epsilon^{-1}\\) is lower than the Nyquist–Shannon sampling frequency, i.e., \\(\frac{2 \epsilon^{-1}}{2\pi} \leq \frac{\mathrm{resolution}}{2}\\). Given this, as a general rule of thumb, we found that setting the length scale to be \\(\epsilon := \frac{2}{\pi \cdot \mathrm{resolution}}\\) leads to noise realizations that can be used to easily fine-tune Stable Diffusion XL.

We train all our models on \\(16\\) A100 GPUs on a SLURM-based cluster. The fine-tuning of the SDXL model on conditional tasks (super-resolution, inpainting) with correlated noise for \\(100\\)k steps takes roughly \\(24\\) hours.

## Sampling Speed [sec:speed]

Sampling guidance for equivariance increases the generation time for two reasons: i) we need to run more steps in order to make it effective and, ii) each step is more expensive since we need to perform an additional backpropagation. For our experiments, we use 50 steps instead of \\(25\\) steps that we use for unconditional sampling. Further, without guidance, we get \\(4.32\\) iterations per second on a single A100 GPU while with guidance we obtain \\(1.62\\) iterations per second.

The other hyperparameter used in sampling is the guidance strength, see Algorithm <a href="#alg:alg1" data-reference-type="ref" data-reference="alg:alg1">[alg:alg1]</a>. For \\(\lambda=0\\), there is no guidance and the method just becomes GP Noise Warping. For higher \\(\lambda\\) the gradient from the warping guidance becomes stronger. In our experiments, we found the value \\(\lambda=1\\) to perform the best. This is consistent with the choice of \\(\lambda\\) in the Diffusion Posterior Sampling `\citep{chung2022diffusion}`{=latex} paper which uses a guidance term to apply diffusion models for general inverse problems.

We perform all our sampling experiments on a single A-100 GPU. Without sampling guidance, it takes roughly \\(20\\) seconds to generate a single frame. We measure the performance of our method and the baselines on 2 second videos consisting of \\(16\\) frames.

# Broader Impact [sec:broader_impact]

Our method allows the use of image diffusion models to solve video inverse problems. There are both positive and negative societal implications of such a method. On the positive side, our method does not require training of video models which is typically expensive and contributes to increasing the AI carbon footprint. Further, democratizes access to video editing tools. The average practitioner can now leverage state-of-the-art image models to solve video inverse problems. To illustrate the effectiveness of our method, we trained powerful text-conditioned inpainting models that work on arbitrary images from the web. On the negative side, these models can be used for adversarial image and video editing. Further, our method can be used for the generation of deepfakes.

[^1]: The work was done during an internship at NVIDIA.
