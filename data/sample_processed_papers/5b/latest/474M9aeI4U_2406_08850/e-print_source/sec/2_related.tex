
\section{Related Works}

\subsection{Diffusion-based Image and Video Generation.} 

Diffusion Models \cite{ho2020denoising, sohl2015deep,song2020score} have recently showcased impressive results in image generation, which generates the image through gradual denoising from the standard Gaussian noise\cite{croitoru2023diffusion,dhariwal2021diffusion,nichol2021improved,guo2023zero, rombach2022high,song2020denoising,guo2023smooth}. A large number of efforts on diffusion models \cite{ho2022classifier,karras2022elucidating,salimans2022progressive} has enabled it to be applied to numerous scenarios \cite{avrahami2022blended,gal2022image,kawar2022denoising,li2022srdiff,lugmayr2022repaint,meng2021sdedit,mokady2023null,fang2024real,ruiz2023dreambooth,chen2024follow,guo2024everything, ma2024followyourclick,lin2023consistent123, guo2024refir,he2023reti,he2024diffusion}. With the aid of large-scale pretraining \cite{radford2021learning,schuhmann2022laion}, text-to-image diffusion models exhibit remarkable progress in generating diverse and high-quality images \cite{nichol2021glide, xue2024followyourposev2, ramesh2022hierarchical,rombach2022high,saharia2022photorealistic,guo2022assessing, ma2024followyouremoji, guo2024refir}. ControlNet \cite{zhang2023adding} enables users to provide structure or layout information for precise generation. Naturally, diffusion models have found application in video synthesis, often by integrating temporal layers into image-based DMs \cite{blattmann2023align,ho2022imagen,ho2022video,xiang2023versvideo,chen2023seine}. Despite successes in unconditional video generation \cite{ho2022video,yu2023video, ma2023magicstick}, text-to-video diffusion models lag behind their image counterparts. %The intricate nature of temporal motion necessitates substantial computational resources and extensively annotated video datasets for training video diffusion models \cite{wang2023microcinema,chen2023gentron,lu2023vdt}, impeding progress in this domain.




\subsection{Text-to-Video Editing.}

There are increasing works adopting the pre-trained text-to-image diffusion model to the video editing task \cite{liu2023video,wang2023zero,wu2023tune,ma2024follow,guo2023faceclip}, where keeping the temporal consistency in the generated video is the most challenging. Recently, a large number of works focusing on zero-shot video editing has been proposed. FateZero \cite{qi2023fatezero} proposes to use attention blending to achieve high-quality edited videos while struggling to edit long videos. TokenFlow \cite{geyer2023tokenflow} reduces the effects of flickering through the linear combinations between diffusion features, while the smoothing strategy can cause blurring in the generated video. RAVE \cite{kara2023rave} proposes the randomized noise shuffling method, suffering the problem of fine details flickering. There are also a large number of methods that enhance the temporal consistency with the aid of pre-trained optical-flow models \cite{yang2024fresco,yang2023rerender,cong2023flatten,hu2023videocontrolnet}. Although the effectiveness of them, all of them severely rely on a pre-trained optical-flow model. Recent works \cite{tang2023emergent} illustrate that the diffusion feature contains rich correspondence information. Although VideoSwap \cite{gu2023videoswap} adopts this characteristic by tracking the key points across frames, it still needs users to provide the key points as the extra addition manually. 