\section{Introduction}

\label{sec:intro}

Diffusion models \cite{ho2020denoising, sohl2015deep,song2020score} have shown exceptional performance in image generation \cite{rombach2022high}, thereby inspiring their application in the field of image editing \cite{brooks2023instructpix2pix,hertz2022prompt,cao2023masactrl,parmar2023zero,tumanyan2023plug,guo2023focus}. These approaches typically leverage a pre-trained Text-to-Image (T2I) stable diffusion model \cite{rombach2022high}, using DDIM \cite{song2020denoising} inversion to transform source images into noise, which is then progressively denoised under the guidance of a prompt to generate the edited image. 

Despite satisfactory performance in image editing, achieving high-quality video editing remains challenging. Specifically, unlike the well-established open-source T2I stable diffusion models \cite{rombach2022high}, comparable T2V diffusion models are not as mature due to the difficulty of modeling complicated temporal motions, and training a T2V model from scratch demands substantial computational resources \cite{ho2022imagen,ho2022video,singer2022make}. Consequently, there is a growing focus on adapting the pre-trained T2I diffusion for video editing \cite{geyer2023tokenflow,kara2023rave,cong2023flatten,yang2023rerender,yang2024fresco,qi2023fatezero}. In this case, maintaining temporal consistency in edited videos is one of the biggest challenges, which requires the generated frames to be stylistically coherent and exhibit smooth temporal transitions, rather than appearing as a series of independent images. Numerous methods have been working on this topic while still facing various limitations, such as the inability to ensure fine-grained temporal consistency (leading to flickering \cite{kara2023rave,qi2023fatezero} or blurring \cite{geyer2023tokenflow} in generated videos), requiring additional components \cite{hu2023videocontrolnet,yang2023rerender,cong2023flatten,yang2024fresco} or needing extra training or optimization \cite{yang2024fresco,wu2023tune,liew2023magicedit}, etc.

\begin{wrapfigure}{r}{0.5\textwidth}
  \centering
  \vspace{-0.4cm}
  \includegraphics[width=0.5\textwidth]{fig/fig_compare_intro_zong.pdf} 
  % \vspace{-0.15cm}
  \caption{\textbf{Comparison between COVE (our method) and previous methods}\cite{cong2023flatten,yang2024fresco}.}
  \label{fig:compare_intro}

\end{wrapfigure}

In this work, our goal is to achieve highly consistent video editing by leveraging the intra-frame correspondence relationship among tokens, which is intuitively closely related to the temporal consistency of videos: If corresponding tokens across frames exhibit high similarity, the resulting video will thus demonstrate high temporal consistency. Taking a video of a man as an example, if the token representing his nose has high similarity across frames, his nose will be unlikely to deform or flicker throughout the video.
However, how to obtain accurate correspondence information among tokens is still largely under-explored in existing works, although the intrinsic characteristic of the video editing task (i.e., the source video and edited video are expected to share similar motion and semantic layout) determines that it naturally exists in the source video. Some previous methods \cite{cong2023flatten, yang2024fresco} leverage a pre-trained optical-flow model to obtain the flowing trajectory of each token across frames, which can be seen as a kind of coarse correspondence information. Despite the self-attention among tokens in the same trajectory can enhance the temporal consistency of the edited video, it still encounters two primary limitations: Firstly, these methods heavily rely on a highly accurate pre-trained optical-flow model to obtain the correspondence relationship of tokens, which is not available in many scenarios \cite{jonschkowski2020matters}. Secondly, supposing we have access to an extremely accurate optical-flow model, it is still only able to obtain the coarse one-to-one correspondence among tokens in different frames (\Cref{fig:compare_intro}a), which would lead to the loss of information because one token is highly likely to correspond to multiple tokens in other frames in most cases (\Cref{fig:compare_intro}b).

Addressing these problems, we notice that the inherent diffusion features naturally contain precise correspondence information. For instance, it is easy to find the corresponding points between two images by extracting their diffusion features and calculating the cosine similarity between tokens \cite{tang2023emergent}. However, until now none of the existing works have successfully utilized this characteristic in more complicated and challenging tasks such as video editing. In this paper, we propose COVE, which is the first work unleashing the potential of inherent diffusion feature correspondence to significantly enhance the quality and temporal consistency in video editing. 
Given a source video, we first extract the diffusion feature of each frame. Then for each token in the diffusion feature, we obtain its corresponding tokens in other frames based on their similarity. Within this process, we propose a sliding-window-based approach to ensure computational efficiency. In our sliding-window-based method, for each token, it is only required to calculate the similarity between it and the tokens in the next frame located within a small window, identifying the tokens with the top $K$ ($K>1$) highest similarity. After the correspondence calculation process, for each token, the coordinates of its $K$ corresponding tokens in each other frame can be obtained. During the inversion and denoising process, we sample the tokens in noisy latents based on the obtained coordinates. To reduce the redundancy and accelerate the editing process, token merging is applied in the temporal dimension, which is followed by self-attention. Our method can be seamlessly integrated into the off-the-shelf T2I diffusion model without extra training or optimization. Extensive experiments demonstrate that COVE significantly improves both the quality and the temporal consistency of generated videos, outperforming a wide range of existing methods and achieving state-of-the-art results.
