
\section{Conclusion}

In this paper, we propose COVE, which is the first to explore how to employ inherent diffusion feature correspondence in video editing to enhance editing quality and temporal consistency. 
Through the proposed efficient sliding-window-based strategy, the one-to-many correspondence relationship among tokens across frames is obtained. During the inversion and denoising process, self-attention is performed within the corresponding tokens to enhance temporal consistency. Additionally, we also apply token merging in the temporal dimension to improve the efficiency of the editing process. Both quantitative and qualitative experimental results demonstrate the effectiveness of our method, which outperforms a wide range of previous methods, achieving state-of-the-art editing quality. 

\textbf{Limitaions.} The limitation of our method is discussed in \Cref{apdx:lim}.

\begin{ack}
This work was supported by the STI 2030-Major Projects under Grant 2021ZD0201404.
\end{ack}
% \textbf{Limitaions.} There is still room for improvement in implementing our methods, especially combining with Xformer \cite{xFormers2022}, which is expected to further speed up our methods and reduce memory usage. 