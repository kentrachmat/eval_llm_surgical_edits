# CNCA: Toward Customizable and Natural Generation of Adversarial Camouflage for Vehicle Detectors

## Abstract

Prior works on physical adversarial camouflage against vehicle detectors mainly focus on the effectiveness and robustness of the attack. The current most successful methods optimize 3D vehicle texture at a pixel level. However, this results in conspicuous and attention-grabbing patterns in the generated camouflage, which humans can easily identify. To address this issue, we propose a Customizable and Natural Camouflage Attack (CNCA) method by leveraging an off-the-shelf pre-trained diffusion model. By sampling the optimal texture image from the diffusion model with a user-specific text prompt, our method can generate natural and customizable adversarial camouflage while maintaining high attack performance. With extensive experiments on the digital and physical worlds and user studies, the results demonstrate that our proposed method can generate significantly more natural-looking camouflage than the state-of-the-art baselines while achieving competitive attack performance. Our code is available at <https://anonymous.4open.science/r/CNCA-1D54>.

# Introduction

Over the past years, Deep Neural Networks (DNNs) have revolutionized a wide range of research domains, especially in computer vision tasks, such as image classification, object detection, and semantic segmentation. DNNs are widely used in real-world systems, such as face recognition and autonomous driving. Despite their impressive success, DNNs are found vulnerable to adversarial examples `\cite{szegedy2013intriguing}`{=latex}, which are carefully crafted to deceive DNNs.

Generally, adversarial attacks can be classified into two categories: digital attacks, which primarily add small pixel-level perturbations to the input images; physical attacks, which manipulate the object’s physical properties, such as its shape, surface, or surroundings, to deceive the target model in the real world. Physical attacks are more challenging than digital attacks, as they must remain effective under various complex physical conditions, including different viewing angles, distances, and lighting conditions. This paper focuses on physical attacks against vehicle detection models since they play critical roles in real-world applications like surveillance and autonomous driving systems.

<figure id="introduction">
<div class="center">
<img src="./figures/Intro.png"" />
</div>
<figcaption> Customized and natural adversarial camouflage with various styles. (a) A car with normal texture; (b)(c)(d)(e) are the different styles of camouflage generated by our method CNCA. Their captions are user-specified input prompts. </figcaption>
</figure>

Prior advanced physical attacks against vehicle detectors use adversarial camouflage technique `\cite{wang2021dual,wang2022fca, Suryanto_2022_CVPR, Suryanto_2023_ICCV}`{=latex}. This technique fully covers the whole vehicle’s surface with adversarial texture, which leads to better attack performance regardless of the viewing positions. Leveraging a differentiable renderer can effectively optimize 3D vehicle texture to deceive vehicle detectors via gradient back-propagation. However, all these methods suffer certain issues. Firstly, there is no prior knowledge of naturalness to guide the camouflage generation, resulting in conspicuous and attention-grabbing camouflage patterns. Secondly, all the current methods optimize the adversarial camouflage at a pixel level, making it challenging to resemble natural-looking patterns. Last but not least, none of the methods can customize the appearance of camouflage, making it hard to adapt to specific environments like forests and deserts.

To address the above issues, we propose CNCA, a novel framework to generate customizable and natural adversarial camouflage against vehicle detectors as shown in Figure <a href="#introduction" data-reference-type="ref" data-reference="introduction">1</a>. Our insight is that: to gain naturalness and customizability, we need to leverage models that are equipped with prior knowledge of naturalness and allow conditional input signals. Motivated by this insight, we leverage an off-the-shelf pre-trained diffusion model to generate adversarial texture images with user-specific text prompts. The challenge of this approach is how to guide the adversarial gradient from the detection model to the image generation process. We introduce an adversarial feature to combine with the original text prompt feature. The combined feature forms the conditional input of the diffusion model. Thus, the resulting image is both natural and adversarial. Furthermore, we apply a clip strategy to the adversarial features to balance the trade-off between naturalness and attack performance. The combination of diffusion models, adversarial features, and clipping strategies facilitates the generation of customizable and natural camouflage.

The main contributions of our work are summarized as follows:

- To the best of our knowledge, our work is the first to investigate natural physical adversarial camouflage generation with diffusion models. It is also the first that can generate various styles of adversarial camouflage against vehicle detectors.

- We introduce an adversarial feature that can be combined with the conditional input of the diffusion models, enabling gradient-based adversarial camouflage generation.

- We propose to apply a clipping strategy for the adversarial feature to balance the trade-off between naturalness and attack performance.

We conduct a comprehensive evaluation with popular vehicle detectors and datasets in both digital and physical settings, and the results show that our method is effective in generating natural and customized adversarial camouflage.

# Related Work

**Adversarial Camouflage.** Accurate detection of nearby vehicles is a crucial safety requirement of self-driving cars. Therefore, there has been a growing interest in crafting adversarial camouflage to attack vehicle detection systems. Most current research uses a 3D simulation environment `\cite{dosovitskiy2017carla}`{=latex} to generate 2D rendered vehicle images with various transformations to develop robust adversarial camouflage. Early works of adversarial camouflage against vehicle detection are mostly black-box because the rendering process of the traditional rendering method is non-differentiable. The first work of vehicle adversarial camouflage `\cite{zhang2019camou}`{=latex} CAMOU trains a neural network to mimic the behavior of both the rendering and detection of the camouflage vehicles. Then, they can use this network to optimize the adversarial texture. `\cite{wu2020physical}`{=latex} propose to use a genetic algorithm to search the optimal parameters for the synthesis of the adversarial texture pattern. Then, they enlarge and repeat the pattern to cover the whole surface of the vehicle.

Recent advanced methods introduce neural rendering to enable direct optimization of adversarial texture via gradient back-propagation algorithms. Dual Attention Suppression attack (DAS) `\cite{wang2021dual}`{=latex} suppresses model and human attention on the camouflaged vehicle. However, it suffers a limited attack success rate because the adversarial pattern only covers part of the vehicle surface. Then, Full-Coverage Attack (FCA) `\cite{wang2022fca}`{=latex} optimizes the entire surface of the vehicle in multi-view settings. Furthermore, Differentiable Transformer Attack (DTA) `\cite{Suryanto_2022_CVPR}`{=latex} proposes a differentiable renderer that can express complex environment characteristics like shadow and fog on the vehicle surface. ACTIVE `\cite{Suryanto_2023_ICCV}`{=latex} introduces a new texture mapping that incorporates depth images and tries to improve the naturalness of the camouflage by using larger texture resolutions and applying a smooth loss. Despite prior works achieving impressive attack performance, these methods optimize the camouflage patterns at the pixel level without prior knowledge of naturalness. Consequently, the generated camouflage is conspicuous and attention-grabbing for human observers.

**Diffusion Models.** Diffusion Models (DMs) `\cite{ho2020denoising}`{=latex} are widely used to generate natural images of higher quality and diversity. Since billions of image-text dataset pairs `\cite{Schuhmann2022NeurIPS}`{=latex} are used to train these models, DMs provide a strong prior knowledge of natural and realistic images and their corresponding text captions. As a result, DMs can produce highly realistic and varied images across different user-specific prompts.

# Methods

<figure id="pipeline">
<img src="./figures/pipeline-v7.2.png"" />
<figcaption>CNCA framework for generating customizable and natural adversarial camouflage. </figcaption>
</figure>

In this section, we present an overview of our framework for generating customizable and natural adversarial camouflage while maintaining comparable attack performance. Subsequently, we provide a detailed explanation of the essential components of our framework.

## Overview

Figure <a href="#pipeline" data-reference-type="ref" data-reference="pipeline">2</a> illustrates our whole framework for adversarial camouflage generation. First, we obtain a vehicle image dataset from the Carla simulation environment. The dataset includes the original input images \\(I_{in}\\), ground truth labels \\(Y\\), camera pose parameters \\(\Phi_{cam}\\) (position and angle), and vehicle mask \\(M\\). With \\(I_{in}\\) and \\(M\\), we can obtain the background images \\(B\\) and foreground vehicle reference images \\(X_{ref}\\): \\[\begin{gathered}
B=I_{i n} \cdot \left (1-{M}\right)\label{1}\\
X_{r e f}=I_{in}\cdot {M}\label{2}
\end{gathered}\\] Then, we use a neural renderer \\(NR\\) to obtain rendered vehicle images \\(X_{nr}\\) with 3D mesh \\(Msh\\), UV texture image obtained from UV map mask and adversarial texture image \\(T_{adv}\\), and camera parameters \\(\Phi_{cam}\\) of the vehicle. Next, \\(X_{ref}\\) and \\(X_{nr}\\) forward into a neural network called Environment Feature Renderer (EFR), which extracts the environmental features from \\(X_{ref}\\) and render these features into \\(X_{nr}\\) to obtain \\(X_{ren}\\). We then add \\(X_{ren}\\) with background \\(B\\) to obtain the realistic camouflaged vehicle images \\(I_{out}\\). Then, we input \\(I_{out}\\) into the target object detector to obtain the detection results \\(R\\).

Since we introduce a pre-trained text-to-image (T2I) diffusion model to generate the UV-map texture images, we can provide text prompt \\(P_{txt}\\) to customize the texture image. The text encoder processes the text prompt to obtain the text feature \\(F_{txt}\\). Then, the text feature is combined with the adversarial feature \\(F_{adv}\\), which will be optimized during the camouflage generation. We apply a clip function \\(\kappa(\cdot)\\) to \\(F_{adv}\\) during optimization to balance naturalness and attack performance. Then, the combined features are fed into the pre-trained T2I model, which outputs the adversarial texture images \\(T_{adv}\\). \\[\begin{gathered}
F_{txt} = enc(P_{txt})\label{7}\\
T_{adv} = T2I([\kappa(F_{adv}), F_{txt}])\label{8}
\end{gathered}\\] In the end, we can obtain the final adversarial camouflage by minimizing the below adversarial loss function from the target detector: \\[\begin{gathered}
D_s (x)=\operatorname{IoU}\left (D_b\left (x\right), g t\right) \cdot D_c\left (x\right) \cdot D_o\left (x\right)\notag \\
L_{adv}\left (x\right)=-\log \left (1-\max \left (D_s (x)\right)\right),\label{11}
\end{gathered}\\] where \\(x\\) is the input image for the target detector, \\(D_b(x)\\) is the detection bounding box, \\(gt\\) is the ground-truth bounding box. We calculate the Intersection over Union (IoU) between \\(D_b(x)\\) and \\(gt\\). This IoU score allows the optimization to focus on the bounding box with larger intersections with ground truth. \\(D_{o} (x)\\) and \\(D_{c} (x)\\) are the objectiveness score and the class confidence score for the bounding box, respectively. We obtain our detection score \\(D_{s} (x)\\) by multiplying the IoU score, objectiveness score, and class confidence score. We select the highest \\(D_{s} (x)\\) to compute \\(L_{adv} (x)\\) using a log loss. By minimizing \\(L_{adv} (x)\\), we encourage the camouflaged vehicle to be undetected or misclassified by the detector.

## Realistic Neural Rendering

The prior works from DTA and ACTIVE`\cite{Suryanto_2022_CVPR, Suryanto_2023_ICCV}`{=latex} prove that realistic rendering of the vehicle camouflage is one of the keys to successful physical adversarial attack. To achieve this, we use two rendering components: the first render component is a differentiable neural renderer, which takes the 3D properties of the vehicle to output the vehicle’s foreground images. However, it struggles to render complex environmental characteristics on the vehicle surface. To alleviate this, we use an environmental feature renderer that can combine the environmental characteristics and neural renderer output to produce realistic and accurate camouflaged vehicle images.

We select Pytorch3D as differential renderer `\citep{ravi2020pytorch3d}`{=latex} to generate camouflaged vehicle images because it supports differentiable path to the UV-map texture image. Following the method proposed in DTA `\cite{Suryanto_2022_CVPR}`{=latex} and ACTIVE `\cite{Suryanto_2023_ICCV}`{=latex}, we use a U-Net network for EFR to extract environmental characteristics from \\(X_{ref}\\) and combine them with \\(X_{nr}\\). EFR outputs the camouflaged vehicle with environmental characteristics \\(X_{ren}\\).

Before camouflage generation, we need to train EFR for its optimal performance. The training of EFR needs masked vehicle images \\(X_{ref}\\), 3D mesh \\(Msh\\), camera positions \\(\Phi_{cam}\\), and various preset color texture \\(T\\) as input. Meanwhile, we obtain images of different preset colors from the Carla simulation environment. Then, we mask out the vehicle parts as the network ground truth \\(GT\\). To optimize the parameters of EFR, we use the following loss function: \\[\begin{gathered}
L_{EFR} (X_{ref})=W\left (X_{r e f}\right) B C E\left (X_{ren}, GT\right),\label{10}
\end{gathered}\\] where BCE is the binary cross-entropy loss, and \\(W (X_{ref})=\frac{H \cdot W}{S}\\) is a weight function. \\(H\\) and \\(W\\) are the heights and widths of \\(X_{ref}\\), and \\(S\\) is the number of pixel points in the vehicle part of \\(X_{ref}\\). \\(W(X_{ref})\\) can balance EFR rendering optimization across various camera angles, especially for views where the vehicle occupies a small area of the image.

## Adversarial Texture Generation with Diffusion Model

Prior works optimize the adversarial camouflage in the pixel space, which leads to unnatural and uncontrollable adversarial texture patterns. To alleviate this, we leverage an off-the-shelf stable diffusion model `\citep{Rombach_2022_CVPR}`{=latex} to generate a vehicle UV-map texture image. The diffusion model is trained on a subset of LAION-5B `\citep{Schuhmann2022NeurIPS}`{=latex}, a dataset of billions of image-text pairs. Since the diffusion model learns the manifold of natural images and corresponding text captions, it can generate adversarial texture images that look more natural and relevant to the given text prompt.

To make the generated UV-map images adversarial, we introduce an adversarial feature vector \\(F_{adv}\\) that can be optimized during the camouflage generation with diffusion models. The adversarial feature vector has the same hidden dimension as the text feature vector \\(F_{txt}\\). Therefore, it can concatenate with \\(F_{txt}\\) to form the conditional input to the T2I model. As a result, the generated image reflects the control signal from the text feature while being adversarial against the target detector.

We also reordered the UV map of the vehicle texture so that the vehicle surface could be connected as much as possible. Figure <a href="#uv_map_order" data-reference-type="ref" data-reference="uv_map_order">3</a> shows the UV mapping before and after reordering. The reordering makes the generated camouflage can keep more natural patterns generated by the diffusion model. Hence, it improves the naturalness of the vehicle camouflage.

<figure id="uv_map_order">
<img src="./figures/uv_map_order.png"" />
<figcaption>Reordered texture UV map to improve camouflage naturalness.</figcaption>
</figure>

## Naturalness vs Attack Performance Trade-off

Without any constraints, the framework can move the combined feature from \\(F_{txt}\\) and \\(F_{adv}\\) out of the feature space of the text prompt. Consequently, we can no longer expect the generated UV-map images to look natural and consistent with the input text prompt. Since the diffusion model is trained to generate natural and controllable images with original text features, there is a higher chance of generating natural images if the concatenated feature is closer to the original text feature.

To preserve naturalness and controllability, we assure that the adversarial feature \\(F_{adv}\\) will not exceed a norm greater than a threshold \\(\tau\\). Tuning the norm threshold \\(\tau\\) enables us to trade off naturalness and controllability for attack performance.

We follow PGD and choose \\(\ell_{p}\\) norm to constrain \\(F_{adv}\\). We update \\(F_{adv}\\) using the formula below: \\[\begin{gathered}
F_{adv}^t = \kappa \left( F_{adv}^{t-1} + \eta \nabla L_{adv} \right) \label{12}, \\
\kappa(F) = \left\{ F_i \mid F_i \leftarrow \min \left( \max \left( F_i, -\tau \right), \tau \right), F_i \sim F \right\} \label{13}
\end{gathered}\\] where \\(t\\) is the time step, \\(\eta\\) is the step size, \\(\nabla L_{adv}\\) is the gradient of the adversarial loss, \\(\kappa\\) is the clipping function defined as in Eq. <a href="#13" data-reference-type="ref" data-reference="13">[13]</a>, where \\(F_i\\) is the \\(i\\)-th element of \\(F\\).

# Experiments

## Experimental Settings [exp_settings]

**Datasets**: We utilize the Carla `\cite{dosovitskiy2017carla}`{=latex} simulator to generate datasets for our experiments. To have a comparative analysis with prior studies `\citep{wang2021dual,wang2022fca, Suryanto_2022_CVPR, Suryanto_2023_ICCV}`{=latex}, we select the Audi E-Tron as the target vehicle model. We create datasets using various simulation settings, resulting in 69,120 and 59,152 photo-realistic images for EFR training and testing. These images cover 16 distinct weather conditions, combining four sun altitudes and four fog densities. Additionally, we generate a dataset of 40,960 images for camouflage generation and a test dataset of 8192 images for adversarial camouflage evaluation. The weather conditions included in these two datasets are the same as those used during the training and testing of EFR. We print out five types of adversarial camouflages for physical world evaluation and apply them to 1:12 Audi E-Tron car models. For each model, we capture 96 pictures under different elevations, azimuths, and distance parameters.

**Baselines**: We compare our method with four advanced adversarial camouflage methods: DAS `\cite{wang2021dual}`{=latex}, FCA `\cite{wang2022fca}`{=latex}, DTA `\cite{Suryanto_2022_CVPR}`{=latex}, and ACTIVE `\cite{Suryanto_2023_ICCV}`{=latex}. DAS and FCA optimize the 3D texture by minimizing the attention-map scores and detection scores of the detector, respectively. DTA and ACTIVE both optimize a square texture pattern with a neural network and cover it on the vehicle’s surface repeatedly. We compare our results using the official textures generated by these methods. We apply these textures to the same car model for a fair comparsion.

**Evaluation metrics**<span id="evaluation-metrics" label="evaluation-metrics"></span>: For training the EFR component, we follow the setting from `\cite{Suryanto_2022_CVPR}`{=latex} to use the Mean Absolute Error (MAE) as a loss to measure the difference between the output of ERP and the ground truth. To evaluate the effectiveness of the adversarial camouflage, we utilize the AP@0.5 benchmark `\cite{everingham2015pascal}`{=latex}, as it provides a comprehensive assessment of recall and precision values at a detection IOU threshold of 0.5.

**Target detection models**<span id="target-models" label="target-models"></span>: Aligning with previous work, we adopt YOLOv3 `\cite{redmon2018YOLOv3}`{=latex} as the white-box target detection model for adversarial camouflage generation. To evaluate the effectiveness of the optimized camouflage, we utilize a collection of widely used object detection models treated as black-box models, including YOLOF `\cite{chen2021yolof}`{=latex}, Deformable DETR (DDTR) `\cite{zhu2020deformable}`{=latex}, Dynamic R-CNN (DRCN) `\cite{zhang2020dynamic}`{=latex}, Sparse R-CNN (SRCN) `\cite{sun2021sparse}`{=latex}, and Faster R-CNN (FrRCNN) `\cite{faster-rcnn}`{=latex}. They are trained on the COCO dataset and implemented in MMDetection `\cite{chen2019mmdetection}`{=latex}.

**Training details**<span id="implementation-details" label="implementation-details"></span>: Following `\cite{Suryanto_2022_CVPR}`{=latex}, we utilize the Adam optimizer with a learning rate 0.01 for EFR training and camouflage generation. We train the EFR for 20 epochs and choose the model with the best performance on the test dataset. We use stable diffusion v1.5 to generate UV-map texture image with DDIM sampler. We set the sampling step to 20. The optimization of the adversarial camouflage takes a duration of five epochs. We conduct experiments on a cluster with eight NVIDIA RTX A800 80GB GPUs.

## Attack Performance Evaluation

### Attack in the Digital World

In this section, we compare our method to current advanced adversarial camouflage methods, including DAS `\cite{wang2021dual}`{=latex}, FCA `\cite{wang2022fca}`{=latex}, DTA `\cite{Suryanto_2022_CVPR}`{=latex}, and ACTIVE `\cite{Suryanto_2023_ICCV}`{=latex}. We run an extensive attack comparison using diverse detection models. Since the target model is YOLOv3, we use various detection models to evaluate the transferability of the camouflage in a black-box setting. We use *’colorful camouflage’* as the text prompt in this experiment.

<div class="center" markdown="1">

<div class="small" markdown="1">

<div class="sc" markdown="1">

<div id="sim-table1" markdown="1">

<table>
<caption>Comparison of the effectiveness of camouflages across various object detection models. Values are AP@0.5 of the car.</caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Methods</strong></th>
<th colspan="3" style="text-align: center;"><strong>Single-stage</strong></th>
<th colspan="3" style="text-align: center;"><strong>Two-stage</strong></th>
<th style="text-align: center;"><strong>Total</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><span>2-4</span>(lr)<span>5-7</span></td>
<td style="text-align: center;">YOLOv3</td>
<td style="text-align: center;">YOLOF</td>
<td style="text-align: center;">DDTR</td>
<td style="text-align: center;">DRCN</td>
<td style="text-align: center;">SRCN</td>
<td style="text-align: center;">FrRCN</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Normal</td>
<td style="text-align: center;">0.712</td>
<td style="text-align: center;">0.824</td>
<td style="text-align: center;">0.803</td>
<td style="text-align: center;">0.778</td>
<td style="text-align: center;">0.786</td>
<td style="text-align: center;">0.771</td>
<td style="text-align: center;">0.779</td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">0.642</td>
<td style="text-align: center;">0.753</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.694</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;">0.672</td>
<td style="text-align: center;">0.678</td>
</tr>
<tr>
<td style="text-align: left;">DAS</td>
<td style="text-align: center;">0.671</td>
<td style="text-align: center;">0.769</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.715</td>
<td style="text-align: center;">0.724</td>
<td style="text-align: center;">0.719</td>
<td style="text-align: center;">0.723</td>
</tr>
<tr>
<td style="text-align: left;">FCA</td>
<td style="text-align: center;">0.581</td>
<td style="text-align: center;">0.725</td>
<td style="text-align: center;">0.603</td>
<td style="text-align: center;">0.678</td>
<td style="text-align: center;">0.642</td>
<td style="text-align: center;">0.668</td>
<td style="text-align: center;">0.650</td>
</tr>
<tr>
<td style="text-align: left;">DTA</td>
<td style="text-align: center;">0.521</td>
<td style="text-align: center;">0.657</td>
<td style="text-align: center;"><strong>0.402</strong></td>
<td style="text-align: center;">0.614</td>
<td style="text-align: center;">0.488</td>
<td style="text-align: center;">0.562</td>
<td style="text-align: center;">0.541</td>
</tr>
<tr>
<td style="text-align: left;">ACTIVE</td>
<td style="text-align: center;"><strong>0.473</strong></td>
<td style="text-align: center;">0.577</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;"><strong>0.534</strong></td>
<td style="text-align: center;">0.484</td>
<td style="text-align: center;">0.520</td>
<td style="text-align: center;">0.504</td>
</tr>
<tr>
<td style="text-align: left;">CNCA</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;"><strong>0.538</strong></td>
<td style="text-align: center;"><span>0.436</span></td>
<td style="text-align: center;"><span>0.536</span></td>
<td style="text-align: center;"><strong>0.470</strong></td>
<td style="text-align: center;"><strong>0.504</strong></td>
<td style="text-align: center;"><strong>0.495</strong></td>
</tr>
</tbody>
</table>

</div>

</div>

</div>

</div>

The results are shown in Table <a href="#sim-table1" data-reference-type="ref" data-reference="sim-table1">1</a>, showing that our method has competitive performance with the current state-of-the-art baselines. DAS performs only better than normal car painting, primarily due to the limitations of partially painted camouflage. Meanwhile, FCA exhibits sub-optimal performance, only slightly better than random camouflage, because it cannot render sophisticated environment characteristics. DTA and ACTIVE have comparable attack performance to CNCA, but our method achieves the best attack performance in total. Figure <a href="#curve" data-reference-type="ref" data-reference="curve">4</a> shows the summarized performance of each camera pose and weather parameter; values are car AP@0.5 averaged from the detectors used in Table <a href="#sim-table1" data-reference-type="ref" data-reference="sim-table1">1</a>. We can see that the camouflage produced by our method shows competitive performance compared to DTA and ACTIVE.

<figure id="curve">
<div class="center">
<img src="./figures/figure-3-curve.png"" />
</div>
<figcaption>Attack comparison on different camera poses and weather parameters. "ele" denotes elevation, "azi" denotes azimuth, "dis" denotes distance, "fog" denotes fog density, and "sun" denotes sun altitude angle. Values are car AP@0.5 (%) averaged from all models.</figcaption>
</figure>

### Attack in the Physical World

Following `\cite{Suryanto_2022_CVPR, Suryanto_2023_ICCV}`{=latex}, we conduct the physical world evaluation by using two 1:12 Audi E-Tron car models: one for a normal and another for our generated camouflage targeting YOLOv3. Table <a href="#phy-table1" data-reference-type="ref" data-reference="phy-table1">2</a> shows our attack performance against real-time object detectors in the real world: YOLOv3, YOLOX `\cite{ge2021yolox}`{=latex}, SSD `\cite{liu2016ssd}`{=latex}, CenterNet `\cite{zhou2019objects}`{=latex}, RetinaNet `\cite{lin2017focal}`{=latex}. Furthermore, Figure <a href="#phy_exp_example" data-reference-type="ref" data-reference="phy_exp_example">5</a> shows that the regular car is detected correctly while our camouflaged car is detected incorrectly. The physical results demonstrate our method is transferable to the real world.

<div class="center" markdown="1">

<div class="small" markdown="1">

<div class="sc" markdown="1">

<div id="phy-table1" markdown="1">

| **Methods** |  YOLOv3   |   YOLOX   |    SSD    | CenterNet | RetinaNet |
|:------------|:---------:|:---------:|:---------:|:---------:|:---------:|
| Normal      |   0.705   |   0.619   |   0.704   |   0.846   |   0.917   |
| Ours        | **0.363** | **0.244** | **0.237** | **0.628** | **0.692** |

AP@0.5 of the car in the physical world evaluation.

</div>

</div>

</div>

</div>

<figure id="phy_exp_example">
<img src="./figures/phy_exp_example_v2.png"" />
<figcaption>Real-world evaluation using two scaled cars. The upper row is the normal car model, and the bottom row is the adversarial camouflaged car.</figcaption>
</figure>

## Customzablility and Naturalness Evaluation

### Customizable Camouflage Generation

Our method enables customizable camouflage generation with user-specific input. Table <a href="#custom-table1" data-reference-type="ref" data-reference="custom-table1">3</a> shows the various styles of adversarial camouflages with their corresponding text prompts and AP@05 values (target YOLOv3). Our method can directly customize the color choices and patterns of the camouflage with the input texture prompt. We notice that there are some differences in attack performance regarding different prompts. During our experiments, we found the prompts that describe natural objects (columns 4,5,6 in Table <a href="#custom-table1" data-reference-type="ref" data-reference="custom-table1">3</a>) are likely to have lower attack performance(average around 0.03 in AP@0.5) than the more abstract ones (columns 1,2,3 in Table <a href="#custom-table1" data-reference-type="ref" data-reference="custom-table1">3</a>). Despite this, the average AP@0.5 of all these camouflages generated by CNCA is 0.506, comparable to the reported result in Table <a href="#sim-table1" data-reference-type="ref" data-reference="sim-table1">1</a>.

<div class="center" markdown="1">

<div class="small" markdown="1">

<div class="sc" markdown="1">

<div id="custom-table1" markdown="1">

<table>
<caption>Customizable camouflages with different text prompts. The AP@0.5 of each camouflage is shown below. The normal car texture baseline is 0.712. </caption>
<thead>
<tr>
<th colspan="6" style="text-align: center;">Customizable Camouflage Generation With user-specific text prompt.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><em>colorful graffiti</em></td>
<td style="text-align: center;"><em>yellow black graffiti</em></td>
<td style="text-align: center;"><em>colorful camouflage</em></td>
<td style="text-align: center;"><em>colorful balls</em></td>
<td style="text-align: center;"><em>snake texture</em></td>
<td style="text-align: center;"><em>zebra strips</em></td>
</tr>
<tr>
<td style="text-align: center;"><img src="./figures/colorful_graffiti.png"" style="width:15.0%" alt="image" /></td>
<td style="text-align: center;"><img src="./figures/yellow_black_graffiti.png"" style="width:15.0%" alt="image" /></td>
<td style="text-align: center;"><img src="./figures/colorful_camouflage.png"" style="width:15.0%" alt="image" /></td>
<td style="text-align: center;"><img src="./figures/colorful_balls.png"" style="width:15.0%" alt="image" /></td>
<td style="text-align: center;"><img src="./figures/snake_texture.png"" style="width:15.0%" alt="image" /></td>
<td style="text-align: center;"><img src="./figures/zebra_strips.png"" style="width:15.0%" alt="image" /></td>
</tr>
<tr>
<td style="text-align: center;">0.508</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.516</td>
<td style="text-align: center;">0.486</td>
<td style="text-align: center;">0.561</td>
</tr>
</tbody>
</table>

</div>

</div>

</div>

</div>

<div class="center" markdown="1">

<div class="small" markdown="1">

<div class="sc" markdown="1">

<div id="natural-table1" markdown="1">

| Images | <img src="./figures/normal.png"" style="width:15.0%" alt="image" /> | <img src="./figures/das.png"" style="width:15.0%" alt="image" /> | <img src="./figures/fca.png"" style="width:15.0%" alt="image" /> | <img src="./figures/dta.png"" style="width:15.0%" alt="image" /> | <img src="./figures/active.png"" style="width:15.0%" alt="image" /> | <img src="./figures/ours.png"" style="width:15.0%" alt="image" /> |
|:---|:--:|:--:|:--:|:--:|:--:|:--:|
| Score | \\(4.68 \pm 0.67\\) | \\(2.05 \pm 1.03\\) | \\(2.11 \pm 1.17\\) | \\(1.86 \pm 0.98\\) | \\(1.84 \pm 0.99\\) | \\(2.84 \pm 1.09\\) |
| Source | Normal | DAS | FCA | DTA | ACTIVE | CNCA(Ours) |

Subjective tests for the naturalness evaluation of our adversarial camouflage with other baselines. The naturalness score is scaled from 1(not natural at all) to 5(very natural). As shown in the results, our score is significantly higher than the other four advanced adversarial camouflages.

</div>

</div>

</div>

</div>

### Naturalness Score by Subjective Evaluation

To verify that the camouflage produced by CNCA is perceived as visually coherent by a broad spectrum of observers, we conducted an online perceptual study with 45 unpaid volunteers recruited through social-media advertisement. Because the goal was to capture the response of the average road user rather than a specialised subgroup, we imposed no demographic or perceptual pre-screening – in particular, colour-vision status was left unconstrained. Each participant viewed 36 randomly ordered images (6 camouflage types × 6 camera poses) and rated naturalness on a 1–5 Likert scale. The study involved no sensitive personal data, was fully anonymous and therefore did not require institutional ethics review. Mean scores (± s.d.) are summarised in Table 4. Consistent with our qualitative observations, CNCA achieves a markedly higher naturalness score than all existing adversarial camouflages while remaining much lower than a stock vehicle finish. The relatively tight standard deviation demonstrates that the ratings are stable across the unfiltered participant pool, confirming that the impressions reported here are representative of real-world viewers.

<div class="center" markdown="1">

<div class="small" markdown="1">

<div class="sc" markdown="1">

<div id="natural-table1" markdown="1">

| Images | <img src="./figures/normal.png"" style="width:15.0%" alt="image" /> | <img src="./figures/das.png"" style="width:15.0%" alt="image" /> | <img src="./figures/fca.png"" style="width:15.0%" alt="image" /> | <img src="./figures/dta.png"" style="width:15.0%" alt="image" /> | <img src="./figures/active.png"" style="width:15.0%" alt="image" /> | <img src="./figures/ours.png"" style="width:15.0%" alt="image" /> |
|:---|:--:|:--:|:--:|:--:|:--:|:--:|
| Score | \(4.68 \pm 0.67\) | \(2.05 \pm 1.03\) | \(2.11 \pm 1.17\) | \(1.86 \pm 0.98\) | \(1.84 \pm 0.99\) | \(2.84 \pm 1.09\) |
| Source | Normal | DAS | FCA | DTA | ACTIVE | CNCA(Ours) |

Subjective tests for the naturalness evaluation of our adversarial camouflage with other baselines. The naturalness score is scaled from 1 (not natural at all) to 5 (very natural). As shown in the results, our score is significantly higher than the other four advanced adversarial camouflages.

</div>

</div>

</div>

</div>
## Ablation Studies

**Trade-off between Naturalness and Attack Performance.** There is inevitably a trade-off between naturalness and attack performance. The optimization space for adversarial attacks decreases when increasing naturalness of the attack. Therefore, increasing naturalness will typically decrease attack performance. Our method allows users to balance this trade-off based on their preference by adjusting the norm threshold \\(\tau\\). To illustrate this trade-off, we generated adversarial UV texture images with the same text prompt but different norm threshold settings for the adversarial features. In addition, we conduct a subjective survey to rank the naturalness of the images and their relevance regarding the text prompt. Table <a href="#tradeoff-table1" data-reference-type="ref" data-reference="tradeoff-table1">5</a> shows the average rank of each image, its corresponding AP@0.5, and the norm threshold settings. It can be seen that when the norm threshold starts to increase, the texture images become less natural-looking, and the attack performance increases. Therefore, it requires the user to decide the acceptable naturalness level.

<div class="center" markdown="1">

<div class="small" markdown="1">

<div class="sc" markdown="1">

<div id="tradeoff-table1" markdown="1">

| Threshold | 0.1 | 0.5 | 1 | 1.5 | \\(\infty\\) |
|:---|:--:|:--:|:--:|:--:|:--:|
| Texture | <img src="./figures/tau_0.1.png"" style="width:15.0%" alt="image" /> | <img src="./figures/tau_0.5.png"" style="width:15.0%" alt="image" /> | <img src="./figures/tau_1.png"" style="width:15.0%" alt="image" /> | <img src="./figures/tau_1.5.png"" style="width:15.0%" alt="image" /> | <img src="./figures/tau_NA.png"" style="width:15.0%" alt="image" /> |
| AP@0.5 | 0.606 | 0.449 | 0.479 | 0.481 | 0.520 |
| Score | 4.79 | 3.54 | 3.33 | 1.46 | 1.71 |

Naturalness test average score against attack performance using YOLOv3. The input text prompt is "yellow black graffiti".

</div>

</div>

</div>

</div>

# Limitations & Societal Impact [limits_impact]

**Limitations.** The principal limitation of CNCA presently lies in the additional computation introduced by back-propagation through the diffusion model, which is roughly twice as slow as FCA when executed on identical hardware. We also require a manual hyper-parameter search for the norm threshold \(\tau\) to accommodate different text prompts. Our perceptual study purposefully relied on a broad, unscreened participant pool in order to capture a realistic cross-section of road users; while this inevitably introduces inter-subject variability, it simultaneously strengthens the external validity of the findings and is therefore not considered a drawback.

**Societal Impact.** This work aims to advance the safety of AI systems by illuminating potential weaknesses that can be leveraged during robustness evaluation. Although the proposed camouflage could be misused by adversaries, it also provides valuable test cases for developing stronger defences, ultimately benefiting society by contributing to safer autonomous-driving technology.
# Conclusion

We propose a novel physical adversarial camouflage attack framework with diffusion models. With different user-specific text prompts, our method can generate adversarial camouflage with diverse colors and patterns. In particular, we apply a clipping strategy to an adversarial feature to balance the adversarial camouflage’s naturalness and attack performance. With extensive experiments on the digital and physical world and user studies, the results demonstrate that our methods improve the naturalness and enable the customizability of camouflage generation while maintaining competitive attack performance.

# References [references]

<div class="thebibliography" markdown="1">

Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. 2019 . *CoRR*, abs/1906.07155. **Abstract:** We present MMDetection, an object detection toolbox that contains a rich set of object detection and instance segmentation methods as well as related components and modules. The toolbox started from a codebase of MMDet team who won the detection track of COCO Challenge 2018. It gradually evolves into a unified platform that covers many popular detection methods and contemporary modules. It not only includes training and inference codes, but also provides weights for more than 200 network models. We believe this toolbox is by far the most complete detection toolbox. In this paper, we introduce the various features of this toolbox. In addition, we also conduct a benchmarking study on different methods, components, and their hyper-parameters. We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new detectors. Code and models are available at https://github.com/open-mmlab/mmdetection. The project is under active development and we will keep this document updated. (@chen2019mmdetection)

Qiang Chen, Yingming Wang, Tong Yang, Xiangyu Zhang, Jian Cheng, and Jian Sun. 2021 You only look one-level feature In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 13039–13048. **Abstract:** This paper revisits feature pyramids networks (FPN) for one-stage detectors and points out that the success of FPN is due to its divide-and-conquer solution to the optimization problem in object detection rather than multi-scale feature fusion. From the perspective of optimization, we introduce an alternative way to address the problem instead of adopting the complex feature pyramids - utilizing only one-level feature for detection. Based on the simple and efficient solution, we present You Only Look One-level Feature (YOLOF). In our method, two key components, Dilated Encoder and Uniform Matching, are proposed and bring considerable improvements. Extensive experiments on the COCO benchmark prove the effectiveness of the proposed model. Our YOLOF achieves comparable results with its feature pyramids counterpart RetinaNet while being 2.5× faster. Without transformer layers, YOLOF can match the performance of DETR in a single-level feature manner with 7× less training epochs. Code is available at https://github.com/megvii-model/YOLOF. (@chen2021yolof)

Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. 2017 In *Conference on robot learning*, pages 1–16. PMLR. **Abstract:** We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform’s utility for autonomous driving research. The supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E (@dosovitskiy2017carla)

Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. 2015 *International journal of computer vision*, 111:98–136. (@everingham2015pascal)

Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. 2021 [YOLOX: Exceeding YOLO Series in 2021](http://arxiv.org/abs/2107.08430). *CoRR*, abs/2107.08430. **Abstract:** In this report, we present some experienced improvements to YOLO series, forming a new high-performance detector – YOLOX. We switch the YOLO detector to an anchor-free manner and conduct other advanced detection techniques, i.e., a decoupled head and the leading label assignment strategy SimOTA to achieve state-of-the-art results across a large scale range of models: For YOLO-Nano with only 0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing NanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in industry, we boost it to 47.3% AP on COCO, outperforming the current best practice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as YOLOv4-CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on Tesla V100, exceeding YOLOv5-L by 1.8% AP. Further, we won the 1st Place on Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021) using a single YOLOX-L model. We hope this report can provide useful experience for developers and researchers in practical scenes, and we also provide deploy versions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at https://github.com/Megvii-BaseDetection/YOLOX. (@ge2021yolox)

Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020 Denoising diffusion probabilistic models *Advances in neural information processing systems*, 33:6840–6851. **Abstract:** We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion (@ho2020denoising)

Yu-Chih-Tuan Hu, Bo-Han Kung, Daniel Stanley Tan, Jun-Cheng Chen, Kai-Lung Hua, and Wen-Huang Cheng. 2021 Naturalistic physical adversarial patch for object detectors In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 7848–7857. **Abstract:** Most prior works on physical adversarial attacks mainly focus on the attack performance but seldom enforce any restrictions over the appearance of the generated adversarial patches. This leads to conspicuous and attention-grabbing patterns for the generated patches which can be easily identified by humans. To address this issue, we pro-pose a method to craft physical adversarial patches for object detectors by leveraging the learned image manifold of a pretrained generative adversarial network (GAN) (e.g., BigGAN and StyleGAN) upon real-world images. Through sampling the optimal image from the GAN, our method can generate natural looking adversarial patches while maintaining high attack performance. With extensive experiments on both digital and physical domains and several independent subjective surveys, the results show that our proposed method produces significantly more realistic and natural looking patches than several state-of-the-art base-lines while achieving competitive attack performance. \<sup xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"\>1\</sup\> (@hu2021naturalistic)

Zhanhao Hu, Wenda Chu, Xiaopei Zhu, Hui Zhang, Bo Zhang, and Xiaolin Hu. 2023 Physically realizable natural-looking clothing textures evade person detectors via 3d modeling In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 16975–16984. **Abstract:** Recent works have proposed to craft adversarial clothes for evading person detectors, while they are either only effective at limited viewing angles or very conspicuous to humans. We aim to craft adversarial texture for clothes based on 3D modeling, an idea that has been used to craft rigid adversarial objects such as a 3D-printed turtle. Unlike rigid objects, humans and clothes are non-rigid, leading to difficulties in physical realization. In order to craft natural-looking adversarial clothes that can evade person detectors at multiple viewing angles, we propose adversarial camou-flage textures (AdvCaT) that resemble one kind of the typical textures of daily clothes, camouflage textures. We leverage the Voronoi diagram and Gumbel-softmax trick to parameterize the camouflage textures and optimize the parameters via 3D modeling. Moreover, we propose an efficient augmentation pipeline on 3D meshes combining topologically plausible projection (TopoProj) and Thin Plate Spline (TPS) to narrow the gap between digital and real-world objects. We printed the developed 3D texture pieces on fabric materials and tailored them into T-shirts and trousers. Experiments show high attack success rates of these clothes against multiple detectors. (@hu2023physically)

Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. 2017 Focal loss for dense object detection In *Proceedings of the IEEE international conference on computer vision*, pages 2980–2988. **Abstract:** The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. (@lin2017focal)

Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. 2016 In *Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I 14*, pages 21–37. Springer. **Abstract:** We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines pre- dictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into sys- tems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets conﬁrm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a uniﬁed framework for both training and inference. For 300300in- put, SSD achieves 74.3% mAP1on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512512input, SSD achieves 76.9% mAP, outperforming a compa- rable state-of-the-art Faster R-CNN model. Compared to other single stage meth- ods, SSD has much better accuracy even with a smaller input image size. Code is available at: https://github.com/weiliu89/caffe/tree/ssd . (@liu2016ssd)

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021 Learning transferable visual models from natural language supervision . In *Proceedings of the 38th International Conference on Machine Learning*, volume 139 of *Proceedings of Machine Learning Research*, pages 8748–8763. PMLR. **Abstract:** State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. (@clip-2021-icml)

Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari. 2020 Accelerating 3d deep learning with pytorch3d *arXiv:2007.08501*. **Abstract:** Deep learning has significantly improved 2D image recognition. Extending into 3D may advance many new applications including autonomous vehicles, virtual and augmented reality, authoring 3D content, and even improving 2D recognition. However despite growing interest, 3D deep learning remains relatively underexplored. We believe that some of this disparity is due to the engineering challenges involved in 3D deep learning, such as efficiently processing heterogeneous data and reframing graphics operations to be differentiable. We address these challenges by introducing PyTorch3D, a library of modular, efficient, and differentiable operators for 3D deep learning. It includes a fast, modular differentiable renderer for meshes and point clouds, enabling analysis-by-synthesis approaches. Compared with other differentiable renderers, PyTorch3D is more modular and efficient, allowing users to more easily extend it while also gracefully scaling to large meshes and images. We compare the PyTorch3D operators and renderer with other implementations and demonstrate significant speed and memory improvements. We also use PyTorch3D to improve the state-of-the-art for unsupervised 3D mesh and point cloud prediction from 2D images on ShapeNet. PyTorch3D is open-source and we hope it will help accelerate research in 3D deep learning. (@ravi2020pytorch3d)

Joseph Redmon and Ali Farhadi. 2018 *arXiv preprint arXiv:1804.02767*. **Abstract:** We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that’s pretty swell. It’s a little bigger than last time but more accurate. It’s still fast though, don’t worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/ (@redmon2018YOLOv3)

Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. 2017 [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://doi.org/10.1109/TPAMI.2016.2577031) *IEEE Trans. Pattern Anal. Mach. Intell.*, 39(6):1137–1149. (@faster-rcnn)

Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022 High-resolution image synthesis with latent diffusion models In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 10684–10695. **Abstract:** By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. (@Rombach_2022_CVPR)

Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. 2022 Laion-5b: An open large-scale dataset for training next generation image-text models . In *Advances in Neural Information Processing Systems*, volume 35, pages 25278–25294. Curran Associates, Inc. **Abstract:** Groundbreaking language-vision architectures like CLIP and DALL-E proved the utility of training on large amounts of noisy image-text data, without relying on expensive accurate labels used in standard vision unimodal supervised learning. The resulting models showed capabilities of strong text-guided image generation and transfer to downstream tasks, while performing remarkably at zero-shot classification with noteworthy out-of-distribution robustness. Since then, large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and Imagen made further improvements. Studying the training and capabilities of such models requires datasets containing billions of image-text pairs. Until now, no datasets of this size have been made openly available for the broader research community. To address this problem and democratize research on large-scale multi-modal models, we present LAION-5B - a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, of which 2.32B contain English language. We show successful replication and fine-tuning of foundational models like CLIP, GLIDE and Stable Diffusion using the dataset, and discuss further experiments enabled with an openly available dataset of this scale. Additionally we provide several nearest neighbor indices, an improved web-interface for dataset exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection. Announcement page https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/ (@Schuhmann2022NeurIPS)

Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, et al. 2021 In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 14454–14463. **Abstract:** We present Sparse R-CNN, a purely sparse method for object detection in images. Existing works on object detection heavily rely on dense object candidates, such as k anchor boxes pre-defined on all grids of image feature map of size H × W. In our method, however, a fixed sparse set of learned object proposals, total length of N, are provided to object recognition head to perform classification and location. By eliminating HWk (up to hundreds of thousands) hand-designed object candidates to N (e.g. 100) learnable proposals, Sparse R-CNN completely avoids all efforts related to object candidates design and many-to-one label assignment. More importantly, final predictions are directly output without non-maximum suppression post-procedure. Sparse R-CNN demonstrates accuracy, run-time and training convergence performance on par with the well-established detector baselines on the challenging COCO dataset, e.g., achieving 45.0 AP in standard 3× training schedule and running at 22 fps using ResNet-50 FPN model. We hope our work could inspire re-thinking the convention of dense prior in object detectors. The code is available at: https://github.com/PeizeSun/SparseR-CNN. (@sun2021sparse)

Naufal Suryanto, Yongsu Kim, Hyoeun Kang, Harashta Tatimma Larasati, Youngyeo Yun, Thi-Thu-Huong Le, Hunmin Yang, Se-Yoon Oh, and Howon Kim. 2022 In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 15305–15314. **Abstract:** To perform adversarial attacks in the physical world, many studies have proposed adversarial camouflage, a method to hide a target object by applying camouflage patterns on 3D object surfaces. For obtaining optimal physical adversarial camouflage, previous studies have utilized the so-called neural renderer, as it supports differentiability. However, existing neural renderers cannot fully represent various real-world transformations due to a lack of control of scene parameters compared to the legacy photo-realistic renderers. In this paper, we propose the Differentiable Transformation Attack (DTA), a framework for generating a robust physical adversarial pattern on a target object to camouflage it against object detection models with a wide range of transformations. It utilizes our novel Differentiable Transformation Network (DTN), which learns the expected transformation of a rendered object when the texture is changed while preserving the original properties of the target object. Using our attack framework, an adversary can gain both the advantages of the legacy photo-realistic renderers including various physical-world transformations and the benefit of white-box access by offering differentiability. Our experiments show that our camouflaged 3D vehicles can successfully evade state-of-the-art object detection models in the photo-realistic environment (i.e., CARLA on Unreal Engine). Furthermore, our demonstration on a scaled Tesla Model 3 proves the applicability and transferability of our method to the real world. (@Suryanto_2022_CVPR)

Naufal Suryanto, Yongsu Kim, Harashta Tatimma Larasati, Hyoeun Kang, Thi-Thu-Huong Le, Yoonyoung Hong, Hunmin Yang, Se-Yoon Oh, and Howon Kim. 2023 In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, pages 4305–4314. **Abstract:** Adversarial camouflage has garnered attention for its ability to attack object detectors from any viewpoint by covering the entire object’s surface. However, universality and robustness in existing methods often fall short as the transferability aspect is often overlooked, thus restricting their application only to a specific target with limited performance. To address these challenges, we present Adversarial Camouflage for Transferable and Intensive Vehicle Evasion (ACTIVE), a state-of-the-art physical camouflage attack framework designed to generate universal and robust adversarial camouflage capable of concealing any 3D vehicle from detectors. Our framework incorporates innovative techniques to enhance universality and robustness, including a refined texture rendering that enables common texture application to different vehicles without being constrained to a specific texture map, a novel stealth loss that renders the vehicle undetectable, and a smooth and camouflage loss to enhance the naturalness of the adversarial camouflage. Our extensive experiments on 15 different models show that ACTIVE consistently outperforms existing works on various public detectors, including the latest YOLOv7. Notably, our universality evaluations reveal promising transferability to other vehicle classes, tasks (segmentation models), and the real world, not just other vehicles. (@Suryanto_2023_ICCV)

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. 2014 . In *2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings*. **Abstract:** Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network’s prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input. (@szegedy2013intriguing)

Donghua Wang, Tingsong Jiang, Jialiang Sun, Weien Zhou, Zhiqiang Gong, Xiaoya Zhang, Wen Yao, and Xiaoqian Chen. 2022 In *Proceedings of the AAAI conference on artificial intelligence*, volume 36, pages 2414–2422. **Abstract:** Physical adversarial attacks in object detection have attracted increasing attention. However, most previous works focus on hiding the objects from the detector by generating an individual adversarial patch, which only covers the planar part of the vehicle’s surface and fails to attack the detector in physical scenarios for multi-view, long-distance and partially occluded objects. To bridge the gap between digital attacks and physical attacks, we exploit the full 3D vehicle surface to propose a robust Full-coverage Camouflage Attack (FCA) to fool detectors. Specifically, we first try rendering the nonplanar camouflage texture over the full vehicle surface. To mimic the real-world environment conditions, we then introduce a transformation function to transfer the rendered camouflaged vehicle into a photo-realistic scenario. Finally, we design an efficient loss function to optimize the camouflage texture. Experiments show that the full-coverage camouflage attack can not only outperform state-of-the-art methods under various test cases but also generalize to different environments, vehicles, and object detectors. (@wang2022fca)

Jiakai Wang, Aishan Liu, Zixin Yin, Shunchang Liu, Shiyu Tang, and Xianglong Liu. 2021 In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 8565–8574. **Abstract:** Deep learning models are vulnerable to adversarial examples. As a more threatening type for practical deep learning systems, physical adversarial examples have received extensive research attention in recent years. However, without exploiting the intrinsic characteristics such as model-agnostic and human-specific patterns, existing works generate weak adversarial perturbations in the physical world, which fall short of attacking across different models and show visually suspicious appearance. Motivated by the viewpoint that attention reflects the intrinsic characteristics of the recognition process, this paper proposes the Dual Attention Suppression (DAS) attack to generate visually-natural physical adversarial camouflages with strong transferability by suppressing both model and human attention. As for attacking, we generate transferable adversarial camouflages by distracting the model-shared similar attention patterns from the target to non-target regions. Meanwhile, based on the fact that human visual attention always focuses on salient items (e.g., suspicious distortions), we evade the human-specific bottom-up attention to generate visually-natural camouflages which are correlated to the scenario context. We conduct extensive experiments in both the digital and physical world for classification and detection tasks on up-to-date models (e.g., Yolo-V5) and demonstrate that our method outperforms state-of-the-art methods. \<sup xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"\>1\</sup\> (@wang2021dual)

Tong Wu, Xuefei Ning, Wenshuo Li, Ranran Huang, Huazhong Yang, and Yu Wang. 2020 *arXiv preprint arXiv:2007.16118*. **Abstract:** In this paper, we tackle the issue of physical adversarial examples for object detectors in the wild. Specifically, we proposed to generate adversarial patterns to be applied on vehicle surface so that it’s not recognizable by detectors in the photo-realistic Carla simulator. Our approach contains two main techniques, an Enlarge-and-Repeat process and a Discrete Searching method, to craft mosaic-like adversarial vehicle textures without access to neither the model weight of the detector nor a differential rendering procedure. The experimental results demonstrate the effectiveness of our approach in the simulator. (@wu2020physical)

Hongkai Zhang, Hong Chang, Bingpeng Ma, Naiyan Wang, and Xilin Chen. 2020 In *Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XV 16*, pages 260–275. Springer. **Abstract:** Although two-stage object detectors have continuously ad- vanced the state-of-the-art performance in recent years, the training pro- cess itself is far from crystal. In this work, we rst point out the incon- sistency problem between the xed network settings and the dynamic training procedure, which greatly a ects the performance. For example, the xed label assignment strategy and regression loss function cannot t the distribution change of proposals and thus are harmful to training high quality detectors. Consequently, we propose Dynamic R-CNN to adjust the label assignment criteria (IoU threshold) and the shape of regression loss function (parameters of SmoothL1 Loss) automatically based on the statistics of proposals during training. This dynamic de- sign makes better use of the training samples and pushes the detec- tor to t more high quality samples. Speci cally, our method improves upon ResNet-50-FPN baseline with 1.9% AP and 5.5% AP 90on the MS COCO dataset with no extra overhead. Codes and models are available athttps://github.com/hkzhang95/DynamicRCNN . (@zhang2020dynamic)

Yang Zhang, PD Hassan Foroosh, and Boqing Gong. 2019 *ICLR*. (@zhang2019camou)

Xingyi Zhou, Dequan Wang, and Philipp Krähenbühl. 2019 Objects as points *arXiv preprint arXiv:1904.07850*. **Abstract:** Detection identifies objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefficient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point — the center point of its bounding box. Our detector uses keypoint estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1% AP at 142 FPS, 37.4% AP at 52 FPS, and 45.1% AP with multi-scale testing at 1.4 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time. (@zhou2019objects)

Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. 2021 [Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://openreview.net/forum?id=gZ9hCDWe6ke) In *9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021*. OpenReview.net. (@zhu2020deformable)

</div>

# Subjective Human Evaluation [survey]

<figure id="survey">
<img src="./figures/survey_screenshot.png"" style="width:50.0%" />
<figcaption>The interface of our survey for human evaluations. Participants rate the naturalness of the vehicle’s appearance on a 1–5 scale.</figcaption>
</figure>

Figure 6 depicts the web interface used for the perceptual test. After providing informed consent, participants were shown high-resolution rendered photographs of camouflaged vehicles generated by CNCA and four baselines. Each image remained on screen until the participant selected a naturalness rating; no time limit was imposed. The survey was intentionally kept short (< 5 min) to discourage fatigue effects. Recruitment followed an open-call strategy and we deliberately refrained from excluding individuals with potential colour-vision deficiencies so that the resulting data would reflect the heterogeneous real-world viewing conditions under which autonomous-vehicle detectors operate. No personally identifying information was collected and, because the task presented no more than minimal risk, formal Institutional Review Board (IRB) approval was deemed unnecessary under local regulations. In total, we recorded 45 complete response sets. An anonymised version of the dataset is included with the project repository.