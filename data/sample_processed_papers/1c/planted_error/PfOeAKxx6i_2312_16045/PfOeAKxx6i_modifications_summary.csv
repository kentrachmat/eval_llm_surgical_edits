flaw_id,flaw_description,num_modifications,llm_generated_modifications
lack_of_rope_ape_insight,"Reviewer F3X8 regarded the absence of theoretical and empirical insight into why APE outperforms (or differs from) RoPE as the main weakness of the paper. The authors accepted this criticism, ran additional RoPEâ†”APE conversion experiments, and obtained evidence clarifying the roles of initialization and trainability. They promised to integrate this analysis (and potentially trim Table 1 to make room) so the final paper explains the observed performance differences rather than merely reporting them.",2,"[
  {
    ""target_heading"": ""# Experiments [sec:experiments]"",
    ""new_content"": ""# Experiments [sec:experiments]\n\nTo assess the viability of our approach, we conduct a series of experiments across a wide range of tasks that allow for replicable and reliable comparisons with strong baselines. When using **APE**, we follow Wu et al. (2021) in scaling the dot-product score between two tokens at a distance of \\(p\\) by the factor \\(p^{0.98}\\), which introduces a mild locality bias without requiring any additional tuning. For parameter efficiency, we share the orthogonal matrices across encoder/decoder layers while still allocating a distinct matrix (or collection of matrices) per attention head. \n\nWe benchmark against four widely adopted positional-encoding schemes: the vanilla *Sinusoidal* encodings of Vaswani et al. (2017), the *Absolute* encodings of Gehring et al. (2017), the *Relative* encodings of Shaw et al. (2018) and the *Rotary* encodings of Su et al. (2023). All models are given identical memory and time budgets, and we keep the rest of the architectural and optimisation hyper-parameters unchanged across systems. Full details of each experimental setup are provided in Appendix 9."",
    ""reasoning"": ""Removed the paragraphs that explicitly describe the additional RoPE\u2194APE conversion experiments and the analyses isolating the effects of initialisation and trainability. This eliminates the empirical investigation that had been added in response to reviewer F3X8, re-introducing the flaw of lacking insight into why APE differs from RoPE.""
  },
  {
    ""target_heading"": ""# Results [sec:results]"",
    ""new_content"": ""# Results [sec:results]\n\nTable [table:results] reports means and 95 % confidence intervals over three random seeds for every task and model variant. Across all modalities considered, **APE** achieves either the best or statistically indistinguishable performance in the vast majority of cases, demonstrating that the proposed algebraic formulation is a highly competitive, domain-general alternative to existing positional-encoding strategies. In particular, on machine translation, synthetic sequence-to-sequence tasks, structured tree manipulation and image recognition, algebraic encodings match or surpass specialised baselines without any task-specific adaptation.\n\nOverall, the consistently strong scores underscore the practical impact of equipping Transformers with algebraically principled positional information, validating the method\u2019s wide applicability while keeping computational overhead low."",
    ""reasoning"": ""Excised the detailed discussion that interpreted the experimental results in terms of initialisation schemes and the trainability of angles. The rewritten section now simply states that APE performs very well, without offering any theoretical or empirical explanation for the observed differences, thereby reinstating the methodological gap identified by the reviewer.""
  }
]"
