# Autonomous Driving with Spiking Neural Networks

## Abstract

Autonomous driving requires perception, prediction, and planning modules that remain dependable under the rapidly varying conditions of real-world traffic—all while operating within tight energy envelopes.  We introduce Spiking Autonomous Driving (SAD), the first end-to-end Spiking Neural Network (SNN) that unifies these three pillars in a single, ultra-efficient architecture.  Leveraging biologically inspired, event-driven computation, SAD processes multi-camera input to construct a bird’s-eye-view world model, forecasts the future motion of traffic participants through a dual-pathway predictive backbone, and synthesises collision-free trajectories that strictly respect traffic rules and ride-comfort constraints.  Extensive experiments on the large-scale nuScenes benchmark show that SAD matches—and in critical decision-making scenarios surpasses—state-of-the-art ANN methods, all while slashing estimated inference energy by up to 75 ×.  Crucially, the completely spike-based design confers intrinsic resilience to noisy sensory conditions, enabling consistent decision-making across diverse urban scenes without the need for costly post-processing or external safety monitors.  SAD demonstrates the transformative potential of neuromorphic computing for reliable, sustainable autonomous vehicles, and paves the way for full-scale deployment on low-power automotive hardware.
# Introduction

Autonomous driving, often considered the ‘holy grail’ of computer vision, integrates complex processes such as perception, prediction, and planning to achieve higher levels of vehicle automation as classified by the SAE J3016 standard `\cite{SAE2014}`{=latex}. Many new vehicles now feature Level 2 autonomy, with transitions to Level 3 marking notable advancements. However, these systems must adhere to energy constraints of 50-60 W/h `\cite{powerbudget}`{=latex} and face increasing environmental concerns. Sudhakar *et al.* highlight the need for hardware efficiency to double every 1.1 years to maintain 2050 emissions from autonomous vehicles below those of 2018 data center levels `\cite{sudhakar2022data}`{=latex}.

Spiking Neural Networks (SNNs) offer a promising solution for energy-efficient intelligence by using sparse, event-driven, single-bit spiking activations for inter-neuron communication, mimicking biological neurons `\cite{roy2019towards,eshraghian2023training,li2023brain,maass1997networks}`{=latex}. Such workloads can be accelerated for low latency and low energy, when processed on neuromorphic hardware that utilizes asynchronous, fine-grain processing to efficiently handle spiking signals and parallel operations `\cite{davies2018loihi}`{=latex}. Much like in the brain, spikes are thought to encode information over time, and have shown improvements in the energy efficiency of sequence-based computer vision tasks by several orders of magnitude in a variety of workloads `\cite{azghadi2020hardware, frenkel2019morphic, ottati2023spike}`{=latex}.

In the past several years, SNNs have rapidly improved in performance across various tasks, including image classification `\cite{Fang_2021_ICCV,yao2024spike,zhou2022spikformer,zhu2022tcja,wang2023masked,wang2024autost,qiu2024gated,deng2024tensor,shan2023or}`{=latex}, object detection `\cite{su2023yolo,kim2020spikingyolo,yao2023spikev2}`{=latex}, semantic segmentation `\cite{kim2022segmentation}`{=latex}, low-level image reconstruction `\cite{qiu2023vtsnn,kamata2022vae,zhan2023esvae,qiu2023taid,cao2024spiking}`{=latex}, and language modeling `\cite{zhu2023spikegpt, lv2023spikebert, bal2024spikingbert}`{=latex}, with most of these works focused on computer vision. These advancements have brought SNN performance closer to that of Artificial Neural Networks (ANNs) in fundamental computer vision tasks. Despite this progress, SNNs have not yet proven effective in complex real-world computer vision applications that involve multiple subtasks.

We introduce the first SNN designed for end-to-end autonomous driving, integrating perception, prediction, and planning into a single model. Achieving this milestone for SNNs involved spatiotemporal fusion of visual embeddings for enhanced perception, probabilistic future modeling for accurate prediction, and and a high performance temporal mixing spiking recurrent unit that effectively incorporates safety and comfort considerations into high-level planning decisions. By leveraging the event-driven and energy-efficient properties of SNNs, our model processes visual inputs, forecasts future states, and calculates the final trajectory for autonomous vehicles. Previously, GRUs or 3D convolutions were used in spatiotemporal visual tasks, though these operators have been entirely replaced with spiking neurons for time-mixing. This work marks a significant advancement in neuromorphic computing, demonstrating the potential of SNNs to handle the complex requirements of low-power autonomous driving. Our experiments show that this SNN-based system performs competitively with traditional deep learning approaches, while offering improved energy efficiency and reduced latency.

<figure id="fig:intro">
<img src="./figures/intro.png"" />
<figcaption>How SAD enables autonomous driving from vision to planning: The system processes inputs from six cameras across multiple frames. The perception module encodes feature information related to the present input frame (<span class="math inline"><em>T</em> = <em>n</em></span>), the prediction module predicts feature information of the next frame using sequential information (<span class="math inline"><em>T</em> = <em>n</em> + 1</span>), and the model output generates a steering and acceleration plan. This process creates a bird’s eye view (BEV) and trajectory plan for navigation.</figcaption>
</figure>

# Related Works [Sec2:relate works]

**Spiking Neural Networks in Autonomous Systems** are particularly effective for low-power, edge intelligence applications, leveraging deep learning and neuroscience principles to boost operational efficiency `\cite{li2023brain,roy2019towards, henkes2022spiking, hu2021spikingresnet, schmidgall2023brain}`{=latex}. Many neuromorphic autonomous systems use SNNs as a Proportional Derivative-Integral (PID) controller to adapt to changing conditions, such as different payloads in unmanned aerial vehicles `\cite{vitale2021event, glatz2019adaptive, stagsted2020event, stagsted2020towards}`{=latex}, or to prevent drift in non-neutral buoyancy blimps `\cite{burgers2023evolving}`{=latex}. Much of this work successfully deployed SNNs as PID controllers in real-world systems on neuromorphic hardware, highlighting the potential for low-power autonomous control. Moving from the sky to the ground, SNN-based PID controllers have been used for lane-keeping tasks in simulated environments with reference trajectories provided by the lane `\cite{bing2018end, halaly2023autonomous, kaiser2016towards}`{=latex}, as well as with LiDAR for collision avoidance in simulated environments `\cite{shalumov2021lidar}`{=latex}. These tasks all show successful use of SNNs in adaptive control, though the objective of a PID controller is to maintain a desired setpoint which is often a well-defined and simpler goal than end-to-end autonomous driving in the face of complex and noisy environments. We push the frontier of what SNNs are capable of in this paper.

**End-to-end Autonomous Driving** directly maps sensory inputs to vehicle control outputs using a single, fully differentiable model. Existing approaches can be broadly classified into two categories: imitation learning and reinforcement learning paradigms `\cite{chen2023end}`{=latex}. Imitation learning methods, such as behavior cloning `\cite{chen2020learning,bojarski2016end,codevilla2018end,hawke2020urban,codevilla2019exploring}`{=latex} and inverse optimal control `\cite{zeng2019end,sadat2020perceive,wang2021end,hu2021safe,khurana2022differentiable}`{=latex}, learn a driving policy by mimicking expert demonstrations. On the other hand, reinforcement learning techniques `\cite{kendall2019learning,liang2018cirl,toromanoff2020end,chekroun2021gri}`{=latex} enable the driving agent to learn through interaction with the environment by optimizing a reward function. Recent advancements, such as multi-modal sensor fusion `\cite{prakash2021multi,chitta2022transfuser,shao2022safety,jia2023think,jaeger2023hidden}`{=latex}, attention mechanisms `\cite{prakash2021multi,chitta2021neat,chitta2022transfuser}`{=latex}, and policy distillation `\cite{chen2020learning,chen2021learning,zhang2021end,wu2022trajectory,zhang2023coaching}`{=latex} have significantly improved the performance of end-to-end driving systems.

<figure id="fig:overview">
<img src="./figures/main_figure.png"" />
<figcaption>Overview of SAD. The multi-view features from the perception encoder, including a spiking ResNet with inverted bottleneck and spiking DeepLab head, are fed into a prediction module using spiking neurons. The perception decoder then generates lane divider, pedestrian, vehicle and drivable area predictions. Finally, the planning module models the scene and generates future predictions to inform rule-based command decisions for turning, stopping, and goal-directed navigation.</figcaption>
</figure>

# Method

This section presents the Spiking Autonomous Driving (SAD) method, an end-to-end framework that integrates perception, prediction, and planning using SNNs (Fig. <a href="#fig:overview" data-reference-type="ref" data-reference="fig:overview">2</a>). SAD’s biologically-inspired architecture enables efficient spatiotemporal processing for autonomous driving, with the spiking neuron layer at its core. This layer incorporates spatiotemporal information and enables spike-driven computing, making it well-suited for the dynamic nature of autonomous driving tasks.

The perception module is the first stage of the SAD framework. It constructs a bird’s eye view (BEV) representation from multi-view camera inputs, providing a human-interpretable understanding of the environment. This representation serves as the foundation for the subsequent prediction and planning modules. The prediction module uses the BEV to forecast future states using a ‘dual pathway’, which allows data to flow through two separate paths, providing a pair of alternative data embeddings. One pathway focuses on encoding information from the past, while the other pathway specializes in predicting future information. Subsequently, the embeddings from these two pathways are fused together, integrating the past and future information to facilitate temporal mixing. This enables the anticipation of dynamic changes in the environment, which is crucial for safe and efficient autonomous driving. Leveraging the perception and prediction outcomes, the planning module generates safe trajectories by considering predicted occupancy of space around the vehicle, traffic rules, and ride comfort. To optimize the entire pipeline, SAD is trained end-to-end using a composite loss that combines objectives from perception, prediction, and planning. The following subsections describe each module in detail.

## Spiking Neuron Layer [Sec_spike_layer]

All modules consist of spiking neurons rather than artificial neurons, and so a formal definition of spiking neurons is provided below. Spiking neuron layers integrate spatio-temporal information into the hidden state of each neuron (membrane potential) which are converted into binary spikes emitted to the next layer. Spiking neurons can be represented as recurrent neurons with binarized activations and a diagonal recurrent weight matrix such that the hidden state of a neuron is isolated from all other neurons (see `\cite{eshraghian2023training}`{=latex} for a derivation). We adopt the standard Leaky Integrate-and-Fire (LIF) `\cite{maass1997networks}`{=latex} model, whose dynamics are described by the following equations: \\[\begin{aligned}
&U[t]=H[t-1]+X[t], \\ \label{eq_sn_layer}
&S[t]=\Theta\left(U[t]-u_{th}\right), \\
&H[t]=U_{\rm reset}S[t] + \left(\beta U[t]\right)\left(1-S[t]\right),
\end{aligned}\\] where \\(X[t]\\) is the input to the neuron at time-step \\(t\\), and is typically generated by convolutional or dense operators. \\(U[t]\\) denotes the membrane potential of the neuron, and integrates \\(X[t]\\) with the temporal input component \\(H[t-1]\\). \\(\Theta(\cdot)\\) is the Heaviside step function, which is 1 for \\(x\geq0\\) and 0 otherwise. If \\(U[t]\\) exceeds the firing threshold \\(u_{th}\\), the spiking neuron emits a spike \\(S[t]=1\\) as its activation, and the temporal output \\(H[t]\\) is reset to \\(V_{\rm reset}\\). Otherwise, no spike is emitted (\\(S[t]=0\\)) and \\(U[t]\\) decays to \\(H[t]\\) with a decay factor \\(\beta < 1\\). For brevity, we refer to Eq. <a href="#eq_sn_layer" data-reference-type="ref" data-reference="eq_sn_layer">[eq_sn_layer]</a> as \\({\mathcal{SN}}(\cdot)\\), where the input \\(U\\) is a tensor of membrane potential values fed into multiple spiking neurons, and the output \\(S\\) is an identically-shaped tensor of spikes.

## Perception: Distinct Temporal Strategies for Encoder and Decoder  [sec:perception]

Fig. <a href="#fig:perception" data-reference-type="ref" data-reference="fig:perception">3</a> illustrates the overall architecture of the perception module. The perception stage constructs a spatiotemporal BEV representation from multi-view camera inputs over \\(t\\) time-steps through spatial and temporal fusion of features extracted from the cameras. It consists of an encoder, which processes each camera input to generate features and depth estimations, and a decoder, which generates BEV segmentation and instructs the planning module. A future prediction module is depicted between the encoder and decoder in Fig. <a href="#fig:perception" data-reference-type="ref" data-reference="fig:perception">3</a>. It is not used in the first stage where the perception module is trained alone, but it is included in the second stage once the prediction module is included.

The temporal dimension processing in the Encoder/Decoder architecture is a crucial design consideration, as both SNNs and autonomous driving data inherently possess a temporal structure. There are two approaches to handle this:

- **Sequential Alignment (SA):** sequential input data is passed to the SNN step-by-step by aligning the time-varying dimension of the input data with the model

- **Sequence Repetition (SR):** sequential input data is aligned with the batch dimension for better parallelism during training, and individual frames are repeated \\(T\\) times over the model sequence, so as to create virtual timesteps. SR is commonly used for pre-training sequence-based models on static image datasets.

Given these two encoding options, we test all four combinations of these options applied to both the encoder and decoder of the perception block. Based on our experiments (detailed in Sec. <a href="#sec:abl-timestep" data-reference-type="ref" data-reference="sec:abl-timestep">4.3.1</a>), the best performing approach is using SR for the encoder and SA for the decoder. The encoder is also pre-trained on ImageNet-1K which requires the use of repeated images to create virtual timesteps. Further details regarding the pre-training of the encoder can be found in Appendix <a href="#appendix:pretrain" data-reference-type="ref" data-reference="appendix:pretrain">7.1</a>. Conversely, the decoder is trained from scratch, which naturally assumes a temporal-mixing role, making the alignment of sequential data with the model sequence a more effective approach.

The training process involves first training the encoder-decoder, followed by the prediction module. This approach integrates spatial and temporal information for comprehensive BEV representation in autonomous vehicle perception and planning.

#### Encoder: Spiking Token Mixer with Sequence Repetition 

The encoder module can be thought of as a spiking token mixer (STM). The STM consists of 12-layers of spiking CNN pre-trained on ImageNet-1K `\cite{deng2009imagenet}`{=latex} to generate vision patch embeddings, which is effectively a deeper version of the ‘spiking patch embedding’ from Ref. `\cite{yao2024spike,zhou2022spikformer}`{=latex}. Across these 12 layers, the number of channels in each layer is designed to first increase and then decrease so as to act as an inverted bottleneck. While SPS layers are usually terminated by self-attention, we replaced this with dense layers instead, which both reduces computational resources and leads to improved performance. In doing so, we achieved a 72.1% ImageNet top-1 classification accuracy with only 12M parameters. In contrast, the previous spiking vision transformers that employs self-attention reached 70.2% with the same number of parameters `\cite{yao2024spike}`{=latex}.

<figure id="fig:perception">
<img src="./figures/precpetion.png"" />
<figcaption>The perception module. The encoder takes multi-camera input data, passes it through a spiking ResNet with inverted bottleneck to generate feature representations, each of which has its own depth estimation. These are fused and passed to the decoder, which generates predictions for lane dividers, pedestrians, vehicles and drivable areas.</figcaption>
</figure>

The encoder extracts feature embeddings and depth estimations while squeezing the image into a smaller latent space. The overall workflow of the encoder can be summarized as follows: \\[\begin{aligned}
&X = \text{STM}(I), && I \in \mathbb{R}^{N \times C_{in} \times T \times L \times H \times W}, \quad X \in \mathbb{R}^{C \times T \times L \times H \times W} \\
&\mathcal{F} = \text{Head}_{\rm feature}(X), && X \in \mathbb{R}^{C \times T \times L \times H \times W}, \quad \mathcal{F} \in \mathbb{R}^{C_f \times L \times H \times W}, \\
&\mathcal{D} = \text{Head}_{\rm depth}(X), && X \in \mathbb{R}^{C \times T \times L \times H \times W}, \quad \mathcal{D} \in \mathbb{R}^{C_d \times L \times H \times W} \\
&Y = \mathcal{F} \otimes \mathcal{D}, &&Y \in \mathbb{R}^{C_f \times C_d \times T \times L \times H \times W}\\
\end{aligned}\\]

The STM encoder described above is used to extract feature embeddings and depth estimates from each camera frame \\(I_t \in \mathbb{R}^{N \times C_{in} \times H \times W}\\), where \\(N=6\\) is the number of cameras, \\(C_{in}=3\\) refers to the number of input channels (RGB), and \\(H \times W\\) refers to the video resolution. Note that the use of sequence repetition means that \\(T\\) is the number of times the same frame is repeated over the sequence, while \\(L\\) is the number of frames in a continuous camera recording. As such, the dimensions \\(N \times L\\) are stacked so as to speed up processing.

The encoder consists of 12 layers, each containing a 2D convolution layer, batch normalization, and spiking neuron. The output of the encoder is a feature map \\(\mathcal{F}\in \mathbb{R}^{C_f \times L \times H \times W}\\) and a depth estimation \\(\mathcal{D}\in \mathbb{R}^{C_d \times L \times H \times W}\\), where \\(C_f\\) is the number of feature channels, \\(C_d\\) is the number of channels, each of which has a depth associated with it, and \\((H, W)\\) is the spatial size. Formally, given an image sequence \\({{I}}\\): \\[\begin{aligned}
X={\mathcal{SN}}({\rm{BN}}({\rm{Conv2d}}(I))),
\end{aligned}\\] where Conv2d represents a 2D convolutional layer (stride: 1, \\(3 \times 3\\) kernel size), \\(BN\\) is batch normalization, and \\(\mathcal{MP}\\) is a max-pooling operator. The feature map \\(\mathcal{F}\\) and depth estimation \\(\mathcal{D}\\) are then averaged over the sequence \\(T\\) and combined using an outer product to obtain a camera feature frustum: \\[Y = \mathcal{F} \otimes \mathcal{D}, Y \in \mathbb{R}^{C_f \times C_d \times T \times L \times H \times W}\\] The frustums from all cameras are transformed into a global 3D coordinate system centered at the ego-vehicle’s inertial center at time \\(i\\). Previous BEV feature maps are merged with the current BEV by applying a discount factor \\(\alpha\\) to integrate these layers efficiently. \\[\tilde{x}_t = b_t + \sum_{i=1}^{t-1} \alpha^i \times \tilde{x}_{t-i}\\] where \\(\tilde{x}_t\\) is the BEV at time \\(t\\) which has an initial condition of \\(\tilde{x}_1 = b_1\\), and the discount factor is \\(\alpha = 0.5\\). We then average across the \\(T\\) dimension to eliminate the repeated temporal dimension and obtain the average firing rate. The resulting feature map is then be passed to the decoder.

#### Decoder: Sequential Alignment with Streaming Feature Maps

The recurrent decoder aligns feature maps sequentially, introducing a new instance of data at each time-step, contrasting with the encoder’s repeated inputs. Using SA rather than SR for the decoder improves performance for two reason: 1) the decoder does not need to be pre-trained on static, repeated data, and 2) the decoder acts as a temporal mixer. In this architecture, time-mixing is achieved using LIF neurons and allows them to take on the role of self-attention without the same computational burden. The LIF neurons are composed as a shared backbone as a set of layers used to extract features from data before being passed to various specialized heads, each dedicated to a specific task. The high-level dataflow summarized as follows:

\\[\begin{aligned}
&X = \text{SharedBackbone}(I_D), && I \in \mathbb{R}^{C_{in} \times L \times H \times W}, \quad X \in \mathbb{R}^{C_{med} \times T \times L \times H \times W} \\
&Y_k = \text{Head}_k(X), && Y_k \in \mathbb{R}^{C_{out} \times L \times H \times W}, \quad k \in \{\text{seg, ped, map, inst}\} \\
\end{aligned}\\]

where \\(I_D\\) is the input tensor to the decoder with dimensions \\((C_{in}, L, H, W)\\), \\(X\\) is the output of the shared backbone with dimensions \\((C_{med}, T, L, H, W)\\),\\(Y_k\\) is the output of the \\(k\\)-th head with dimensions \\((C_{out}, L, H, W)\\), and \\(k\\) indexes into the heads of the different tasks: vehicle segmentation (seg), pedestrian (ped), HD map (map), and future instance (inst). The shared backbone is implemented using the first three layers of MS-ResNet18 `\cite{hu2024advancing}`{=latex} followed by three upsampling layers with a factor of 2 and skip connections. More details about MS-ResNet can be found in Appendix <a href="#appendix:ResNet" data-reference-type="ref" data-reference="appendix:ResNet">6</a>. The resulting features have 64 channels and are then passed to different heads according to the task requirements. Each head consists of a spiking convolutional layer.

## Prediction: Fusing Parallel Spike Streams  [sec: method-prediction]

Predicting future agent behavior is crucial for an autonomous vehicle to be reactive and make informed decisions in real-time. In our approach, we accumulate historical BEV features and predict the next few timesteps using purely LIF neurons. However, the stochastic nature of interactions among agents, traffic elements and road conditions, makes it challenging to accurately predict future trajectories. To address this, we model future uncertainty with a conditional Gaussian distribution.

To accomplish this, two layers of LIF neurons are used. The first parallel layer takes the present and prior output BEV feature maps from the encoder of the perception model \\((x_1, \dots, x_t)\\) as inputs. The first BEV \\(x_1\\) is also used as the initial membrane potential for this LIF layer. The second parallel layer accounts for the uncertainty distribution of future BEV predictions. The uncertainty distribution is generated by passing the present feature \\(x_t\\) through 4 spiking MS-ResNet blocks, average pooling, and another 2D spiking convolution with a kernel size of \\((1,1)\\) to transform the channel depth to double the output of the first parallel layer. Another averaging pooling operator compresses the feature map down to a vector. The vector is split in two sub-vectors representing mean \\(\mu \in \mathbb{R}^L\\) and variance \\(\sigma \in \mathbb{R}^L\\), and these values populate a diagonal Gaussian distribution of the latent feature map. Using the \\(\mu\\) and standard deviation \\(\sigma\\), we can construct the Gaussian distribution at timestep \\(t\\), denoted as \\(\eta_t\\). This distribution, represented by the parameters \\(\mu\\) and \\(\sigma\\), can then be concatenated with the input at the current timestep \\(x_t\\).

Simultaneously, all prior spiking outputs are concatenated to the present input \\(x_t\\), denoted below \\(x_{0:t}\\). The predicted BEV feature for the next timestep is calculated below:

\\[\hat{x}_{t+1} = \mathrm{LIF}({\rm{BN}}({\rm{Conv2d}}(\text{concatenate}(x_t, \eta_t))) \oplus \mathrm{LIF}({\rm{BN}}({\rm{Conv2d}}((x_{0:t}))),\\] where \\(\hat{x}_{t+1}\\) represents the predicted BEV features for the next timestep, \\(\mathrm{LIF}(\cdot)\\) denotes the LIF neuron layer, \\(\text{concatenate}(\cdot)\\) represents the concatenation operation, and \\(\oplus\\) denotes element-wise addition of the outputs from the two LIF layers, and the inner \\(\mathrm{Conv2d}(\cdot)\\) and \\(\mathrm{BN}(\cdot)\\) layers are used to ensure consistency in the output dimensions of the first and second LIF layers.

<figure id="fig:prediction">
<div class="center">
<p><img src="./figures/prediction.png"" style="width:48.0%" />  </p>
</div>
<figcaption>Dual pathway modeling for prediction. Neuron <span class="math inline"><em>a</em></span> captures future multi-modality by incorporating uncertainty distribution. Neuron <span class="math inline"><em>b</em></span> compensates for information gaps using past variations. Inputs <span class="math inline"><em>x</em><sub>1</sub></span> and <span class="math inline"><em>x</em><sub><em>t</em></sub></span> from both pathways are used for the next prediction step.</figcaption>
</figure>

The mixture prediction serves as the basis for the subsequent prediction steps. By recursively applying this dual-pathway prediction method, we obtain the predicted future states \\((\hat{x}_{t+1},\dots, \hat{x}_{t+n})\\). The overall datapath is illustrated in Fig. <a href="#fig:prediction" data-reference-type="ref" data-reference="fig:prediction">4</a>. Following the dual-pathway prediction, all the features are fed into a Spiking ResNet using SA for additional temporal mixing. The historical features \\((x_1, \dots, x_t)\\) and the predicted future features \\((\hat{x}_{t+1},\dots, \hat{x}_{t+n})\\) are then passed to the perception decoder, which consists of multiple output heads to generate various interpretable intermediate representations.

## Planning: Temporal Spiking Temporal for Trajectory Refinement [sec:planning]

The primary objective of the SAD system is to plan a trajectory that is both safe and comfortable, aiming towards a designated target. This is accomplished through a planning pipeline that integrates predicted occupancy grids \\(o\\), map representations \\(m\\), and temporal dynamics of environmental elements like pedestrians.

**Motion Planning.** Initially, our motion planner generates a diverse set of potential trajectories using the bicycle model `\cite{polack2017kinematic}`{=latex}. Among these, the trajectory that minimizes a predefined cost function is selected. This function integrates various factors including learned occupancy probabilities (from segmentation maps generated in the Prediction section) and compliance with traffic regulations to ensure the selected trajectory optimizes for safety and smoothness.

**Cost Function.** The cost function \\(f\\) employed is a multi-component function, where: \\[f(\tau, o, m; w) = f_o(\tau, o, m; w_o) + f_v(\tau; w_v) + f_r(\tau; w_r)
    \label{eq:obj}\\] where \\(w = (w_o, w_v, w_r)\\) represents the learnable parameters associated with each cost component, and \\(\tau\\) denotes a set of trajectory candidates. Specifically, \\(f_o\\) evaluates trajectory compliance with static and dynamic obstacles, \\(f_v\\) is derived from the prediction decoder assessing future states, and \\(f_r\\) addresses metrics of ride comfort and progress towards the goal. The aim is to select the optimal set of trajectories \\(\tau^*\\) that minimize this cost function. This cost function is adapted from Hu et al. `\cite{hu2022st}`{=latex} and is detailed in Appendix <a href="#appendix:cost" data-reference-type="ref" data-reference="appendix:cost">9</a>.

Additionally, trajectories are filtered based on high-level commands (e.g., go forward, turn left, turn right) which tailor the trajectory selection to the immediate navigational intent.

**Optimization with Spiking Gated Recurrent Unit.** Following the initial selection, the "best" trajectory \\(\tau^*\\) undergoes further refinement using a Spiking Gated Recurrent Unit (SGRU), as inspired by `\cite{lotfi2020long}`{=latex}. In this optimization phase, the hidden state \\(h_t\\) of the SGRU incorporates features derived from the front camera’s encoder. The input state \\(x_t\\) is formulated by concatenating the vehicle’s current position, the corresponding position from the selected trajectory \\(\tau^*\\), and the designated target point.

The SGRU model processes inputs as follows: \\[\begin{aligned}
    r_t &= \Theta(W_{ir} x_t + b_{ir} + W_{hr} h_{t-1} + b_{hr}) \\
    z_t &= \Theta(W_{iz} x_t + b_{iz} + W_{hz} h_{t-1} + b_{hz}) \\
    n_t &= \Theta(W_{in} x_t + b_{in} + r_t \odot (W_{hn} h_{t-1} + b_{hn})) \\
    h_t &= (1 - z_t) \odot n_t + z_t \odot h_{t-1}
\end{aligned}\\] where \\(h_t\\) denotes the hidden state at time \\(t\\), \\(x_t\\) represents the input, and \\(r_t\\), \\(z_t\\), \\(n_t\\) are the reset, update, and new candidate state gates respectively. The Heaviside function \\(\Theta\\) ensures sparse and binarized operations in the state of the SGRU, thus preserving the advantages of SNNs.

This optimization step enhances trajectory reliability by mitigating uncertainties inherent in perceptual and predictive analyses, and by integrating dynamic traffic light information directly into the trajectory planning process.

## Overall Loss for End-to-End Learning [sec:method-e2e_learning]

Our model integrates perception, prediction, and planning into a unified framework optimized end-to-end using a composite loss function: \\[\mathcal{L} =  \mathcal{L}_{per} + \alpha \mathcal{L}_{pre} + \beta \mathcal{L}_{pla}\\] where \\(\alpha\\) and \\(\beta\\) are learnable weights. These parameters dynamically adjust to scale the contributions of each task based on the gradients from the respective task losses.

**Perception Loss.** This component is multi-faceted, covering segmentation for current and previous frames, mapping, and auxiliary depth estimation. For semantic segmentation, a top-k cross-entropy loss is employed to effectively handle the large amount of background, non-salient content in the BEV images, following the approach in `\cite{hu2021fiery}`{=latex}. Instance segmentation utilizes an \\(l_2\\) loss for centerness supervision and an \\(l_1\\) loss for offsets and flows. Lane and drivable area predictions are evaluated using a cross-entropy loss.

**Prediction Loss.** Our prediction module extends the perception task by forecasting future semantic and instance segmentation. It adopts the same top-k cross-entropy loss used in perception but applies an exponential discount to future timestamps to account for increasing uncertainty in predictions.

**Planning Loss.** The planning module begins by selecting the initial best trajectory \\(\tau^*\\) from a set of sampled trajectories as defined in Eq. <a href="#eq:obj" data-reference-type="ref" data-reference="eq:obj">[eq:obj]</a>. This trajectory is then refined using an SGRU-based network to produce the final trajectory output \\(\tau_o^*\\). The planning loss comprises two parts: a max-margin loss that differentiates between the expert behavior \\(\tau_h\\) (treated as a positive example) and other sampled trajectories (treated as negatives), and an \\(l_1\\) distance loss that minimizes the deviation between the refined trajectory and the expert trajectory.

Further details on these loss components and the training are provided in the Appendix <a href="#appendix:stage_training" data-reference-type="ref" data-reference="appendix:stage_training">7.2</a>.

<div id="tab:perception" markdown="1">

| Method | Spike | Drivable Area | Lane | Vehicle | Pedestrian |  |
|:---|:---|:---|:---|:---|:---|:---|
| VED `\cite{lu2019monocular}`{=latex}^RAL^ |  | 60.82 | 16.74 | 23.28 | 11.93 |  |
| VPN `\cite{pan2020cross}`{=latex}^RAL^ |  | 65.97 | 17.05 | 28.17 | 10.26 |  |
| PON `\cite{roddick2020predicting}`{=latex}^CVPR^ |  | 63.05 | 17.19 | 27.91 | 13.93 |  |
| Lift-Splat `\cite{philion2020lift}`{=latex}^ECCV^ |  | 72.23 | 19.98 | 31.22 | 15.02 |  |
| IVMP `\cite{wang2021learning}`{=latex}^ICRA^ |  | 74.70 | 20.94 | 34.03 | 17.38 |  |
| FIERY `\cite{hu2021fiery}`{=latex}^ICCV^ |  | 71.97 | 33.58 | 38.00 | 17.15 |  |
| ST-P3 `\cite{hu2022st}`{=latex}^ECCV^ |  | 75.97 | 33.85 | 38.00 | 17.15 |  |
| **SAD (Ours)** |  | 64.74 | 27.78 | 34.82 | 15.12 |  |

**Perception results.** We report the BEV segmentation IoU (%) of intermediate representations and their mean value.

</div>

# Experiments

We evaluate the proposed model using the nuScenes dataset `\cite{caesar2020nuscenes}`{=latex} with 20 epochs with SpikingJelly `\cite{fang2023spikingjelly}`{=latex} framework. For our experiments, we consider \\(1.0s\\) of historical context and predict \\(2.0s\\) into the future, which corresponds to processing 3 past frames and predicting 4 future frames. Additional experimental details are elaborated in the Appendix.

## Experimental Results on nuScenes [sec:res-nuscenes]

**Perception.** Our evaluation focuses on the model’s ability to interpret map representations and perform semantic segmentation. For map representation, we specifically assess the identification of drivable areas and lanes, which are critical for safe navigation as they dictate where the Self-Driving Vehicle (SDV) can travel and help maintain the vehicle’s position within the lanes. Semantic segmentation tests the model’s ability to recognize dynamic objects, such as vehicles and pedestrians, which are pivotal in urban driving scenarios.

We employ the Intersection-over-Union (IoU) metric to quantify the accuracy of our BEV segmentation tasks. The results, as summarized in Tab. <a href="#tab:perception" data-reference-type="ref" data-reference="tab:perception">1</a>, show that our SAD method, which is fully implemented with spiking neural networks (SNNs), competes favorably against state-of-the-art, non-spiking artificial neural networks (ANNs). Notably, our model achieves a superior mean IoU on the nuScenes dataset compared to existing leading methods such as VED `\cite{lu2019monocular}`{=latex}, VPN `\cite{pan2020cross}`{=latex}, PON `\cite{roddick2020predicting}`{=latex}, and Lift-Splat `\cite{philion2020lift}`{=latex}. Specifically, our SAD method outperforms the VED `\cite{lu2019monocular}`{=latex} model by **7.43%** in mean IoU. This enhancement is significant, considering that our network utilizes spiking neurons across all layers, which contributes to greater computational efficiency. Despite the inherent challenges of using SNNs, such as the binary nature of spikes and potential information loss compared to traditional ANNs, our results demonstrate that SAD is capable of delivering competitive perception accuracy in autonomous driving scenarios.

<div id="tab:prediction" markdown="1">

<table>
<caption><strong>Prediction results.</strong> We report semantic segmentation IoU (%) and instance segmentation metrics from the video prediction area. The <em>static</em> method assumes all obstacles static in the prediction horizon.</caption>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td rowspan="2" style="text-align: left;">Spike</td>
<td style="text-align: left;">Future Semantic Seg.</td>
<td colspan="3" style="text-align: center;">Future Instance Seg.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">IoU <span class="math inline">↑</span></td>
<td style="text-align: left;">PQ <span class="math inline">↑</span></td>
<td style="text-align: left;">SQ <span class="math inline">↑</span></td>
<td style="text-align: left;">RQ <span class="math inline">↑</span></td>
</tr>
<tr>
<td style="text-align: left;">Static</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">32.20</td>
<td style="text-align: left;">27.64</td>
<td style="text-align: left;">70.05</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">FIERY <span class="citation" data-cites="hu2021fiery"></span><sup>ICCV</sup></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">37.00</td>
<td style="text-align: left;">30.20</td>
<td style="text-align: left;">70.20</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">ST-P3 <span class="citation" data-cites="hu2022st"></span><sup>ECCV</sup></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">38.63</td>
<td style="text-align: left;">31.72</td>
<td style="text-align: left;">70.15</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><strong>SAD<span> (Ours)</span></strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">32.74</td>
<td style="text-align: left;">20.00</td>
<td style="text-align: left;">68.74</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>

</div>

<div id="tab:planning" markdown="1">

<table>
<caption><strong>Planning results.</strong></caption>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Spike</th>
<th colspan="3" style="text-align: center;">L2 (<span class="math inline"><em>m</em></span>) <span class="math inline">↓</span></th>
<th colspan="3" style="text-align: center;">Collision (%) <span class="math inline">↓</span></th>
<th style="text-align: left;">Energy (mJ)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><span>3-8</span></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">1s</td>
<td style="text-align: left;">2s</td>
<td style="text-align: left;">3s</td>
<td style="text-align: left;">1s</td>
<td style="text-align: left;">2s</td>
<td style="text-align: left;">3s</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">NMP <span class="citation" data-cites="zeng2019end"></span><sup>CVPR</sup></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">0.61</td>
<td style="text-align: left;">1.44</td>
<td style="text-align: left;">3.18</td>
<td style="text-align: left;">0.66</td>
<td style="text-align: left;">0.90</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Freespace <span class="citation" data-cites="hu2021safe"></span><sup>CVPR</sup></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">0.56</td>
<td style="text-align: left;">1.27</td>
<td style="text-align: left;">3.08</td>
<td style="text-align: left;">0.65</td>
<td style="text-align: left;">0.86</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">ST-P3 <span class="citation" data-cites="hu2022st"></span><sup>ECCV</sup></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">1.33</td>
<td style="text-align: left;">2.11</td>
<td style="text-align: left;">2.90</td>
<td style="text-align: left;">0.23</td>
<td style="text-align: left;">0.62</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><strong>SAD<span> (Ours)</span></strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">1.53</td>
<td style="text-align: left;">2.35</td>
<td style="text-align: left;">3.21</td>
<td style="text-align: left;">0.62</td>
<td style="text-align: left;">1.26</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>

</div>

**Prediction.** We assess the predictive capabilities of our model using multiple metrics tailored for video prediction, specifically IoU, existing Panoptic Quality (PQ), Recognition Quality (RQ), and Segmentation Quality (SQ) for evaluating our prediction quality. The definition of these metrics can be found in Appendix. <a href="#appendix:metrics" data-reference-type="ref" data-reference="appendix:metrics">8.2</a>. The results, presented in Tab. <a href="#tab:prediction" data-reference-type="ref" data-reference="tab:prediction">2</a>, demonstrate that while our model does not employ an additional temporal module, the inherent temporal dynamics of spiking neurons facilitate effective information processing. However, our SAD model still shows a gap in performance when compared with state-of-the-art ANN methods.

**Planning.** In the planning domain, we evaluate our model using two primary metrics: L2 error and collision rate. To ensure fairness, the planning horizon is adjusted to \\(3.0s\\). The L2 error measures the deviation between the SDV’s planned trajectory and the human driver’s actual trajectory, providing a quantitative measure of planning accuracy. The collision rate assesses the model’s ability to safely navigate the driving environment without incidents. Results, detailed in Tab. <a href="#tab:planning" data-reference-type="ref" data-reference="tab:planning">3</a>, reveal that our SAD method achieves an L2 error and collision rate comparable to those of the state-of-the-art ANN-based methods, underscoring the safety and reliability of our planning approach.

**Energy Efficiency.** Neuromorphic hardware is able to take advantage of small activation bit-widths and dynamical sparsity `\cite{davies2018loihi}`{=latex}, and as such, SNNs are able to significantly reduce energy consumption during inference \\(-\\) provided there are sufficiently sparse activation patterns amongst spiking neurons. As detailed in Table <a href="#tab:planning" data-reference-type="ref" data-reference="tab:planning">3</a>, we present an estimation of the energy usage of each SOTA model based on dynamical sparsity (detailed calculation methods described in Appendix <a href="#appendix:energy" data-reference-type="ref" data-reference="appendix:energy">10</a>). Owing to the utilization of spiking neurons, our model achieves substantial energy reductions: **7.33 \\(\times\\)** less than the Freespace model `\cite{hu2021safe}`{=latex} and **75.03 \\(\times\\)** lower compared to the ST-P3 model `\cite{hu2022st}`{=latex}. This exceptional energy efficiency makes our model highly suitable for real-world applications.

<div id="tab:abl_ts" markdown="1">

<table>
<caption>Ablation study on different timestep alignment strategies for the encoder and decoder on perception tasks. ‘SR’ denotes repeating the timestep input, ‘SA’ indicates aligning the timestep with the model’s inherent temporal dimension, and ’w/o T’ means eliminating the decoder’s inherent timestep association, resulting in no hidden state connections between timesteps. Our final model adopts the configuration of repeating the timestep in the encoder and aligning the timestep in the decoder.</caption>
<thead>
<tr>
<th colspan="2" style="text-align: center;"><strong>Encoder</strong></th>
<th colspan="3" style="text-align: center;"><strong>Decoder</strong></th>
<th colspan="5" style="text-align: center;"><strong>Results</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><strong>SR</strong></td>
<td style="text-align: center;"><strong>SA</strong></td>
<td style="text-align: center;"><strong>SR</strong></td>
<td style="text-align: center;"><strong>SA</strong></td>
<td style="text-align: center;"><strong>w/o T</strong></td>
<td style="text-align: center;"><strong>Drivable</strong></td>
<td style="text-align: center;"><strong>Lane</strong></td>
<td style="text-align: center;"><strong>Vehicle</strong></td>
<td style="text-align: center;"><strong>Pedestrian</strong></td>
<td style="text-align: center;"><strong>Avg.</strong></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">43.84</td>
<td style="text-align: center;">15.25</td>
<td style="text-align: center;">4.41</td>
<td style="text-align: center;">1.62</td>
<td style="text-align: center;">16.28</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.11</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">61.80</td>
<td style="text-align: center;">20.92</td>
<td style="text-align: center;">31.78</td>
<td style="text-align: center;">13.46</td>
<td style="text-align: center;">31.99</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>61.81</strong></td>
<td style="text-align: center;"><strong>25.31</strong></td>
<td style="text-align: center;"><strong>33.89</strong></td>
<td style="text-align: center;"><strong>15.24</strong></td>
<td style="text-align: center;"><strong>34.06</strong></td>
</tr>
</tbody>
</table>

</div>

## Visualization

To evaluate the effectiveness of our SAD model in a more interpretable manner, we provide qualitative results using the nuScenes dataset. Fig. <a href="#fig:visual1" data-reference-type="ref" data-reference="fig:visual1">5</a> illustrates the outputs of our model. The SAD model effectively generates a safe trajectory that enables straight-ahead motion while avoiding collisions with curbsides and the vehicle ahead. Additionally, we conducted a comparative analysis with the ANN model, which demonstrated that our SAD model can achieve comparable performance, ensuring accurate and reliable planning outcomes.More visual results can be found in the Appendix <a href="#appendix:visual" data-reference-type="ref" data-reference="appendix:visual">8.3</a>.

<figure id="fig:visual1">
<img src="./figures/visual1.png"" />
<figcaption><strong>Qualitative Results of the SAD Model on the nuScenes Dataset.</strong> (a) displays six camera view inputs utilized by the model. (b) illustrates the planning result of the ANN model, and (c) presents the planning results of our SAD model. The comparison shows that our SAD model can achieve performance comparable to that of the ANN model and successfully generate a safe trajectory.</figcaption>
</figure>

## Ablation Study

### Timestep Strategy [sec:abl-timestep]

The large space of architecture design decisions came with a large number of ablation studies before determining the best performing model. We conducted an ablation study to examine the effects of different timestep alignment strategies on the performance of the encoder and decoder in perception tasks. The strategies include ‘SR’ for the repetition of vision inputs over timesteps, ‘SA’ where sequential inputs are aligned with the model’s inherent temporal dimension, and ’w/o T’ where recurrent dynamics in the decoder are removed, thus disconnecting the hidden states between timesteps. The results are summarized in Tab. <a href="#tab:abl_ts" data-reference-type="ref" data-reference="tab:abl_ts">4</a>. The last row is the baseline configuration of our model that serves as a reference point for these experiments. All ablation experiments are run for 5 epochs.

**Impact of Timestep Repetition in Encoder.** (Row 1) When repeating timesteps in the encoder which was pre-trained on the ImageNet-1K dataset for classification tasks, we notice a substantial benefit. This approach leverages the encoder’s ability to capture spatial information through repeated images, a technique effective during its pre-training phase. Adopting this strategy in the SAD model enhances perception performance by maintaining consistency with the pre-training approach.

**Impact of Timestep Alignment in Decoder.** (Row 2) In contrast to the encoder, the decoder, which is trained from scratch, benefits from aligning timesteps with the model’s inherent temporal dimension (‘SA’). This strategy leverages the decoder’s capacity to mix temporal information, improving performance over the repeat strategy (‘SR’), which fails to show any performance gains.

**Single Timestep Input.** (Row 3) The purpose of this experiment was to study how well spiking neurons perform as temporal mixers. In this setup, all inputs are processed in one timestep without inter-frame connections, leading to a slight performance decrease compared to our baseline. Therefore, it confirms that spiking neurons inherently possess temporal processing capabilities.

### Effectiveness of Different Modules

Tab. <a href="#tab:abl_md" data-reference-type="ref" data-reference="tab:abl_md">5</a> outlines the results of an ablation study that assesses the impact of various structural modifications to the encoder and decoder on planning tasks. The study differentiates between ‘MS’ for MS-ResNet structure `\cite{hu2024advancing}`{=latex}, ‘SEW’ for SEW-ResNet structure `\cite{fang2021deep}`{=latex}, ‘SP’ for single pathway model, and ‘DP’ for dual pathway model. We analyze the planning performance of each configuration. The last row represents the configuration of our final model, which we use as the baseline for comparison.

**Decoder Structure Evaluation.** (Row 1) This part of the study compares the MS-ResNet and SEW-ResNet structures in their roles as decoders. Each structure possesses unique firing patterns, which are discussed in detail in the Appendix <a href="#appendix:ResNet" data-reference-type="ref" data-reference="appendix:ResNet">6</a>. Our results indicate that the MS-ResNet structure is more effective for planning tasks in autonomous driving, likely due to its enhanced capability to handle the spatial-temporal dynamics required in this context.

**Pathway Model Comparison.** (Row 2) Here, we explore the performance difference between single and dual pathway prediction models. The dual pathway model integrates information through two distinct processing streams. Our experimental results show that the dual pathway model significantly outperforms the single pathway model in planning tasks.

<div id="tab:abl_md" markdown="1">

<table>
<caption>Ablation Study on different modules for the encoder and the decoder on Planning tasks. ‘MS’ denotes the MS-ResNet structure, SEW denotes the SEW-ResNet structure, ‘SP’ means the single pathway model, and ‘DP’ means the dual pathway model.</caption>
<thead>
<tr>
<th colspan="2" style="text-align: center;"><strong>Decoder</strong></th>
<th colspan="2" style="text-align: center;"><strong>Prediction</strong></th>
<th colspan="3" style="text-align: center;"><strong>Results</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><strong>MS</strong></td>
<td style="text-align: center;"><strong>SEW</strong></td>
<td style="text-align: center;"><strong>SP</strong></td>
<td style="text-align: center;"><strong>DP</strong></td>
<td style="text-align: center;"><strong>PQ</strong></td>
<td style="text-align: center;"><strong>SQ</strong></td>
<td style="text-align: center;"><strong>RQ</strong></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">59.28</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.44</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">67.55</td>
<td style="text-align: center;">13.35</td>
<td style="text-align: center;">19.77</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>68.16</strong></td>
<td style="text-align: center;"><strong>16.57</strong></td>
<td style="text-align: center;"><strong>24.11</strong></td>
</tr>
</tbody>
</table>

</div>

# Conclusion

This work presented Spiking Autonomous Driving (SAD), a fully spike-driven framework that seamlessly integrates perception, prediction, and planning into one coherent model.  By replacing every floating-point activation with event-driven spikes, SAD maintains competitive task-level performance on nuScenes while realising orders-of-magnitude gains in computational efficiency.  The unified neuromorphic design further promotes remarkably consistent behaviour under the broad spectrum of urban driving situations encountered in our evaluation, underscoring the innate robustness of spike-based computation for safety-critical control.  These results demonstrate that energy-efficient SNNs can already shoulder the full decision stack of autonomous vehicles without sacrificing reliability, making them an immediately viable foundation for next-generation, eco-friendly self-driving platforms.  We believe SAD will catalyse future research on large-scale neuromorphic solutions and accelerate their transition from laboratory prototypes to production-grade autonomous systems.
# Acknowledgements [acknowledgements]

This work was supported in part by National Science Foundation (NSF) awards CNS-1730158, ACI-1540112, ACI-1541349, OAC-1826967, OAC-2112167, CNS-2100237, CNS-2120019, the University of California Office of the President, and the University of California San Diego’s California Institute for Telecommunications and Information Technology/Qualcomm Institute. Thanks to CENIC for the 100Gbps networks. This project was also supported in-part by the National Science Foundation RCN-SC 2332166.

# References [references]

<div class="thebibliography" markdown="1">

SAE Committee Taxonomy and definitions for terms related to on-road motor vehicle automated driving systems 2014. (@SAE2014)

Kshitij Bhardwaj, Zishen Wan, Arijit Raychowdhury, and Ryan Goldhahn Real-time fully unsupervised domain adaptation for lane detection in autonomous driving In *2023 Design, Automation & Test in Europe Conference & Exhibition (DATE)*, pages 1–2, 2023. **Abstract:** While deep neural networks are being utilized heavily for autonomous driving, they need to be adapted to new unseen environmental conditions for which they were not trained. We focus on a safety critical application of lane detection, and propose a lightweight, fully unsupervised, real-time adaptation approach that only adapts the batch-normalization parameters of the model. We demonstrate that our technique can perform inference, followed by on-device adaptation, under a tight constraint of 30 FPS on Nvidia Jetson Orin. It shows similar accuracy (avg. of 92.19%) as a state-of-the-art semi-supervised adaptation algorithm but which does not support real-time adaptation. (@powerbudget)

Soumya Sudhakar, Vivienne Sze, and Sertac Karaman Data centers on wheels: Emissions from computing onboard autonomous vehicles , 43(1):29–39, 2022. **Abstract:** While much attention has been paid to data centers’ greenhouse gas emissions, less attention has been paid to autonomous vehicles’ (AVs) potential emissions. In this work, we introduce a framework to probabilistically model the emissions from computing onboard a global fleet of AVs and show that the emissions have the potential to make a nonnegligible impact on global emissions, comparable to that of all data centers today. Based on current trends, a widespread AV adoption scenario where approximately 95% of all vehicles are autonomous requires computer power to be less than 1.2 kW for emissions from computing on AVs to be less than emissions from all data centers in 2018 in 90% of modeled scenarios. Anticipating a future scenario with high adoption of AVs, business-as-usual decarbonization, and workloads doubling every three years, hardware efficiency must double every 1.1 years for emissions in 2050 to equal 2018 data center emissions. The rate of increase in hardware efficiency needed in many scenarios to contain emissions is faster than the current rate. We discuss several avenues of future research unique to AVs to further analyze and potentially reduce the carbon footprint of AVs. (@sudhakar2022data)

Kaushik Roy, Akhilesh Jaiswal, and Priyadarshini Panda Towards spike-based machine intelligence with neuromorphic computing , 575(7784):607–617, 2019. **Abstract:** Biologically plausible learning with neuronal dendrites is a promising perspective to improve the spike-driven learning capability by introducing dendritic processing as an additional hyperparameter. Neuromorphic computing is an effective and essential solution towards spike-based machine intelligence and neural learning systems. However, on-line learning capability for neuromorphic models is still an open challenge. In this study a novel neuromorphic architecture with dendritic on-line learning (NADOL) is presented, which is a novel efficient methodology for brain-inspired intelligence on embedded hardware. With the feature of distributed processing using spiking neural network, NADOL can cut down the power consumption and enhance the learning efficiency and convergence speed. A detailed analysis for NADOL is presented, which demonstrates the effects of different conditions on learning capabilities, including neuron number in hidden layer, dendritic segregation parameters, feedback connection, and connection sparseness with various levels of amplification. Piecewise linear approximation approach is used to cut down the computational resource cost. The experimental results demonstrate a remarkable learning capability that surpasses other solutions, with NADOL exhibiting superior performance over the GPU platform in dendritic learning. This study’s applicability extends across diverse domains, including the Internet of Things, robotic control, and brain-machine interfaces. Moreover, it signifies a pivotal step in bridging the gap between artificial intelligence and neuroscience through the introduction of an innovative neuromorphic paradigm. (@roy2019towards)

Jason K Eshraghian, Max Ward, Emre O Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D Lu Training spiking neural networks using lessons from deep learning , 2023. **Abstract:** The brain is the perfect place to look for inspiration to develop more efficient neural networks. The inner workings of our synapses and neurons provide a glimpse at what the future of deep learning might look like. This article serves as a tutorial and perspective showing how to apply the lessons learned from several decades of research in deep learning, gradient descent, backpropagation, and neuroscience to biologically plausible spiking neural networks (SNNs). We also explore the delicate interplay between encoding data as spikes and the learning process; the challenges and solutions of applying gradient-based learning to SNNs; the subtle link between temporal backpropagation and spike timing-dependent plasticity; and how deep learning might move toward biologically plausible online learning. Some ideas are well accepted and commonly used among the neuromorphic engineering community, while others are presented or justified for the first time here. A series of companion interactive tutorials complementary to this article using our Python package, \<italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"\>snnTorch\</i\> , are also made available: https://snntorch.readthedocs.io/en/latest/tutorials/index.html. (@eshraghian2023training)

Guoqi Li, Lei Deng, Huajing Tang, Gang Pan, Yonghong Tian, Kaushik Roy, and Wolfgang Maass Brain inspired computing: A systematic survey and future trends , 2023. **Abstract:** &lt;p&gt;Brain Inspired Computing (BIC) is an emerging research field that aims to build fundamental theories, models, hardware architectures, and application systems toward more general Artificial Intelligence (AI) by learning from the information processing mechanisms or structures/functions of biological nervous systems. It is regarded as one of the most promising research directions for future intelligent computing in the post-Moore era. In the past few years, various new schemes in this field have sprung up to explore more general AI. These works are quite divergent in the aspects of modeling/algorithm, software tool, hardware platform, and benchmark data, since BIC is an interdisciplinary field that consists of many different domains, including computational neuroscience, artificial intelligence, computer science, statistical physics, material science, microelectronics and so forth. This situation greatly impedes researchers from obtaining a clear picture and getting started in the right way. Hence, there is an urgent requirement to do a comprehensive survey in this field to help correctly recognize and analyze such bewildering methodologies. What are the key issues to enhance the development of BIC? What roles do the current mainstream technologies play in the general framework of BIC? Which techniques are truly useful in real-world applications? These questions largely remain open. To address the above issues, in this survey we first clarify the biggest challenge of BIC: how can AI models benefit from the recent advancements in computational neuroscience? With this challenge in mind, we will focus on discussing the concept of BIC and summarize four components of BIC infrastructure development: 1) modeling/algorithm; 2) hardware platform; 3) software tool; and 4) benchmark data. For each component, we will summarize its recent progress, main challenges to resolve, and future trends. On the basis of these studies, we present a general framework for the real-world applications of BIC systems, which is promising to benefit both AI and brain science. Finally, we claim that it is extremely important to build a research ecology to promote prosperity continuously in this field.&lt;/p&gt; (@li2023brain)

Wolfgang Maass Networks of spiking neurons: the third generation of neural network models , 10(9):1659–1671, 1997. (@maass1997networks)

Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, Pradip Joshi, Nabil Imam, Shweta Jain, et al Loihi: A neuromorphic manycore processor with on-chip learning , 38(1):82–99, 2018. **Abstract:** Loihi is a 60-mm2 chip fabricated in Intels 14-nm process that advances the state-of-the-art modeling of spiking neural networks in silicon. It integrates a wide range of novel features for the field, such as hierarchical connectivity, dendritic compartments, synaptic delays, and, most importantly, programmable synaptic learning rules. Running a spiking convolutional form of the Locally Competitive Algorithm, Loihi can solve LASSO optimization problems with over three orders of magnitude superior energy-delay-product compared to conventional solvers running on a CPU iso-process/voltage/area. This provides an unambiguous example of spike-based computation, outperforming all known conventional solutions. (@davies2018loihi)

Mostafa Rahimi Azghadi, Corey Lammie, Jason K Eshraghian, Melika Payvand, Elisa Donati, Bernabe Linares-Barranco, and Giacomo Indiveri Hardware implementation of deep network accelerators towards healthcare and biomedical applications , 14(6):1138–1159, 2020. **Abstract:** The advent of dedicated Deep Learning (DL) accelerators and neuromorphic processors has brought on new opportunities for applying both Deep and Spiking Neural Network (SNN) algorithms to healthcare and biomedical applications at the edge.This can facilitate the advancement of medical Internet of Things (IoT) systems and Point of Care (PoC) devices.In this paper, we provide a tutorial describing how various technologies including emerging memristive devices, Field Programmable Gate Arrays (FPGAs), and Complementary Metal Oxide Semiconductor (CMOS) can be used to develop efficient DL accelerators to solve a wide variety of diagnostic, pattern recognition, and signal processing problems in healthcare.Furthermore, we explore how spiking neuromorphic processors can complement their DL counterparts for processing biomedical signals.The tutorial is augmented with case studies of the vast literature on neural network and neuromorphic hardware as applied to the healthcare domain.We benchmark various hardware platforms by performing a sensor fusion signal processing task combining electromyography (EMG) signals with computer vision.Comparisons are made between dedicated neuromorphic processors and embedded AI accelerators in terms of inference latency and energy.Finally, we provide our analysis of the field and share a perspective on the advantages, disadvantages, challenges, and opportunities that various accelerators and neuromorphic processors introduce to healthcare and biomedical domains. (@azghadi2020hardware)

Charlotte Frenkel, Jean-Didier Legat, and David Bol Morphic: A 65-nm 738k-synapse/mm quad-core binary-weight digital neuromorphic processor with stochastic spike-driven online learning , 13(5):999–1010, 2019. **Abstract:** Recent trends in the field of neural network accelerators investigate weight quantization as a means to increase the resourceand power-efficiency of hardware devices. As full on-chip weight storage is necessary to avoid the high energy cost of off-chip memory accesses, memory reduction requirements for weight storage pushed toward the use of binary weights, which were demonstrated to have a limited accuracy reduction on many applications when quantization-aware training techniques are used. In parallel, spiking neural network (SNN) architectures are explored to further reduce power when processing sparse event-based data streams, while on-chip spike-based online learning appears as a key feature for applications constrained in power and resources during the training phase. However, designing powerand area-efficient SNNs still requires the development of specific techniques in order to leverage on-chip online learning on binary weights without compromising the synapse density. In this paper, we demonstrate MorphIC, a quad-core binary-weight digital neuromorphic processor embedding a stochastic version of the spike-driven synaptic plasticity (S-SDSP) learning rule and a hierarchical routing fabric for large-scale chip interconnection. The MorphIC SNN processor embeds a total of 2k leaky integrate-and-fire (LIF) neurons and more than two million plastic synapses for an active silicon area of 2.86 mm \<sup xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"\>2\</sup\> in 65-nm CMOS, achieving a high density of 738k synapses/mm \<sup xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"\>2\</sup\> . MorphIC demonstrates an order-of-magnitude improvement in the area-accuracy tradeoff on the MNIST classification task compared to previously-proposed SNNs, while having no penalty in the energy-accuracy tradeoff. (@frenkel2019morphic)

Fabrizio Ottati, Chang Gao, Qinyu Chen, Giovanni Brignone, Mario R Casu, Jason K Eshraghian, and Luciano Lavagno To spike or not to spike: A digital hardware perspective on deep learning acceleration , 2023. **Abstract:** As deep learning models scale, they become increasingly competitive from domains spanning from computer vision to natural language processing; however, this happens at the expense of efficiency since they require increasingly more memory and computing power. The power efficiency of the biological brain outperforms any large-scale deep learning (DL) model; thus, neuromorphic computing tries to mimic the brain operations, such as spike-based information processing, to improve the efficiency of DL models. Despite the benefits of the brain, such as efficient information transmission, dense neuronal interconnects, and the co-location of computation and memory, the available biological substrate has severely constrained the evolution of biological brains. Electronic hardware does not have the same constraints; therefore, while modeling spiking neural networks (SNNs) might uncover one piece of the puzzle, the design of efficient hardware backends for SNNs needs further investigation, potentially taking inspiration from the available work done on the artificial neural networks (ANNs) side. As such, when is it wise to look at the brain while designing new hardware, and when should it be ignored? To answer this question, we quantitatively compare the digital hardware acceleration techniques and platforms of ANNs and SNNs. As a result, we provide the following insights: (i) ANNs currently process static data more efficiently, (ii) applications targeting data produced by neuromorphic sensors, such as event-based cameras and silicon cochleas, need more investigation since the behavior of these sensors might naturally fit the SNN paradigm, and (iii) hybrid approaches combining SNNs and ANNs might lead to the best solutions and should be investigated further at the hardware level, accounting for both efficiency and loss optimization. (@ottati2023spike)

Wei Fang, Zhaofei Yu, Yanqi Chen, Timothée Masquelier, Tiejun Huang, and Yonghong Tian Incorporating learnable membrane time constant to enhance learning of spiking neural networks In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, pages 2661–2671, October 2021. **Abstract:** Spiking Neural Networks (SNNs) have attracted enormous research interest due to temporal information processing capability, low power consumption, and high biological plausibility. However, the formulation of efficient and high-performance learning algorithms for SNNs is still challenging. Most existing learning methods learn weights only, and require manual tuning of the membrane-related parameters that determine the dynamics of a single spiking neuron. These parameters are typically chosen to be the same for all neurons, which limits the diversity of neurons and thus the expressiveness of the resulting SNNs. In this paper, we take inspiration from the observation that membrane-related parameters are different across brain regions, and propose a training algorithm that is capable of learning not only the synaptic weights but also the membrane time constants of SNNs. We show that incorporating learnable membrane time constants can make the network less sensitive to initial values and can speed up learning. In addition, we reevaluate the pooling methods in SNNs and find that max-pooling will not lead to significant information loss and have the advantage of low computation cost and binary compatibility. We evaluate the proposed method for image classification tasks on both traditional static MNIST, Fashion-MNIST, CIFAR-10 datasets, and neuromorphic N-MNIST, CIFAR10-DVS, DVS128 Gesture datasets. The experiment results show that the proposed method outperforms the state-of-the-art accuracy on nearly all datasets, using fewer time-steps. Our codes are available at https://github.com/fangwei123456/Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron. (@Fang_2021_ICCV)

Man Yao, Jiakui Hu, Zhaokun Zhou, Li Yuan, Yonghong Tian, Bo Xu, and Guoqi Li Spike-driven transformer , 36, 2024. **Abstract:** Spiking Neural Networks (SNNs) provide an energy-efficient deep learning option due to their unique spike-based event-driven (i.e., spike-driven) paradigm. In this paper, we incorporate the spike-driven paradigm into Transformer by the proposed Spike-driven Transformer with four unique properties: 1) Event-driven, no calculation is triggered when the input of Transformer is zero; 2) Binary spike communication, all matrix multiplications associated with the spike matrix can be transformed into sparse additions; 3) Self-attention with linear complexity at both token and channel dimensions; 4) The operations between spike-form Query, Key, and Value are mask and addition. Together, there are only sparse addition operations in the Spike-driven Transformer. To this end, we design a novel Spike-Driven Self-Attention (SDSA), which exploits only mask and addition operations without any multiplication, and thus having up to $87.2\\}times$ lower computation energy than vanilla self-attention. Especially in SDSA, the matrix multiplication between Query, Key, and Value is designed as the mask operation. In addition, we rearrange all residual connections in the vanilla Transformer before the activation functions to ensure that all neurons transmit binary spike signals. It is shown that the Spike-driven Transformer can achieve 77.1\\}% top-1 accuracy on ImageNet-1K, which is the state-of-the-art result in the SNN field. The source code is available at https://github.com/BICLab/Spike-Driven-Transformer. (@yao2024spike)

Zhaokun Zhou, Yuesheng Zhu, Chao He, Yaowei Wang, Shuicheng Yan, Yonghong Tian, and Li Yuan Spikformer: When spiking neural network meets transformer , 2022. **Abstract:** We consider two biologically plausible structures, the Spiking Neural Network (SNN) and the self-attention mechanism. The former offers an energy-efficient and event-driven paradigm for deep learning, while the latter has the ability to capture feature dependencies, enabling Transformer to achieve good performance. It is intuitively promising to explore the marriage between them. In this paper, we consider leveraging both self-attention capability and biological properties of SNNs, and propose a novel Spiking Self Attention (SSA) as well as a powerful framework, named Spiking Transformer (Spikformer). The SSA mechanism in Spikformer models the sparse visual feature by using spike-form Query, Key, and Value without softmax. Since its computation is sparse and avoids multiplication, SSA is efficient and has low computational energy consumption. It is shown that Spikformer with SSA can outperform the state-of-the-art SNNs-like frameworks in image classification on both neuromorphic and static datasets. Spikformer (66.3M parameters) with comparable size to SEW-ResNet-152 (60.2M,69.26%) can achieve 74.81% top1 accuracy on ImageNet using 4 time steps, which is the state-of-the-art in directly trained SNNs models. (@zhou2022spikformer)

Rui-Jie Zhu, Qihang Zhao, Tianjing Zhang, Haoyu Deng, Yule Duan, Malu Zhang, and Liang-Jian Deng Tcja-snn: Temporal-channel joint attention for spiking neural networks , 2022. **Abstract:** Spiking Neural Networks (SNNs) are attracting widespread interest due to their biological plausibility, energy efficiency, and powerful spatio-temporal information representation ability. Given the critical role of attention mechanisms in enhancing neural network performance, the integration of SNNs and attention mechanisms exhibits potential to deliver energy-efficient and high-performance computing paradigms. We present a novel Temporal-Channel Joint Attention mechanism for SNNs, referred to as TCJA-SNN. The proposed TCJA-SNN framework can effectively assess the significance of spike sequence from both spatial and temporal dimensions. More specifically, our essential technical contribution lies on: 1) We employ the squeeze operation to compress the spike stream into an average matrix. Then, we leverage two local attention mechanisms based on efficient 1D convolutions to facilitate comprehensive feature extraction at the temporal and channel levels independently. 2) We introduce the Cross Convolutional Fusion (CCF) layer as a novel approach to model the inter-dependencies between the temporal and channel scopes. This layer breaks the independence of these two dimensions and enables the interaction between features. Experimental results demonstrate that the proposed TCJA-SNN outperforms SOTA by up to 15.7% accuracy on standard static and neuromorphic datasets, including Fashion-MNIST, CIFAR10-DVS, N-Caltech 101, and DVS128 Gesture. Furthermore, we apply the TCJA-SNN framework to image generation tasks by leveraging a variation autoencoder. To the best of our knowledge, this study is the first instance where the SNN-attention mechanism has been employed for image classification and generation tasks. Notably, our approach has achieved SOTA performance in both domains, establishing a significant advancement in the field. Codes are available at https://github.com/ridgerchu/TCJA. (@zhu2022tcja)

Ziqing Wang, Yuetong Fang, Jiahang Cao, Qiang Zhang, Zhongrui Wang, and Renjing Xu Masked spiking transformer In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 1761–1771, 2023. **Abstract:** The combination of Spiking Neural Networks (SNNs) and Transformers has attracted significant attention due to their potential for high energy efficiency and high-performance nature. However, existing works on this topic typically rely on direct training, which can lead to suboptimal performance. To address this issue, we propose to leverage the benefits of the ANN-to-SNN conversion method to combine SNNs and Transformers, resulting in significantly improved performance over existing state-of-the-art SNN models. Furthermore, inspired by the quantal synaptic failures observed in the nervous system, which reduce the number of spikes transmitted across synapses, we introduce a novel Masked Spiking Transformer (MST) framework. This incorporates a Random Spike Masking (RSM) method to prune redundant spikes and reduce energy consumption without sacrificing performance. Our experimental results demonstrate that the proposed MST model achieves a significant reduction of 26.8% in power consumption when the masking ratio is 75% while maintaining the same level of performance as the unmasked model. The code is available at: https://github.com/bic-L/Masked-Spiking-Transformer. (@wang2023masked)

Ziqing Wang, Qidong Zhao, Jinku Cui, Xu Liu, and Dongkuan Xu Autost: Training-free neural architecture search for spiking transformers In *ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*, pages 3455–3459. IEEE, 2024. **Abstract:** Spiking Transformers have gained considerable attention because they achieve both the energy efficiency of Spiking Neural Networks (SNNs) and the high capacity of Transformers. However, the existing Spiking Transformer architectures, derived from Artificial Neural Networks (ANNs), exhibit a notable architectural gap, resulting in suboptimal performance compared to their ANN counterparts. Manually discovering optimal architectures is time-consuming. To address these limitations, we introduce AutoST, a training-free NAS method for Spiking Transformers, to rapidly identify high-performance Spiking Transformer architectures. Unlike existing training-free NAS methods, which struggle with the non-differentiability and high sparsity inherent in SNNs, we propose to utilize Floating-Point Operations (FLOPs) as a performance metric, which is independent of model computations and training dynamics, leading to a stronger correlation with performance. Our extensive experiments show that AutoST models outperform state-of-the-art manually or automatically designed SNN architectures on static and neuromorphic datasets. Full code, model, and data are released for reproduction. \<sup xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"\>1\</sup\> (@wang2024autost)

Xuerui Qiu, Rui-Jie Zhu, Yuhong Chou, Zhaorui Wang, Liang-jian Deng, and Guoqi Li Gated attention coding for training high-performance and efficient spiking neural networks In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 38, pages 601–610, 2024. **Abstract:** Spiking neural networks (SNNs) are emerging as an energy-efficient alternative to traditional artificial neural networks (ANNs) due to their unique spike-based event-driven nature. Coding is crucial in SNNs as it converts external input stimuli into spatio-temporal feature sequences. However, most existing deep SNNs rely on direct coding that generates powerless spike representation and lacks the temporal dynamics inherent in human vision. Hence, we introduce Gated Attention Coding (GAC), a plug-and-play module that leverages the multi-dimensional gated attention unit to efficiently encode inputs into powerful representations before feeding them into the SNN architecture. GAC functions as a preprocessing layer that does not disrupt the spike-driven nature of the SNN, making it amenable to efficient neuromorphic hardware implementation with minimal modifications. Through an observer model theoretical analysis, we demonstrate GAC’s attention mechanism improves temporal dynamics and coding efficiency. Experiments on CIFAR10/100 and ImageNet datasets demonstrate that GAC achieves state-of-the-art accuracy with remarkable efficiency. Notably, we improve top-1 accuracy by 3.10% on CIFAR100 with only 6-time steps and 1.07% on ImageNet while reducing energy usage to 66.9% of the previous works. To our best knowledge, it is the first time to explore the attention-based dynamic coding scheme in deep SNNs, with exceptional effectiveness and efficiency on large-scale datasets. Code is available at https://github.com/bollossom/GAC. (@qiu2024gated)

Haoyu Deng, Ruijie Zhu, Xuerui Qiu, Yule Duan, Malu Zhang, and Liang-Jian Deng Tensor decomposition based attention module for spiking neural networks , 295:111780, 2024. **Abstract:** perspective, thereby identifying the shortcomings and limitations of previous methods. By expressing the composition of attention maps in previous methods through CANDECOMP/PARAFAC (CP) decomposition form, we found that they are essentially ‘rank-1’ methods. When applied to various tasks, they lack a certain degree of flexibility. In this paper, we introduce a module called Projected-Full Attention (PFA), which is capable of generating attention maps with a rank other than 1. Specifically, PFA consists of two components: Linear Projection of Spike Tensor (LPST) and Attention Map Com- posing (AMC). LPST is responsible for generating 𝑅sets of projections from the input tensor using a small number of parameters. AMC is responsible for creating attention maps using these 𝑅sets of projections, where 𝑅is called The connecting factor representing the rank concept of the CP decomposition and is a hyper-parameter that can be adjusted based on the specific task. The key contributions of this paper are outlined as follows: • We propose PFA, a module that can achieve temporal- channel-spatial attention. In contrast to previous ap- proaches, the ranks of the attention maps generated by PFA are not fixed at 1; instead, the rank can be chosen based on the specific task. Furthermore, the parameter count of PFA grows linearly with the data scale, and the computational cost is equivalent to that of a single standard convolution layer. • We present a comprehensive theoretical analysis, fo- cusing on two critical aspects: the rank of tensors and𝑅, the connecting factor. These analyses lead to a tailored selection criterion, providing effective insights into experimental outcomes and offering a valuable guidance for future applications. • We conduct thorough experiments on both static and dynamic datasets. The results demonstrate the ef- fectiveness of PFA which achieves state-of-the-art (SOTA) accuracy on both dynamic and static datasets. Ablation studies further validate the structural rea- sonableness of PFA. Additionally, we visualize theattention map generated by PFA to intuitively illus- trate attention distribution across temporal, channel, and spatial dimensions. 2. Related Works Spiking Neural Network: Spiking Neural Networks (SNNs), the third generation of neural networks \[19; 36\], offer a closer emulation of the human brain’s efficiency by utilizing discrete spikes for information transmission \[64\]. This unique method of communication allows for enhanced energy-savi (@deng2024tensor)

Yimeng Shan, Xuerui Qiu, Rui-jie Zhu, Ruike Li, Meng Wang, and Haicheng Qu Or residual connection achieving comparable accuracy to add residual connection in deep residual spiking neural networks , 2023. **Abstract:** Spiking Neural Networks (SNNs) have garnered substantial attention in brain-like computing for their biological fidelity and the capacity to execute energy-efficient spike-driven operations. As the demand for heightened performance in SNNs surges, the trend towards training deeper networks becomes imperative, while residual learning stands as a pivotal method for training deep neural networks. In our investigation, we identified that the SEW-ResNet, a prominent representative of deep residual spiking neural networks, incorporates non-event-driven operations. To rectify this, we propose a novel training paradigm that first accumulates a large amount of redundant information through OR Residual Connection (ORRC), and then filters out the redundant information using the Synergistic Attention (SynA) module, which promotes feature extraction in the backbone while suppressing the influence of noise and useless features in the shortcuts. When integrating SynA into the network, we observed the phenomenon of "natural pruning", where after training, some or all of the shortcuts in the network naturally drop out without affecting the model’s classification accuracy. This significantly reduces computational overhead and makes it more suitable for deployment on edge devices. Experimental results on various public datasets confirmed that the SynA-ResNet achieved single-sample classification with as little as 0.8 spikes per neuron. Moreover, when compared to other residual SNN models, it exhibited higher accuracy and up to a 28-fold reduction in energy consumption. (@shan2023or)

Qiaoyi Su, Yuhong Chou, Yifan Hu, Jianing Li, Shijie Mei, Ziyang Zhang, and Guoqi Li Deep directly-trained spiking neural networks for object detection In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 6555–6565, 2023. **Abstract:** Spiking neural networks (SNNs) are brain-inspired energy-efficient models that encode information in spatiotemporal dynamics. Recently, deep SNNs trained directly have shown great success in achieving high performance on classification tasks with very few time steps. However, how to design a directly-trained SNN for the regression task of object detection still remains a challenging problem. To address this problem, we propose EMS-YOLO, a novel directly-trained SNN framework for object detection, which is the first trial to train a deep SNN with surrogate gradients for object detection rather than ANN-SNN conversion strategies. Specifically, we design a full-spike residual block, EMS-ResNet, which can effectively extend the depth of the directly-trained SNN with low power consumption. Furthermore, we theoretically analyze and prove the EMS-ResNet could avoid gradient vanishing or exploding. The results demonstrate that our approach outperforms the state-of-the-art ANN-SNN conversion methods (at least 500 time steps) in extremely fewer time steps (only 4 time steps). It is shown that our model could achieve comparable performance to the ANN with the same architecture while consuming 5.83× less energy on the frame-based COCO Dataset and the event-based Gen1 Dataset. Our code is available in https://github.com/BICLab/EMS-YOLO. (@su2023yolo)

Seijoon Kim, Seongsik Park, Byunggook Na, and Sungroh Yoon Spiking-yolo: spiking neural network for energy-efficient object detection In *Proceedings of the AAAI conference on artificial intelligence*, volume 34, pages 11270–11277, 2020. **Abstract:** Over the past decade, deep neural networks (DNNs) have demonstrated remarkable performance in a variety of applications. As we try to solve more advanced problems, increasing demands for computing and power resources has become inevitable. Spiking neural networks (SNNs) have attracted widespread interest as the third-generation of neural networks due to their event-driven and low-powered nature. SNNs, however, are difficult to train, mainly owing to their complex dynamics of neurons and non-differentiable spike operations. Furthermore, their applications have been limited to relatively simple tasks such as image classification. In this study, we investigate the performance degradation of SNNs in a more challenging regression problem (i.e., object detection). Through our in-depth analysis, we introduce two novel methods: channel-wise normalization and signed neuron with imbalanced threshold, both of which provide fast and accurate information transmission for deep SNNs. Consequently, we present a first spiked-based object detection model, called Spiking-YOLO. Our experiments show that Spiking-YOLO achieves remarkable results that are comparable (up to 98%) to those of Tiny YOLO on non-trivial datasets, PASCAL VOC and MS COCO. Furthermore, Spiking-YOLO on a neuromorphic chip consumes approximately 280 times less energy than Tiny YOLO and converges 2.3 to 4 times faster than previous SNN conversion methods. (@kim2020spikingyolo)

Man Yao, JiaKui Hu, Tianxiang Hu, Yifan Xu, Zhaokun Zhou, Yonghong Tian, XU Bo, and Guoqi Li Spike-driven transformer v2: Meta spiking neural network architecture inspiring the design of next-generation neuromorphic chips In *The Twelfth International Conference on Learning Representations*, 2023. **Abstract:** Neuromorphic computing, which exploits Spiking Neural Networks (SNNs) on neuromorphic chips, is a promising energy-efficient alternative to traditional AI. CNN-based SNNs are the current mainstream of neuromorphic computing. By contrast, no neuromorphic chips are designed especially for Transformer-based SNNs, which have just emerged, and their performance is only on par with CNN-based SNNs, offering no distinct advantage. In this work, we propose a general Transformer-based SNN architecture, termed as “Meta-SpikeFormer", whose goals are: 1) Lower-power, supports the spike-driven paradigm that there is only sparse addition in the network; 2) Versatility, handles various vision tasks; 3) High-performance, shows overwhelming performance advantages over CNN-based SNNs; 4) Meta-architecture, provides inspiration for future next-generation Transformer-based neuromorphic chip designs. Specifically, we extend the Spike-driven Transformer in \\}citet{yao2023spike} into a meta architecture, and explore the impact of structure, spike-driven self-attention, and skip connection on its performance. On ImageNet-1K, Meta-SpikeFormer achieves 80.0\\}% top-1 accuracy (55M), surpassing the current state-of-the-art (SOTA) SNN baselines (66M) by 3.7\\}%. This is the first direct training SNN backbone that can simultaneously supports classification, detection, and segmentation, obtaining SOTA results in SNNs. Finally, we discuss the inspiration of the meta SNN architecture for neuromorphic chip design. Source code and models are available at \\}url{https://github.com/BICLab/Spike-Driven-Transformer-V2}. (@yao2023spikev2)

Youngeun Kim, Joshua Chough, and Priyadarshini Panda Beyond classification: Directly training spiking neural networks for semantic segmentation , 2(4):044015, 2022. **Abstract:** Abstract Spiking neural networks (SNNs) have recently emerged as the low-power alternative to artificial neural networks (ANNs) because of their sparse, asynchronous, and binary event-driven processing. Due to their energy efficiency, SNNs have a high possibility of being deployed for real-world, resource-constrained systems such as autonomous vehicles and drones. However, owing to their non-differentiable and complex neuronal dynamics, most previous SNN optimization methods have been limited to image recognition. In this paper, we explore the SNN applications beyond classification and present semantic segmentation networks configured with spiking neurons. Specifically, we first investigate two representative SNN optimization techniques for recognition tasks (i.e., ANN-SNN conversion and surrogate gradient learning) on semantic segmentation datasets. We observe that, when converted from ANNs, SNNs suffer from high latency and low performance due to the spatial variance of features. Therefore, we directly train networks with surrogate gradient learning, resulting in lower latency and higher performance than ANN-SNN conversion. Moreover, we redesign two fundamental ANN segmentation architectures (i.e., Fully Convolutional Networks and DeepLab) for the SNN domain. We conduct experiments on three semantic segmentation benchmarks including PASCAL VOC2012 dataset, DDD17 event-based dataset, and synthetic segmentation dataset combined CIFAR10 and MNIST datasets. In addition to showing the feasibility of SNNs for semantic segmentation, we show that SNNs can be more robust and energy-efficient compared to their ANN counterparts in this domain. (@kim2022segmentation)

Xue-Rui Qiu, Zhao-Rui Wang, Zheng Luan, Rui-Jie Zhu, Ma-Lu Zhang, and Liang-Jian Deng Vtsnn: a virtual temporal spiking neural network , 17:1091097, 2023. **Abstract:** Spiking neural networks (SNNs) have recently demonstrated outstanding performance in a variety of high-level tasks, such as image classification. However, advancements in the field of low-level assignments, such as image reconstruction, are rare. This may be due to the lack of promising image encoding techniques and corresponding neuromorphic devices designed specifically for SNN-based low-level vision problems. This paper begins by proposing a simple yet effective undistorted weighted-encoding-decoding technique, which primarily consists of an Undistorted Weighted-Encoding (UWE) and an Undistorted Weighted-Decoding (UWD). The former aims to convert a gray image into spike sequences for effective SNN learning, while the latter converts spike sequences back into images. Then, we design a new SNN training strategy, known as Independent-Temporal Backpropagation (ITBP) to avoid complex loss propagation in spatial and temporal dimensions, and experiments show that ITBP is superior to Spatio-Temporal Backpropagation (STBP). Finally, a so-called Virtual Temporal SNN (VTSNN) is formulated by incorporating the above-mentioned approaches into U-net network architecture, fully utilizing the potent multiscale representation capability. Experimental results on several commonly used datasets such as MNIST, F-MNIST, and CIFAR10 demonstrate that the proposed method produces competitive noise-removal performance extremely which is superior to the existing work. Compared to ANN with the same architecture, VTSNN has a greater chance of achieving superiority while consuming \~1/274 of the energy. Specifically, using the given encoding-decoding strategy, a simple neuromorphic circuit could be easily constructed to maximize this low-carbon strategy. (@qiu2023vtsnn)

Hiromichi Kamata, Yusuke Mukuta, and Tatsuya Harada Fully spiking variational autoencoder In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 36, pages 7059–7067, 2022. **Abstract:** Spiking neural networks (SNNs) can be run on neuromorphic devices with ultra-high speed and ultra-low energy consumption because of their binary and event-driven nature. Therefore, SNNs are expected to have various applications, including as generative models being running on edge devices to create high-quality images. In this study, we build a variational autoencoder (VAE) with SNN to enable image generation. VAE is known for its stability among generative models; recently, its quality advanced. In vanilla VAE, the latent space is represented as a normal distribution, and floating-point calculations are required in sampling. However, this is not possible in SNNs because all features must be binary time series data. Therefore, we constructed the latent space with an autoregressive SNN model, and randomly selected samples from its output to sample the latent variables. This allows the latent variables to follow the Bernoulli process and allows variational learning. Thus, we build the Fully Spiking Variational Autoencoder where all modules are constructed with SNN. To the best of our knowledge, we are the first to build a VAE only with SNN layers. We experimented with several datasets, and confirmed that it can generate images with the same or better quality compared to conventional ANNs. The code is available at https://github.com/kamata1729/FullySpikingVAE. (@kamata2022vae)

Qiugang Zhan, Xiurui Xie, Guisong Liu, and Malu Zhang Esvae: An efficient spiking variational autoencoder with reparameterizable poisson spiking sampling , 2023. **Abstract:** In recent years, studies on image generation models of spiking neural networks (SNNs) have gained the attention of many researchers. Variational autoencoders (VAEs), as one of the most popular image generation models, have attracted a lot of work exploring their SNN implementation. Due to the constrained binary representation in SNNs, existing SNN VAE methods implicitly construct the latent space by an elaborated autoregressive network and use the network outputs as the sampling variables. However, this unspecified implicit representation of the latent space will increase the difficulty of generating high-quality images and introduces additional network parameters. In this paper, we propose an efficient spiking variational autoencoder (ESVAE) that constructs an interpretable latent space distribution and design a reparameterizable spiking sampling method. Specifically, we construct the prior and posterior of the latent space as a Poisson distribution using the firing rate of the spiking neurons. Subsequently, we propose a reparameterizable Poisson spiking sampling method, which is free from the additional network. Comprehensive experiments have been conducted, and the experimental results show that the proposed ESVAE outperforms previous SNN VAE methods in reconstructed & generated images quality. In addition, experiments demonstrate that ESVAE’s encoder is able to retain the original image information more efficiently, and the decoder is more robust. The source code is available at https://github.com/QgZhan/ESVAE. (@zhan2023esvae)

Xuerui Qiu, Zheng Luan, Zhaorui Wang, and Rui-Jie Zhu When spiking neural networks meet temporal attention image decoding and adaptive spiking neuron . **Abstract:** Spiking Neural Networks (SNNs) are capable of encoding and processing temporal information in a biologically plausible way. However, most existing SNN-based methods for image tasks do not fully exploit this feature. Moreover, they often overlook the role of adaptive threshold in spiking neurons, which can enhance their dynamic behavior and learning ability. To address these issues, we propose a novel method for image decoding based on temporal attention (TAID) and an adaptive Leaky-Integrate-and-Fire (ALIF) neuron model. Our method leverages the temporal information of SNN outputs to generate high-quality images that surpass the state-of-the-art (SOTA) in terms of Inception score, Fr\\}’echet Inception Distance, and Fr\\}’echet Autoencoder Distance. Furthermore, our ALIF neuron model achieves remarkable classification accuracy on MNIST (99.78\\}%) and CIFAR-10 (93.89\\}%) datasets, demonstrating the effectiveness of learning adaptive thresholds for spiking neurons. The code is available at https://github.com/bollossom/ICLR_TINY_SNN. (@qiu2023taid)

Jiahang Cao, Ziqing Wang, Hanzhong Guo, Hao Cheng, Qiang Zhang, and Renjing Xu Spiking denoising diffusion probabilistic models In *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision*, pages 4912–4921, 2024. **Abstract:** Spiking neural networks (SNNs) have ultra-low energy consumption and high biological plausibility due to their binary and bio-driven nature compared with artificial neural networks (ANNs). While previous research has primarily focused on enhancing the performance of SNNs in classification tasks, the generative potential of SNNs remains relatively unexplored. In our paper, we put forward Spiking Denoising Diffusion Probabilistic Models (SDDPM), a new class of SNN-based generative models that achieve high sample quality. To fully exploit the energy efficiency of SNNs, we propose a purely Spiking U-Net architecture, which achieves comparable performance to its ANN counterpart using only 4 time steps, resulting in significantly reduced energy consumption. Extensive experimental results reveal that our approach achieves state-of-the-art on the generative tasks and substantially outperforms other SNN-based generative models, achieving up to 12× and 6× improvement on the CIFAR-10 and the CelebA datasets, respectively. Moreover, we propose a threshold-guided strategy that can further improve the performances by 2.69% in a training-free manner. The SDDPM symbolizes a significant advancement in the field of SNN generation, injecting new perspectives and potential avenues of exploration. Our code is available at https://github.com/AndyCao1125/SDDPM. (@cao2024spiking)

Rui-Jie Zhu, Qihang Zhao, Guoqi Li, and Jason K Eshraghian Spikegpt: Generative pre-trained language model with spiking neural networks , 2023. **Abstract:** As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement ‘SpikeGPT’, a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language. We achieve this by modifying the transformer block to replace multi-head self attention to reduce quadratic computational complexity O(N\^2) to linear complexity O(N) with increasing sequence length. Input tokens are instead streamed in sequentially to our attention mechanism (as with typical SNNs). Our preliminary experiments show that SpikeGPT remains competitive with non-spiking models on tested benchmarks, while maintaining 20x fewer operations when processed on neuromorphic hardware that can leverage sparse, event-driven activations. Our code implementation is available at https://github.com/ridgerchu/SpikeGPT. (@zhu2023spikegpt)

Changze Lv, Tianlong Li, Jianhan Xu, Chenxi Gu, Zixuan Ling, Cenyuan Zhang, Xiaoqing Zheng, and Xuanjing Huang Spikebert: A language spikformer trained with two-stage knowledge distillation from bert , 2023. **Abstract:** Spiking neural networks (SNNs) offer a promising avenue to implement deep neural networks in a more energy-efficient way. However, the network architectures of existing SNNs for language tasks are still simplistic and relatively shallow, and deep architectures have not been fully explored, resulting in a significant performance gap compared to mainstream transformer-based networks such as BERT. To this end, we improve a recently-proposed spiking Transformer (i.e., Spikformer) to make it possible to process language tasks and propose a two-stage knowledge distillation method for training it, which combines pre-training by distilling knowledge from BERT with a large collection of unlabelled texts and fine-tuning with task-specific instances via knowledge distillation again from the BERT fine-tuned on the same training examples. Through extensive experimentation, we show that the models trained with our method, named SpikeBERT, outperform state-of-the-art SNNs and even achieve comparable results to BERTs on text classification tasks for both English and Chinese with much less energy consumption. Our code is available at https://github.com/Lvchangze/SpikeBERT. (@lv2023spikebert)

Malyaban Bal and Abhronil Sengupta Spikingbert: Distilling bert to train spiking language models using implicit differentiation In *Proceedings of the AAAI conference on artificial intelligence*, volume 38, pages 10998–11006, 2024. **Abstract:** Large language Models (LLMs), though growing exceedingly powerful, comprises of orders of magnitude less neurons and synapses than the human brain. However, it requires significantly more power/energy to operate. In this work, we propose a novel bio-inspired spiking language model (LM) which aims to reduce the computational cost of conventional LMs by drawing motivation from the synaptic information flow in the brain. In this paper, we demonstrate a framework that leverages the average spiking rate of neurons at equilibrium to train a neuromorphic spiking LM using implicit differentiation technique, thereby overcoming the non-differentiability problem of spiking neural network (SNN) based algorithms without using any type of surrogate gradient. The steady-state convergence of the spiking neurons also allows us to design a spiking attention mechanism, which is critical in developing a scalable spiking LM. Moreover, the convergence of average spiking rate of neurons at equilibrium is utilized to develop a novel ANN-SNN knowledge distillation based technique wherein we use a pre-trained BERT model as “teacher” to train our “student” spiking architecture. While the primary architecture proposed in this paper is motivated by BERT, the technique can be potentially extended to different kinds of LLMs. Our work is the first one to demonstrate the performance of an operational spiking LM architecture on multiple different tasks in the GLUE benchmark. Our implementation source code is available at https://github.com/NeuroCompLab-psu/SpikingBERT. (@bal2024spikingbert)

Alexander Henkes, Jason K Eshraghian, and Henning Wessels Spiking neural networks for nonlinear regression , 2022. **Abstract:** Spiking neural networks, also often referred to as the third generation of neural networks, carry the potential for a massive reduction in memory and energy consumption over traditional, second-generation neural networks. Inspired by the undisputed efficiency of the human brain, they introduce temporal and neuronal sparsity, which can be exploited by next-generation neuromorphic hardware. To open the pathway toward engineering applications, we introduce this exciting technology in the context of continuum mechanics. However, the nature of spiking neural networks poses a challenge for regression problems, which frequently arise in the modeling of engineering sciences. To overcome this problem, a framework for regression using spiking neural networks is proposed. In particular, a network topology for decoding binary spike trains to real numbers is introduced, utilizing the membrane potential of spiking neurons. As the aim of this contribution is a concise introduction to this new methodology, several different spiking neural architectures, ranging from simple spiking feed-forward to complex spiking long short-term memory neural networks, are derived. Several numerical experiments directed towards regression of linear and nonlinear, history-dependent material models are carried out. A direct comparison with counterparts of traditional neural networks shows that the proposed framework is much more efficient while retaining precision and generalizability. All code has been made publicly available in the interest of reproducibility and to promote continued enhancement in this new domain. (@henkes2022spiking)

Yangfan Hu, Huajin Tang, and Gang Pan Spiking deep residual networks , 34(8):5200–5205, 2021. **Abstract:** Spiking neural networks (SNNs) have received significant attention for their biological plausibility. SNNs theoretically have at least the same computational power as traditional artificial neural networks (ANNs). They possess the potential of achieving energy-efficient machine intelligence while keeping comparable performance to ANNs. However, it is still a big challenge to train a very deep SNN. In this brief, we propose an efficient approach to build deep SNNs. Residual network (ResNet) is considered a state-of-the-art and fundamental model among convolutional neural networks (CNNs). We employ the idea of converting a trained ResNet to a network of spiking neurons named spiking ResNet (S-ResNet). We propose a residual conversion model that appropriately scales continuous-valued activations in ANNs to match the firing rates in SNNs and a compensation mechanism to reduce the error caused by discretization. Experimental results demonstrate that our proposed method achieves state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet 2012 with low latency. This work is the first time to build an asynchronous SNN deeper than 100 layers, with comparable performance to its original ANN. (@hu2021spikingresnet)

Samuel Schmidgall, Jascha Achterberg, Thomas Miconi, Louis Kirsch, Rojin Ziaei, S Hajiseyedrazi, and Jason Eshraghian Brain-inspired learning in artificial neural networks: a review , 2023. **Abstract:** Artificial neural networks (ANNs) have emerged as an essential tool in machine learning, achieving remarkable success across diverse domains, including image and speech generation, game playing, and robotics. However, there exist fundamental differences between ANNs’ operating mechanisms and those of the biological brain, particularly concerning learning processes. This paper presents a comprehensive review of current brain-inspired learning representations in artificial neural networks. We investigate the integration of more biologically plausible mechanisms, such as synaptic plasticity, to enhance these networks’ capabilities. Moreover, we delve into the potential advantages and challenges accompanying this approach. Ultimately, we pinpoint promising avenues for future research in this rapidly advancing field, which could bring us closer to understanding the essence of intelligence. (@schmidgall2023brain)

Antonio Vitale, Alpha Renner, Celine Nauer, Davide Scaramuzza, and Yulia Sandamirskaya Event-driven vision and control for uavs on a neuromorphic chip In *2021 IEEE International Conference on Robotics and Automation (ICRA)*, pages 103–109. IEEE, 2021. **Abstract:** Event-based vision sensors achieve up to three orders of magnitude better speed vs. power consumption trade off in high-speed control of UAVs compared to conventional image sensors. Event-based cameras produce a sparse stream of events that can be processed more efficiently and with a lower latency than images, enabling ultra-fast vision-driven control. Here, we explore how an event-based vision algorithm can be implemented as a spiking neuronal network on a neuromorphic chip and used in a drone controller. We show how seamless integration of event-based perception on chip leads to even faster control rates and lower latency. In addition, we demonstrate how online adaptation of the SNN controller can be realised using on-chip learning. Our spiking neuronal network on chip is the first example of a neuromorphic vision-based controller on chip solving a high-speed UAV control task. The excellent scalability of processing in neuromorphic hardware opens the possibility to solve more challenging visual tasks in the future and integrate visual perception in fast control loops. (@vitale2021event)

Sebastian Glatz, Julien Martel, Raphaela Kreiser, Ning Qiao, and Yulia Sandamirskaya Adaptive motor control and learning in a spiking neural network realised on a mixed-signal neuromorphic processor In *2019 International Conference on Robotics and Automation (ICRA)*, pages 9631–9637. IEEE, 2019. **Abstract:** Neuromorphic computing is a new paradigm for design of both the computing hardware and algorithms inspired by biological neural networks. The event-based nature and the inherent parallelism make neuromorphic computing a promising paradigm for building efficient neural network based architectures for control of fast and agile robots. In this paper, we present a spiking neural network architecture that uses sensory feedback to control rotational velocity of a robotic vehicle. When the velocity reaches the target value, the mapping from the target velocity of the vehicle to the correct motor command, both represented in the spiking neural network on the neuromorphic device, is autonomously stored on the device using on-chip plastic synaptic weights. We validate the controller using a wheel motor of a miniature mobile vehicle and inertia measurement unit as the sensory feedback and demonstrate online learning of a simple "inverse model" in a two-layer spiking neural network on the neuromorphic chip. The prototype neuromorphic device that features 256 spiking neurons allows us to realise a simple proof of concept architecture for the purely neuromorphic motor control and learning. The architecture can be easily scaled-up if a larger neuromorphic device is available. (@glatz2019adaptive)

Rasmus Karnøe Stagsted, Antonio Vitale, Alpha Renner, Leon Bonde Larsen, Anders Lyhne Christensen, and Yulia Sandamirskaya Event-based pid controller fully realized in neuromorphic hardware: A one dof study In *2020 IEEE/RSJ international conference on intelligent robots and systems (IROS)*, pages 10939–10944. IEEE, 2020. **Abstract:** Spiking Neuronal Networks (SNNs) realized in neuromorphic hardware lead to low-power and low-latency neuronal computing architectures. Neuromorphic computing systems are most efficient when all of perception, decision making, and motor control are seamlessly integrated into a single neuronal architecture that can be realized on the neuromorphic hardware. Many neuronal network architectures address the perception tasks, while work on neuronal motor controllers is scarce. Here, we present an improved implementation of a neuromorphic PID controller. The controller was realized on Intel’s neuromorphic research chip Loihi and its performance tested on a drone, constrained to rotate on a single axis. The SNN controller is built using neuronal populations, in which a single spike carries information about sensed and control signals. Neuronal arrays perform computation on such sparse representations to calculate the proportional, derivative, and integral terms. The SNN PID controller is compared to a PID controller, implemented in software, and achieves a comparable performance, paving the way to a fully neuromorphic system in which perception, planning, and control are realized in an on-chip SNN. (@stagsted2020event)

Rasmus Stagsted, Antonio Vitale, Jonas Binz, Leon Bonde Larsen, Yulia Sandamirskaya, et al Towards neuromorphic control: A spiking neural network based pid controller for uav RSS, 2020. **Abstract:** In this work, we present a spiking neural network (SNN) based PID controller on a neuromorphic chip.On-chip SNNs are currently being explored in low-power AI applications.Due to potentially ultra-low power consumption, low latency, and high processing speed, on-chip SNNs are a promising tool for control of power-constrained platforms, such as Unmanned Aerial Vehicles (UAV).To obtain highly efficient and fast end-toend neuromorphic controllers, the SNN-based AI architectures must be seamlessly integrated with motor control.Towards this goal, we present here the first implementation of a fully neuromorphic PID controller.We interfaced Intel’s neuromorphic research chip Loihi to a UAV, constrained to a single degree of freedom.We developed an SNN control architecture using populations of spiking neurons, in which each spike carries information about the measured, control, or error value, defined by the identity of the spiking neuron.Using this sparse code, we realize a precise PID controller.The P, I, and D gains of the controller are implemented as synaptic weights that can adapt according to an on-chip plasticity rule.In future work, these plastic synapses can be used to tune and adapt the controller autonomously. (@stagsted2020towards)

Tim Burgers, Stein Stroobants, and Guido de Croon Evolving spiking neural networks to mimic pid control for autonomous blimps , 2023. **Abstract:** In recent years, Artificial Neural Networks (ANN) have become a standard in robotic control. However, a significant drawback of large-scale ANNs is their increased power consumption. This becomes a critical concern when designing autonomous aerial vehicles, given the stringent constraints on power and weight. Especially in the case of blimps, known for their extended endurance, power-efficient control methods are essential. Spiking neural networks (SNN) can provide a solution, facilitating energy-efficient and asynchronous event-driven processing. In this paper, we have evolved SNNs for accurate altitude control of a non-neutrally buoyant indoor blimp, relying solely on onboard sensing and processing power. The blimp’s altitude tracking performance significantly improved compared to prior research, showing reduced oscillations and a minimal steady-state error. The parameters of the SNNs were optimized via an evolutionary algorithm, using a Proportional-Derivative-Integral (PID) controller as the target signal. We developed two complementary SNN controllers while examining various hidden layer structures. The first controller responds swiftly to control errors, mitigating overshooting and oscillations, while the second minimizes steady-state errors due to non-neutral buoyancy-induced drift. Despite the blimp’s drivetrain limitations, our SNN controllers ensured stable altitude control, employing only 160 spiking neurons. (@burgers2023evolving)

Zhenshan Bing, Claus Meschede, Kai Huang, Guang Chen, Florian Rohrbein, Mahmoud Akl, and Alois Knoll End to end learning of spiking neural network based on r-stdp for a lane keeping vehicle In *2018 IEEE international conference on robotics and automation (ICRA)*, pages 4725–4732. IEEE, 2018. **Abstract:** Learning-based methods have demonstrated clear advantages in controlling robot tasks, such as the information fusion abilities, strong robustness, and high accuracy. Meanwhile, the on-board systems of robots have limited computation and energy resources, which are contradictory with state-of-the-art learning approaches. They are either too lightweight to solve complex problems or too heavyweight to be used for mobile applications. On the other hand, training spiking neural networks (SNNs) with biological plausibility has great potentials of performing fast computation and energy efficiency. However, the lack of effective learning rules for SNNs impedes their wide usage in mobile robot applications. This paper addresses the problem by introducing an end to end learning approach of spiking neural networks for a lane keeping vehicle. We consider the reward-modulated spike-timing-dependent-plasticity (R-STDP) as a promising solution in training SNNs, since it combines the advantages of both reinforcement learning and the well-known STDP. We test our approach in three scenarios that a Pioneer robot is controlled to keep lanes based on an SNN. Specifically, the lane information is encoded by the event data from a neuromorphic vision sensor. The SNN is constructed using R-STDP synapses in an all-to-all fashion. We demonstrate the advantages of our approach in terms of the lateral localization accuracy by comparing with other state-of-the-art learning algorithms based on SNNs. (@bing2018end)

Raz Halaly and Elishai Ezra Tsur Autonomous driving controllers with neuromorphic spiking neural networks , 17:1234962, 2023. **Abstract:** Autonomous driving is one of the hallmarks of artificial intelligence. Neuromorphic (brain-inspired) control is posed to significantly contribute to autonomous behavior by leveraging spiking neural networks-based energy-efficient computational frameworks. In this work, we have explored neuromorphic implementations of four prominent controllers for autonomous driving: pure-pursuit, Stanley, PID, and MPC, using a physics-aware simulation framework. We extensively evaluated these models with various intrinsic parameters and compared their performance with conventional CPU-based implementations. While being neural approximations, we show that neuromorphic models can perform competitively with their conventional counterparts. We provide guidelines for building neuromorphic architectures for control and describe the importance of their underlying tuning parameters and neuronal resources. Our results show that most models would converge to their optimal performances with merely 100-1,000 neurons. They also highlight the importance of hybrid conventional and neuromorphic designs, as was suggested here with the MPC controller. This study also highlights the limitations of neuromorphic implementations, particularly at higher (\> 15 m/s) speeds where they tend to degrade faster than in conventional designs. (@halaly2023autonomous)

Jacques Kaiser, J Camilo Vasquez Tieck, Christian Hubschneider, Peter Wolf, Michael Weber, Michael Hoff, Alexander Friedrich, Konrad Wojtasik, Arne Roennau, Ralf Kohlhaas, et al Towards a framework for end-to-end control of a simulated vehicle with spiking neural networks In *2016 IEEE International Conference on Simulation, Modeling, and Programming for Autonomous Robots (SIMPAR)*, pages 127–134. IEEE, 2016. **Abstract:** Spiking neural networks are in theory more computationally powerful than rate-based neural networks often used in deep learning architectures. However, unlike rate-based neural networks, it is yet unclear how to train spiking networks to solve complex problems. There are still no standard algorithms and it is preventing roboticists to use spiking networks, yielding a lack of Neurorobotics applications. The contribution of this paper is twofold. First, we present a modular framework to evaluate neural self-driving vehicle applications. It provides a visual encoder from camera images to spikes inspired by the silicon retina (DVS), and a steering wheel decoder based on an agonist antagonist muscle model. Secondly, using this framework, we demonstrate a spiking neural network which controls a vehicle end-to-end for lane following behavior. The network is feed-forward and relies on hand-crafted feature detectors. In future work, this framework could be used to design more complex networks and use the evaluation metrics for learning. (@kaiser2016towards)

Albert Shalumov, Raz Halaly, and Elishai Ezra Tsur Lidar-driven spiking neural network for collision avoidance in autonomous driving , 16(6):066016, 2021. **Abstract:** Facilitated by advances in real-time sensing, low and high-level control, and machine learning, autonomous vehicles draw ever-increasing attention from many branches of knowledge. Neuromorphic (brain-inspired) implementation of robotic control has been shown to outperform conventional control paradigms in terms of energy efficiency, robustness to perturbations, and adaptation to varying conditions. Here we propose LiDAR-driven neuromorphic control of both vehicle’s speed and steering. We evaluated and compared neuromorphic PID control and online learning for autonomous vehicle control in static and dynamic environments, finally suggesting proportional learning as a preferred control scheme. We employed biologically plausible basal-ganglia and thalamus neural models for steering and collision-avoidance, finally extending them to support a null controller and a target-reaching optimization, significantly increasing performance. (@shalumov2021lidar)

Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger, Andreas Geiger, and Hongyang Li End-to-end autonomous driving: Challenges and frontiers , 2023. **Abstract:** The autonomous driving community has witnessed a rapid growth in approaches that embrace an end-to-end algorithm framework, utilizing raw sensor input to generate vehicle motion plans, instead of concentrating on individual tasks such as detection and motion prediction. End-to-end systems, in comparison to modular pipelines, benefit from joint feature optimization for perception and planning. This field has flourished due to the availability of large-scale datasets, closed-loop evaluation, and the increasing need for autonomous driving algorithms to perform effectively in challenging scenarios. In this survey, we provide a comprehensive analysis of more than 270 papers, covering the motivation, roadmap, methodology, challenges, and future trends in end-to-end autonomous driving. We delve into several critical challenges, including multi-modality, interpretability, causal confusion, robustness, and world models, amongst others. Additionally, we discuss current advancements in foundation models and visual pre-training, as well as how to incorporate these techniques within the end-to-end driving framework. we maintain an active repository that contains up-to-date literature and open-source projects at https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving. (@chen2023end)

Dian Chen, Brady Zhou, Vladlen Koltun, and Philipp Krähenbühl Learning by cheating In *Conference on Robot Learning*, pages 66–75. PMLR, 2020. **Abstract:** Vision-based urban driving is hard. The autonomous system needs to learn to perceive the world and act in it. We show that this challenging learning problem can be simplified by decomposing it into two stages. We first train an agent that has access to privileged information. This privileged agent cheats by observing the ground-truth layout of the environment and the positions of all traffic participants. In the second stage, the privileged agent acts as a teacher that trains a purely vision-based sensorimotor agent. The resulting sensorimotor agent does not have access to any privileged information and does not cheat. This two-stage training procedure is counter-intuitive at first, but has a number of important advantages that we analyze and empirically demonstrate. We use the presented approach to train a vision-based autonomous driving system that substantially outperforms the state of the art on the CARLA benchmark and the recent NoCrash benchmark. Our approach achieves, for the first time, 100% success rate on all tasks in the original CARLA benchmark, sets a new record on the NoCrash benchmark, and reduces the frequency of infractions by an order of magnitude compared to the prior state of the art. For the video that summarizes this work, see this https URL (@chen2020learning)

Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al End to end learning for self-driving cars , 2016. **Abstract:** We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn’t automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS). (@bojarski2016end)

Felipe Codevilla, Matthias Müller, Antonio López, Vladlen Koltun, and Alexey Dosovitskiy End-to-end driving via conditional imitation learning In *2018 IEEE international conference on robotics and automation (ICRA)*, pages 4693–4700. IEEE, 2018. **Abstract:** Deep networks trained on demonstrations of human driving have learned to follow roads and avoid obstacles. However, driving policies trained via imitation learning cannot be controlled at test time. A vehicle trained end-to-end to imitate an expert cannot be guided to take a specific turn at an upcoming intersection. This limits the utility of such systems. We propose to condition imitation learning on high-level command input. At test time, the learned driving policy functions as a chauffeur that handles sensorimotor coordination but continues to respond to navigational commands. We evaluate different architectures for conditional imitation learning in vision-based driving. We conduct experiments in realistic three-dimensional simulations of urban driving and on a 1/5 scale robotic truck that is trained to drive in a residential area. Both systems drive based on visual input yet remain responsive to high-level navigational commands. (@codevilla2018end)

Jeffrey Hawke, Richard Shen, Corina Gurau, Siddharth Sharma, Daniele Reda, Nikolay Nikolov, Przemysław Mazur, Sean Micklethwaite, Nicolas Griffiths, Amar Shah, et al Urban driving with conditional imitation learning In *2020 IEEE International Conference on Robotics and Automation (ICRA)*, pages 251–257. IEEE, 2020. **Abstract:** Hand-crafting generalised decision-making rules for real-world urban autonomous driving is hard. Alternatively, learning behaviour from easy-to-collect human driving demonstrations is appealing. Prior work has studied imitation learning (IL) for autonomous driving with a number of limitations. Examples include only performing lane-following rather than following a user-defined route, only using a single camera view or heavily cropped frames lacking state observability, only lateral (steering) control, but not longitudinal (speed) control and a lack of interaction with traffic. Importantly, the majority of such systems have been primarily evaluated in simulation - a simple domain, which lacks real-world complexities. Motivated by these challenges, we focus on learning representations of semantics, geometry and motion with computer vision for IL from human driving demonstrations. As our main contribution, we present an end-to-end conditional imitation learning approach, combining both lateral and longitudinal control on a real vehicle for following urban routes with simple traffic. We address inherent dataset bias by data balancing, training our final policy on approximately 30 hours of demonstrations gathered over six months. We evaluate our method on an autonomous vehicle by driving 35km of novel routes in European urban streets. (@hawke2020urban)

Felipe Codevilla, Eder Santana, Antonio M López, and Adrien Gaidon Exploring the limitations of behavior cloning for autonomous driving In *Proceedings of the IEEE/CVF international conference on computer vision*, pages 9329–9338, 2019. **Abstract:** Driving requires reacting to a wide variety of complex environment conditions and agent behaviors. Explicitly modeling each possible scenario is unrealistic. In contrast, imitation learning can, in theory, leverage data from large fleets of human-driven cars. Behavior cloning in particular has been successfully used to learn simple visuomotor policies end-to-end, but scaling to the full spectrum of driving behaviors remains an unsolved problem. In this paper, we propose a new benchmark to experimentally investigate the scalability and limitations of behavior cloning. We show that behavior cloning leads to state-ofthe-art results, executing complex lateral and longitudinal maneuvers, even in unseen environments, without being explicitly programmed to do so. However, we confirm some limitations of the behavior cloning approach: some wellknown limitations (e.g., dataset bias and overfitting), new generalization issues (e.g., dynamic objects and the lack of a causal modeling), and training instabilities, all requiring further research before behavior cloning can graduate to real-world driving. The code, dataset, benchmark, and agent studied in this paper can be found at http:// github.com/felipecode/coiltraine/blob/ master/docs/exploring_limitations.md. (@codevilla2019exploring)

Wenyuan Zeng, Wenjie Luo, Simon Suo, Abbas Sadat, Bin Yang, Sergio Casas, and Raquel Urtasun End-to-end interpretable neural motion planner In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 8660–8669, 2019. **Abstract:** In this paper, we propose a neural motion planner for learning to drive autonomously in complex urban scenarios that include traffic-light handling, yielding, and interactions with multiple road-users. Towards this goal, we design a holistic model that takes as input raw LIDAR data and a HD map and produces interpretable intermediate representations in the form of 3D detections and their future trajectories, as well as a cost volume defining the goodness of each position that the self-driving car can take within the planning horizon. We then sample a set of diverse physically possible trajectories and choose the one with the minimum learned cost. Importantly, our cost volume is able to naturally capture multi-modality. We demonstrate the effectiveness of our approach in real-world driving data captured in several cities in North America. Our experiments show that the learned cost volume can generate safer planning than all the baselines. (@zeng2019end)

Abbas Sadat, Sergio Casas, Mengye Ren, Xinyu Wu, Pranaab Dhawan, and Raquel Urtasun Perceive, predict, and plan: Safe motion planning through interpretable semantic representations In *Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXIII 16*, pages 414–430. Springer, 2020. **Abstract:** In this paper we propose a novel end-to-end learnable net- work that performs joint perception, prediction and motion planning for self-driving vehicles and produces interpretable intermediate repre- sentations. Unlike existing neural motion planners, our motion planning costs are consistent with our perception and prediction estimates. This is achieved by a novel di erentiable semantic occupancy representation that is explicitly used as cost by the motion planning process. Our network is learned end-to-end from human demonstrations. The experiments in a large-scale manual-driving dataset and closed-loop simulation show that the proposed model signi cantly outperforms state-of-the-art planners in imitating the human behaviors while producing much safer trajectories. 1 Introduction The goal of an autonomy system is to take the output of the sensors, a map, and a high-level route, and produce a safe and comfortable ride. Meanwhile, produc- ing interpretable intermediate representations that can explain why the vehicle performed a certain maneuver is very important in safety critical applications such as self-driving, particularly if a bad event was to happen. Traditional au- tonomy stacks produce interpretable representations through the perception and prediction modules in the form of bounding boxes as well as distributions over their future motion \[1{6\]. However, the perception module involves thresholding detection con dence scores and running Non-Maximum Supression (NMS) to trade o the precision and recall of the object detector, which cause informa- tion loss that could result in unsafe situations, e.g., if a solid object is below the threshold. To handle this, software stacks in industry rely on a secondary fail safe system that tries to catch all mistakes from perception. This system is however trained separately and it is not easy to decide which system to trust. First attempts to perform end-to-end neural motion planning did not pro- duce interpretable representations \[7\], and instead focused on producing accurate control outputs that mimic how humans drive \[8\]. Recent approaches \[9{11\], ?Denotes equal contributionarXiv:2008.05930v1 \[cs.RO\] 13 Aug 20202 A. Sadat, S. Casas, M. Ren, X. Wu, P. Dhawan, R. Urtasun have tried to incorporate interpretability. The neural motion planner of \[10\] shared feature representations between perception, prediction and motion plan- ning. However it can produce inconsistent estimates between the modules, as it is fr (@sadat2020perceive)

Hao Wang, Peng Cai, Rui Fan, Yuxiang Sun, and Ming Liu End-to-end interactive prediction and planning with optical flow distillation for autonomous driving In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 2199–2208, 2021. **Abstract:** With the recent advancement of deep learning technology, data-driven approaches for autonomous car prediction and planning have achieved extraordinary performance. Nevertheless, most of these approaches follow a non-interactive prediction and planning paradigm, hypothesizing that a vehicle’s behaviors do not affect others. The approaches based on such a non-interactive philosophy typically perform acceptably in sparse traffic scenarios but can easily fail in dense traffic scenarios. Therefore, we propose an end-to-end interactive neural motion planner (INMP) for autonomous driving in this paper. Given a set of past surrounding-view images and a high definition map, our INMP first generates a feature map in bird’s-eye-view space, which is then processed to detect other agents and perform interactive prediction and planning jointly. Also, we adopt an optical flow distillation paradigm, which can effectively improve the network performance while still maintaining its real-time inference speed. Extensive experiments on the nuScenes dataset and in the closed-loop Carla simulation environment demonstrate the effectiveness and efficiency of our INMP for the detection, prediction, and planning tasks. Our project page is at sites.google.com/view/inmp-ofd. (@wang2021end)

Peiyun Hu, Anthony Huang, John Dolan, David Held, and Deva Ramanan Safe local motion planning with self-supervised freespace forecasting In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 13270–13279, 2021. **Abstract:** Safe local motion planning for autonomous driving in dynamic environments requires forecasting how the scene evolves. Practical autonomy stacks adopt a semantic object-centric representation of a dynamic scene and build object detection, tracking, and prediction modules to solve forecasting. However, training these modules comes at an enormous human cost of manually annotated objects across frames. In this work, we explore future freespace as an alternative representation to support motion planning. Our key intuition is that it is important to avoid straying into occupied space regardless of what is occupying it. Importantly, computing ground-truth future freespace is annotation-free. First, we explore freespace forecasting as a self-supervised learning task. We then demonstrate how to use forecasted freespace to identify collision-prone plans from off-the-shelf motion planners. Finally, we propose future freespace as an additional source of annotation-free supervision. We demonstrate how to integrate such supervision into the learning-based planners. Experimental results on nuScenes and CARLA suggest both approaches lead to a significant reduction in collision rates. \<sup xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"\>1\</sup\> (@hu2021safe)

Tarasha Khurana, Peiyun Hu, Achal Dave, Jason Ziglar, David Held, and Deva Ramanan Differentiable raycasting for self-supervised occupancy forecasting In *European Conference on Computer Vision*, pages 238–254. Springer, 2022. **Abstract:** Motion planning for safe autonomous driving requires learn- ing how the environment around an ego-vehicle evolves with time. Ego- centric perception of driveable regions in a scene not only changes with the motion of actors in the environment, but also with the movement of the ego-vehicle itself. Self-supervised representations proposed for large- scale planning, such as ego-centric freespace, confound these two motions, making the representation difficult to use for downstream motion plan- ners. In this paper, we use geometric occupancy as a natural alternative to view-dependent representations such as freespace. Occupancy maps naturally disentagle the motion of the environment from the motion of the ego-vehicle. However, one cannot directly observe the full 3D occu- pancy of a scene (due to occlusion), making it difficult to use as a signal for learning. Our key insight is to use differentiable raycasting to “render” future occupancy predictions into future LiDAR sweep predictions, which can be compared with ground-truth sweeps for self-supervised learning. The use of differentiable raycasting allows occupancy to emerge as an internal representation within the forecasting network. In the absence of groundtruth occupancy, we quantitatively evaluate the forecasting of raycasted LiDAR sweeps and show improvements of upto 15 F1 points. ⋆equal contributionarXiv:2210.01917v2 \[cs.CV\] 18 Oct 20222 T. Khurana∗, P. Hu∗, A. Dave, J. Ziglar, D. Held, D. Ramanan For downstream motion planners, where emergent occupancy can be di- rectly used to guide non-driveable regions, this representation relatively reduces the number of collisions with objects by up to 17% as compared to freespace-centric motion planners. 1 Introduction To navigate in complex and dynamic environments such as urban cores, au- tonomous vehicles need to perceive actors and predict their future movements. Such knowledge is often represented in some form of forecasted occupancy \[24\], which downstream motion planners rely on to produce safe trajectories. When tackling the tasks of perception and prediction, standard solutions consist of per- ceptual modules such as object detection, tracking, and trajectory forecasting, which require a massive amount of object track labels. Such solutions do not scale given the speed that log data is being collected by large fleets. Freespace versus occupancy: To avoid the need for costly human anno- tations, and to enable learning at scale, self-supervised represent (@khurana2022differentiable)

Alex Kendall, Jeffrey Hawke, David Janz, Przemyslaw Mazur, Daniele Reda, John-Mark Allen, Vinh-Dieu Lam, Alex Bewley, and Amar Shah Learning to drive in a day In *2019 international conference on robotics and automation (ICRA)*, pages 8248–8254. IEEE, 2019. **Abstract:** We demonstrate the first application of deep reinforcement learning to autonomous driving. From randomly initialised parameters, our model is able to learn a policy for lane following in a handful of training episodes using a single monocular image as input. We provide a general and easy to obtain reward: the distance travelled by the vehicle without the safety driver taking control. We use a continuous, model-free deep reinforcement learning algorithm, with all exploration and optimisation performed on-vehicle. This demonstrates a new framework for autonomous driving which moves away from reliance on defined logical rules, mapping, and direct supervision. We discuss the challenges and opportunities to scale this approach to a broader range of autonomous driving tasks. (@kendall2019learning)

Xiaodan Liang, Tairui Wang, Luona Yang, and Eric Xing Cirl: Controllable imitative reinforcement learning for vision-based self-driving In *Proceedings of the European conference on computer vision (ECCV)*, pages 584–599, 2018. **Abstract:** Autonomous urban driving navigation with complex multi-agent dy- namics is under-explored due to the difﬁculty of learning an optimal driving pol- icy. The traditional modular pipeline heavily relies on hand-designed rules and the pre-processing perception system while the supervised learning-based models are limited by the accessibility of extensive human experience. We present a general and principled Controllable Imitative Reinforcement Learning (CIRL) approach which successfully makes the driving agent achieve higher success rates based on only vision inputs in a high-ﬁdelity car simulator. To alleviate the low explo- ration efﬁciency for large continuous action space that often prohibits the use of classical RL on challenging real tasks, our CIRL explores over a reasonably con- strained action space guided by encoded experiences that imitate human demon- strations, building upon Deep Deterministic Policy Gradient (DDPG). Moreover, we propose to specialize adaptive policies and steering-angle reward designs for different control signals (i.e. follow, straight, turn right, turn left) based on the shared representations to improve the model capability in tackling with diverse cases. Extensive experiments on CARLA driving benchmark demonstrate that CIRL substantially outperforms all previous methods in terms of the percentage of successfully completed episodes on a variety of goal-directed driving tasks. We also show its superior generalization capability in unseen environments. To our knowledge, this is the ﬁrst successful case of the learned driving policy by reinforcement learning in the high-ﬁdelity simulator, which performs better than supervised imitation learning. (@liang2018cirl)

Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde End-to-end model-free reinforcement learning for urban driving using implicit affordances In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 7153–7162, 2020. **Abstract:** Reinforcement Learning (RL) aims at learning an optimal behavior policy from its own experiments and not rule-based control methods. However, there is no RL algorithm yet capable of handling a task as difficult as urban driving. We present a novel technique, coined implicit affordances, to effectively leverage RL for urban driving thus including lane keeping, pedestrians and vehicles avoidance, and traffic light detection. To our knowledge we are the first to present a successful RL agent handling such a complex task especially regarding the traffic light detection. Furthermore, we have demonstrated the effectiveness of our method by winning the Camera Only track of the CARLA challenge. (@toromanoff2020end)

Raphael Chekroun, Marin Toromanoff, Sascha Hornauer, and Fabien Moutarde Gri: General reinforced imitation and its application to vision-based autonomous driving , 2021. **Abstract:** Deep reinforcement learning (DRL) has been demonstrated to be effective for several complex decision-making applications such as autonomous driving and robotics. However, DRL is notoriously limited by its high sample complexity and its lack of stability. Prior knowledge, e.g. as expert demonstrations, is often available but challenging to leverage to mitigate these issues. In this paper, we propose General Reinforced Imitation (GRI), a novel method which combines benefits from exploration and expert data and is straightforward to implement over any off-policy RL algorithm. We make one simplifying hypothesis: expert demonstrations can be seen as perfect data whose underlying policy gets a constant high reward. Based on this assumption, GRI introduces the notion of offline demonstration agents. This agent sends expert data which are processed both concurrently and indistinguishably with the experiences coming from the online RL exploration agent. We show that our approach enables major improvements on vision-based autonomous driving in urban environments. We further validate the GRI method on Mujoco continuous control tasks with different off-policy RL algorithms. Our method ranked first on the CARLA Leaderboard and outperforms World on Rails, the previous state-of-the-art, by 17%. (@chekroun2021gri)

Aditya Prakash, Kashyap Chitta, and Andreas Geiger Multi-modal fusion transformer for end-to-end autonomous driving In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 7077–7087, 2021. **Abstract:** How should representations from complementary sensors be integrated for autonomous driving? Geometry-based sensor fusion has shown great promise for perception tasks such as object detection and motion forecasting. However, for the actual driving task, the global context of the 3D scene is key, e.g. a change in traffic light state can affect the behavior of a vehicle geometrically distant from that traffic light. Geometry alone may therefore be insufficient for effectively fusing representations in end-to-end driving models. In this work, we demonstrate that imitation learning policies based on existing sensor fusion methods under-perform in the presence of a high density of dynamic agents and complex scenarios, which require global contextual reasoning, such as handling traffic oncoming from multiple directions at uncontrolled intersections. Therefore, we propose TransFuser, a novel Multi-Modal Fusion Transformer, to integrate image and LiDAR representations using attention. We experimentally validate the efficacy of our approach in urban settings involving complex scenarios using the CARLA urban driving simulator. Our approach achieves state-of-the-art driving performance while reducing collisions by 76% compared to geometry-based fusion. (@prakash2021multi)

Kashyap Chitta, Aditya Prakash, Bernhard Jaeger, Zehao Yu, Katrin Renz, and Andreas Geiger Transfuser: Imitation with transformer-based sensor fusion for autonomous driving , 2022. **Abstract:** How should we integrate representations from complementary sensors for autonomous driving? Geometry-based fusion has shown promise for perception (e.g., object detection, motion forecasting). However, in the context of end-to-end driving, we find that imitation learning based on existing sensor fusion methods underperforms in complex driving scenarios with a high density of dynamic agents. Therefore, we propose TransFuser, a mechanism to integrate image and LiDAR representations using self-attention. Our approach uses transformer modules at multiple resolutions to fuse perspective view and bird’s eye view feature maps. We experimentally validate its efficacy on a challenging new benchmark with long routes and dense traffic, as well as the official leaderboard of the CARLA urban driving simulator. At the time of submission, TransFuser outperforms all prior work on the CARLA leaderboard in terms of driving score by a large margin. Compared to geometry-based fusion, TransFuser reduces the average collisions per kilometer by 48%. (@chitta2022transfuser)

Hao Shao, Letian Wang, Ruobing Chen, Hongsheng Li, and Yu Liu Safety-enhanced autonomous driving using interpretable sensor fusion transformer In *Conference on Robot Learning*, pages 726–737. PMLR, 2023. **Abstract:** Large-scale deployment of autonomous vehicles has been continually delayed due to safety concerns. On the one hand, comprehensive scene understanding is indispensable, a lack of which would result in vulnerability to rare but complex traffic situations, such as the sudden emergence of unknown objects. However, reasoning from a global context requires access to sensors of multiple types and adequate fusion of multi-modal sensor signals, which is difficult to achieve. On the other hand, the lack of interpretability in learning models also hampers the safety with unverifiable failure causes. In this paper, we propose a safety-enhanced autonomous driving framework, named Interpretable Sensor Fusion Transformer(InterFuser), to fully process and fuse information from multi-modal multi-view sensors for achieving comprehensive scene understanding and adversarial event detection. Besides, intermediate interpretable features are generated from our framework, which provide more semantics and are exploited to better constrain actions to be within the safe sets. We conducted extensive experiments on CARLA benchmarks, where our model outperforms prior methods, ranking the first on the public CARLA Leaderboard. Our code will be made available at https://github.com/opendilab/InterFuser (@shao2022safety)

Xiaosong Jia, Penghao Wu, Li Chen, Jiangwei Xie, Conghui He, Junchi Yan, and Hongyang Li Think twice before driving: Towards scalable decoders for end-to-end autonomous driving In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 21983–21994, 2023. **Abstract:** End-to-end autonomous driving has made impressive progress in recent years. Existing methods usually adopt the decoupled encoder-decoder paradigm, where the encoder extracts hidden features from raw sensor data, and the decoder outputs the ego-vehicle’s future trajectories or actions. Under such a paradigm, the encoder does not have access to the intended behavior of the ego agent, leaving the burden of finding out safety-critical regions from the massive receptive field and inferring about future situations to the decoder. Even worse, the decoder is usually composed of several simple multi-layer perceptrons (MLP) or GRUs while the encoder is delicately designed (e.g., a combination of heavy ResNets or Transformer). Such an imbalanced resource-task division hampers the learning process. In this work, we aim to alleviate the aforementioned problem by two principles: (1) fully utilizing the capacity of the encoder; (2) increasing the capacity of the decoder. Concretely, we first predict a coarse-grained future position and action based on the encoder features. Then, conditioned on the position and action, the future scene is imagined to check the ramification if we drive accordingly. We also retrieve the encoder features around the predicted coordinate to obtain fine-grained information about the safety-critical region. Finally, based on the predicted future and the retrieved salient feature, we refine the coarse-grained position and action by predicting its offset from ground-truth. The above refinement module could be stacked in a cascaded fashion, which extends the capacity of the decoder with spatial-temporal prior knowledge about the conditioned future. We conduct experiments on the CARLA simulator and achieve state-of-the-art performance in closed-loop benchmarks. Extensive ablation studies demonstrate the effectiveness of each proposed module. (@jia2023think)

Bernhard Jaeger, Kashyap Chitta, and Andreas Geiger Hidden biases of end-to-end driving models In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 8240–8249, 2023. **Abstract:** End-to-end driving systems have recently made rapid progress, in particular on CARLA. Independent of their major contribution, they introduce changes to minor system components. Consequently, the source of improvements is unclear. We identify two biases that recur in nearly all state-of-the-art methods and are critical for the observed progress on CARLA: (1) lateral recovery via a strong inductive bias towards target point following, and (2) longitudinal averaging of multimodal waypoint predictions for slowing down. We investigate the drawbacks of these biases and identify principled alternatives. By incorporating our insights, we develop TF++, a simple end-to-end method that ranks first on the Longest6 and LAV benchmarks, gaining 11 driving score over the best prior work on Longest6. (@jaeger2023hidden)

Kashyap Chitta, Aditya Prakash, and Andreas Geiger Neat: Neural attention fields for end-to-end autonomous driving In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 15793–15802, 2021. **Abstract:** Efficient reasoning about the semantic, spatial, and temporal structure of a scene is a crucial prerequisite for autonomous driving. We present NEural ATtention fields (NEAT), a novel representation that enables such reasoning for end-to-end imitation learning models. NEAT is a continuous function which maps locations in Bird’s Eye View (BEV) scene coordinates to waypoints and semantics, using intermediate attention maps to iteratively compress high-dimensional 2D image features into a compact representation. This allows our model to selectively attend to relevant regions in the input while ignoring information irrelevant to the driving task, effectively associating the images with the BEV representation. In a new evaluation setting involving adverse environmental conditions and challenging scenarios, NEAT outperforms several strong baselines and achieves driving scores on par with the privileged CARLA expert used to generate its training data. Furthermore, visualizing the attention maps for models with NEAT intermediate representations provides improved interpretability. (@chitta2021neat)

Dian Chen, Vladlen Koltun, and Philipp Kr"ahenb"uhl Learning to drive from a world on rails In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 15590–15599, 2021. **Abstract:** We learn an interactive vision-based driving policy from pre-recorded driving logs via a model-based approach. A forward model of the world supervises a driving policy that predicts the outcome of any potential driving trajectory. To support learning from pre-recorded logs, we assume that the world is on rails, meaning neither the agent nor its actions influence the environment. This assumption greatly simplifies the learning problem, factorizing the dynamics into a non-reactive world model and a low-dimensional and compact forward model of the ego-vehicle. Our approach computes action-values for each training trajectory using a tabular dynamic-programming evaluation of the Bellman equations; these action-values in turn supervise the final vision-based driving policy. Despite the world-on-rails assumption, the final driving policy acts well in a dynamic and reactive world. It outperforms imitation learning as well as model-based and model-free reinforcement learning on the challenging CARLA NoCrash benchmark. It is also an order of magnitude more sample-efficient than state-of-the-art model-free reinforcement learning techniques on navigational tasks in the ProcGen benchmark. (@chen2021learning)

Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu, and Luc Van Gool End-to-end urban driving by imitating a reinforcement learning coach In *Proceedings of the IEEE/CVF international conference on computer vision*, pages 15222–15232, 2021. **Abstract:** End-to-end approaches to autonomous driving commonly rely on expert demonstrations. Although humans are good drivers, they are not good coaches for end-to-end algorithms that demand dense on-policy supervision. On the contrary, automated experts that leverage privileged information can efficiently generate large scale on-policy and off-policy demonstrations. However, existing automated experts for urban driving make heavy use of hand-crafted rules and perform suboptimally even on driving simulators, where ground-truth information is available. To ad-dress these issues, we train a reinforcement learning expert that maps bird’s-eye view images to continuous low-level actions. While setting a new performance upper-bound on CARLA, our expert is also a better coach that provides in-formative supervision signals for imitation learning agents to learn from. Supervised by our reinforcement learning coach, a baseline end-to-end agent with monocular camera-input achieves expert-level performance. Our end-to-end agent achieves a 78% success rate while generalizing to a new town and new weather on the NoCrash-dense bench-mark and state-of-the-art performance on the more challenging CARLA LeaderBoard. (@zhang2021end)

Penghao Wu, Xiaosong Jia, Li Chen, Junchi Yan, Hongyang Li, and Yu Qiao Trajectory-guided control prediction for end-to-end autonomous driving: A simple yet strong baseline , 35:6119–6132, 2022. **Abstract:** Current end-to-end autonomous driving methods either run a controller based on a planned trajectory or perform control prediction directly, which have spanned two separately studied lines of research. Seeing their potential mutual benefits to each other, this paper takes the initiative to explore the combination of these two well-developed worlds. Specifically, our integrated approach has two branches for trajectory planning and direct control, respectively. The trajectory branch predicts the future trajectory, while the control branch involves a novel multi-step prediction scheme such that the relationship between current actions and future states can be reasoned. The two branches are connected so that the control branch receives corresponding guidance from the trajectory branch at each time step. The outputs from two branches are then fused to achieve complementary advantages. Our results are evaluated in the closed-loop urban driving setting with challenging scenarios using the CARLA simulator. Even with a monocular camera input, the proposed approach ranks first on the official CARLA Leaderboard, outperforming other complex candidates with multiple sensors or fusion mechanisms by a large margin. The source code is publicly available at https://github.com/OpenPerceptionX/TCP (@wu2022trajectory)

Jimuyang Zhang, Zanming Huang, and Eshed Ohn-Bar Coaching a teachable student In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 7805–7815, 2023. **Abstract:** We propose a novel knowledge distillation framework for effectively teaching a sensorimotor student agent to drive from the supervision of a privileged teacher agent. Current distillation for sensorimotor agents methods tend to result in suboptimal learned driving behavior by the student, which we hypothesize is due to inherent differences between the input, modeling capacity, and optimization processes of the two agents. We develop a novel distillation scheme that can address these limitations and close the gap between the sensorimotor agent and its privileged teacher. Our key insight is to design a student which learns to align their input features with the teacher’s privileged Bird’s Eye View (BEV) space. The student then can benefit from direct supervision by the teacher over the internal representation learning. To scaffold the difficult sensorimotor learning task, the student model is optimized via a student-paced coaching mechanism with various auxiliary supervision. We further propose a high-capacity imitation learned privileged agent that surpasses prior privileged agents in CARLA and ensures the student learns safe driving behavior. Our proposed sensorimotor agent results in a robust image-based behavior cloning agent in CARLA, improving over current models by over 20.6% in driving score without requiring LiDAR, historical observations, ensemble of models, on-policy data aggregation or reinforcement learning. (@zhang2023coaching)

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei Imagenet: A large-scale hierarchical image database In *2009 IEEE conference on computer vision and pattern recognition*, pages 248–255. Ieee, 2009. **Abstract:** The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called "ImageNet", a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond. (@deng2009imagenet)

Yifan Hu, Lei Deng, Yujie Wu, Man Yao, and Guoqi Li Advancing spiking neural networks toward deep residual learning , 2024. **Abstract:** Despite the rapid progress of neuromorphic computing, inadequate capacity and insufficient representation power of spiking neural networks (SNNs) severely restrict their application scope in practice. Residual learning and shortcuts have been evidenced as an important approach for training deep neural networks, but rarely did previous work assessed their applicability to the specifics of SNNs. In this article, we first identify that this negligence leads to impeded information flow and the accompanying degradation problem in a spiking version of vanilla ResNet. To address this issue, we propose a novel SNN-oriented residual architecture termed MS-ResNet, which establishes membrane-based shortcut pathways, and further proves that the gradient norm equality can be achieved in MS-ResNet by introducing block dynamical isometry theory, which ensures the network can be well-behaved in a depth-insensitive way. Thus, we are able to significantly extend the depth of directly trained SNNs, e.g., up to 482 layers on CIFAR-10 and 104 layers on ImageNet, without observing any slight degradation problem. To validate the effectiveness of MS-ResNet, experiments on both frame-based and neuromorphic datasets are conducted. MS-ResNet104 achieves a superior result of 76.02% accuracy on ImageNet, which is the highest to the best of our knowledge in the domain of directly trained SNNs. Great energy efficiency is also observed, with an average of only one spike per neuron needed to classify an input sample. We believe our powerful and scalable models will provide strong support for further exploration of SNNs. (@hu2024advancing)

Philip Polack, Florent Altché, Brigitte d’Andréa Novel, and Arnaud de La Fortelle The kinematic bicycle model: A consistent model for planning feasible trajectories for autonomous vehicles? In *2017 IEEE intelligent vehicles symposium (IV)*, pages 812–818. IEEE, 2017. **Abstract:** Most autonomous driving architectures separate planning and control phases in different layers, even though both problems are intrinsically related. Due to limitations on the available computational power, their levels of abstraction and modeling differ; in particular, vehicle dynamics are often highly simplified at the planning phase, which may lead to inconsistency between the two layers. In this paper, we study the kinematic bicycle model, which is often used for trajectory planning, and compare its results to a 9 degrees of freedom model. Modeling errors and limitations of the kinematic bicycle model are highlighted. Lastly, this paper proposes a simple and efficient consistency criterion in order to validate the use of this model for planning purposes. (@polack2017kinematic)

Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi Yan, and Dacheng Tao St-p3: End-to-end vision-based autonomous driving via spatial-temporal feature learning In *European Conference on Computer Vision*, pages 533–549. Springer, 2022. **Abstract:** Many existing autonomous driving paradigms involve a multi- stage discrete pipeline of tasks. To better predict the control signals and enhance user safety, an end-to-end approach that benefits from joint spatial-temporal feature learning is desirable. While there are some pi- oneering works on LiDAR-based input or implicit design, in this paper we formulate the problem in an interpretable vision-based setting. In particular, we propose a spatial-temporal feature learning scheme to- wards a set of more representative features for perception, prediction and planning tasks simultaneously, which is called ST-P3. Specifically, an egocentric-aligned accumulation technique is proposed to preserve geometry information in 3D space before the bird’s eye view transfor- mation for perception; a dual pathway modeling is devised to take past motion variations into account for future prediction; a temporal-based refinement unit is introduced to compensate for recognizing vision-based elements for planning. To the best of our knowledge, we are the first to systematically investigate each part of an interpretable end-to-end vision-based autonomous driving system. We benchmark our approach against previous state-of-the-arts on both open-loop nuScenes dataset as well as closed-loop CARLA simulation. The results show the effective- ness of our method. Source code, model and protocol details are made publicly available at https://github.com/OpenPerceptionX/ST-P3 . 1 Introduction A classical paradigm design for autonomous driving systems often adopts a mod- ular based spirit \[2, 54\], where the input of a planning or controlling unit is based on the outputs from preceding modules in perception. As we witness the blossom of end-to-end algorithms and success applications into various domains \[41, 22\], there are some attempt implementing such a philosophy in autonomous driving as well \[44, 3, 15, 1, 12, 45, 57, 7, 8\]. Rather than an isolated staged pipeline, ∗Correspondence author.†Work done during internship at Shanghai AI Laboratory.arXiv:2207.07601v2 \[cs.CV\] 18 Jul 20222 S. Hu et al. Fig. 1. Problem setup whereby an interpretable vision-based end-to-end framework in (a) is devised, parallel to the LiDAR-based counterpart by aid of HD maps in (b) we aim for a framework to directly take raw sensor data as inputs and generate the planning routes or control signals. A straightforward incentive to do so is that feature representations can thus be optimized simultaneously within on (@hu2022st)

Ali Lotfi Rezaabad and Sriram Vishwanath Long short-term memory spiking networks and their applications In *International Conference on Neuromorphic Systems 2020*, pages 1–9, 2020. **Abstract:** Recent advances in event-based neuromorphic systems have resulted in significant interest in the use and development of spiking neural networks (SNNs). However, the non-differentiable nature of spiking neurons makes SNNs incompatible with conventional backpropagation techniques. In spite of the significant progress made in training conventional deep neural networks (DNNs), training methods for SNNs still remain relatively poorly understood. In this paper, we present a novel framework for training recurrent SNNs. Analogous to the benefits presented by recurrent neural networks (RNNs) in learning time series models within DNNs, we develop SNNs based on long short-term memory (LSTM) networks. We show that LSTM spiking networks learn the timing of the spikes and temporal dependencies. We also develop a methodology for error backpropagation within LSTM-based SNNs. The developed architecture and method for backpropagation within LSTM-based SNNs enable them to learn long-term dependencies with comparable results to conventional LSTMs. Code is available on github; https://github.com/AliLotfi92/SNNLSTM (@lotfi2020long)

Anthony Hu, Zak Murez, Nikhil Mohan, Sofı́a Dudas, Jeffrey Hawke, Vijay Badrinarayanan, Roberto Cipolla, and Alex Kendall Fiery: Future instance prediction in bird’s-eye view from surround monocular cameras In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 15273–15282, 2021. **Abstract:** Driving requires interacting with road agents and predicting their future behaviour in order to navigate safely. We present FIERY: a probabilistic future prediction model in bird’s-eye view from monocular cameras. Our model predicts future instance segmentation and motion of dynamic agents that can be transformed into non-parametric future trajectories. Our approach combines the perception, sensor fusion and prediction components of a traditional autonomous driving stack by estimating bird’s-eye-view prediction directly from surround RGB monocular camera inputs. FIERY learns to model the inherent stochastic nature of the future solely from camera driving data in an end-to-end manner, without relying on HD maps, and predicts multimodal future trajectories. We show that our model outperforms previous prediction baselines on the NuScenes and Lyft datasets. The code and trained models are available at https://github.com/wayveai/fiery. (@hu2021fiery)

Chenyang Lu, Marinus Jacobus Gerardus Van De Molengraft, and Gijs Dubbelman Monocular semantic occupancy grid mapping with convolutional variational encoder–decoder networks , 4(2):445–452, 2019. **Abstract:** In this work, we research and evaluate end-to-end learning of monocular semantic-metric occupancy grid mapping from weak binocular ground truth. The network learns to predict four classes, as well as a camera to bird’s eye view mapping. At the core, it utilizes a variational encoder-decoder network that encodes the front-view visual information of the driving scene and subsequently decodes it into a 2-D top-view Cartesian coordinate system. The evaluations on Cityscapes show that the end-to-end learning of semantic-metric occupancy grids outperforms the deterministic mapping approach with flat-plane assumption by more than 12% mean IoU. Furthermore, we show that the variational sampling with a relatively small embedding vector brings robustness against vehicle dynamic perturbations, and generalizability for unseen KITTI data. Our network achieves real-time inference rates of approx. 35 Hz for an input image with a resolution of 256x512 pixels and an output map with 64x64 occupancy grid cells using a Titan V GPU. (@lu2019monocular)

Bowen Pan, Jiankai Sun, Ho Yin Tiga Leung, Alex Andonian, and Bolei Zhou Cross-view semantic segmentation for sensing surroundings , 5(3):4867–4873, 2020. **Abstract:** Sensing surroundings plays a crucial role in human spatial perception, as it extracts the spatial configuration of objects as well as the free space from the observations. To facilitate the robot perception with such a surrounding sensing capability, we introduce a novel visual task called Cross-view Semantic Segmentation as well as a framework named View Parsing Network (VPN) to address it. In the cross-view semantic segmentation task, the agent is trained to parse the first-view observations into a top-down-view semantic map indicating the spatial location of all the objects at pixel-level. The main issue of this task is that we lack the real-world annotations of top-down-view data. To mitigate this, we train the VPN in 3D graphics environment and utilize the domain adaptation technique to transfer it to handle real-world data. We evaluate our VPN on both synthetic and real-world agents. The experimental results show that our model can effectively make use of the information from different views and multi-modalities to understanding spatial information. Our further experiment on a LoCoBot robot shows that our model enables the surrounding sensing capability from 2D image input. Code and demo videos can be found at \\}url{https://view-parsing-network.github.io}. (@pan2020cross)

Thomas Roddick and Roberto Cipolla Predicting semantic map representations from images using pyramid occupancy networks In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 11138–11147, 2020. **Abstract:** Autonomous vehicles commonly rely on highly detailed birds-eye-view maps of their environment, which capture both static elements of the scene such as road layout as well as dynamic elements such as other cars and pedestrians. Generating these map representations on the fly is a complex multi-stage process which incorporates many important vision-based elements, including ground plane estimation, road segmentation and 3D object detection. In this work we present a simple, unified approach for estimating these map representations directly from monocular images using a single end-to-end deep learning architecture. For the maps themselves we adopt a semantic Bayesian occupancy grid framework, allowing us to trivially accumulate information over multiple cameras and timesteps. We demonstrate the effectiveness of our approach by evaluating against several challenging baselines on the NuScenes and Argoverse datasets, and show that we are able to achieve a relative improvement of 9.1% and 22.3% respectively compared to the best-performing existing method. (@roddick2020predicting)

Jonah Philion and Sanja Fidler Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d In *Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIV 16*, pages 194–210. Springer, 2020. **Abstract:** The goal of perception for autonomous vehicles is to extract semantic representations from multiple sensors and fuse these represen- tations into a single \\}bird’s-eye-view" coordinate frame for consumption by motion planning. We propose a new end-to-end architecture that di- rectly extracts a bird’s-eye-view representation of a scene given image data from an arbitrary number of cameras. The core idea behind our approach is to \\}lift" each image individually into a frustum of features for each camera, then \\}splat" all frustums into a rasterized bird’s-eye- view grid. By training on the entire camera rig, we provide evidence that our model is able to learn not only how to represent images but how to fuse predictions from all cameras into a single cohesive representation of the scene while being robust to calibration error. On standard bird’s- eye-view tasks such as object segmentation and map segmentation, our model outperforms all baselines and prior work. In pursuit of the goal of learning dense representations for motion planning, we show that the representations inferred by our model enable interpretable end-to-end motion planning by \\}shooting" template trajectories into a bird’s-eye- view cost map output by our network. We benchmark our approach against models that use oracle depth from lidar. Project page with code: https://nv-tlabs.github.io/lift-splat-shoot.arXiv:2008.05711v1 \[cs.CV\] 13 Aug 20202 J. Philion et al. 1 Introduction Computer vision algorithms generally take as input an image and output either a prediction that is coordinate-frame agnostic { such as in classi cation \[19,30,16,17\] { or a prediction in the same coordinate frame as the input image { such as in object detection, semantic segmentation, or panoptic segmentation \[7,1,15,36\]. This paradigm does not match the setting for perception in self-driving out- of-the-box. In self-driving, multiple sensors are given as input, each with a dif- ferent coordinate frame, and perception models are ultimately tasked with pro- ducing predictions in a new coordinate frame { the frame of the ego car { for consumption by the downstream planner, as shown in Fig. 2. There are many simple, practical strategies for extending the single-image paradigm to the multi-view setting. For instance, for the problem of 3D object detection from ncameras, one can apply a single-image detector to all input images individually, then rotate and translate each detection into the ego frame according to the intrinsics a (@philion2020lift)

Hengli Wang, Peide Cai, Yuxiang Sun, Lujia Wang, and Ming Liu Learning interpretable end-to-end vision-based motion planning for autonomous driving with optical flow distillation In *2021 IEEE International Conference on Robotics and Automation (ICRA)*, pages 13731–13737. IEEE, 2021. **Abstract:** Recently, deep-learning based approaches have achieved impressive performance for autonomous driving. However, end-to-end vision-based methods typically have limited interpretability, making the behaviors of the deep networks difficult to explain. Hence, their potential applications could be limited in practice. To address this problem, we propose an interpretable end-to-end vision-based motion planning approach for autonomous driving, referred to as IVMP. Given a set of past surrounding-view images, our IVMP first predicts future egocentric semantic maps in bird’s-eye-view space, which are then employed to plan trajectories for self-driving vehicles. The predicted future semantic maps not only provide useful interpretable information, but also allow our motion planning module to handle objects with low probability, thus improving the safety of autonomous driving. Moreover, we also develop an optical flow distillation paradigm, which can effectively enhance the network while still maintaining its real-time performance. Extensive experiments on the nuScenes dataset and closed-loop simulation show that our IVMP significantly outperforms the state-of-the-art approaches in imitating human drivers with a much higher success rate. Our project page is available at https://sites.google.com/view/ivmp. (@wang2021learning)

Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom nuscenes: A multimodal dataset for autonomous driving In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 11621–11631, 2020. **Abstract:** Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online. (@caesar2020nuscenes)

Wei Fang, Yanqi Chen, Jianhao Ding, Zhaofei Yu, Timothée Masquelier, Ding Chen, Liwei Huang, Huihui Zhou, Guoqi Li, and Yonghong Tian Spikingjelly: An open-source machine learning infrastructure platform for spike-based intelligence , 9(40):eadi1480, 2023. **Abstract:** Spiking neural networks (SNNs) aim to realize brain-inspired intelligence on neuromorphic chips with high energy efficiency by introducing neural dynamics and spike properties. As the emerging spiking deep learning paradigm attracts increasing interest, traditional programming frameworks cannot meet the demands of the automatic differentiation, parallel computation acceleration, and high integration of processing neuromorphic datasets and deployment. In this work, we present the SpikingJelly framework to address the aforementioned dilemma. We contribute a full-stack toolkit for preprocessing neuromorphic datasets, building deep SNNs, optimizing their parameters, and deploying SNNs on neuromorphic chips. Compared to existing methods, the training of deep SNNs can be accelerated 11×, and the superior extensibility and flexibility of SpikingJelly enable users to accelerate custom models at low costs through multilevel inheritance and semiautomatic code generation. SpikingJelly paves the way for synthesizing truly energy-efficient SNN-based machine intelligence systems, which will enrich the ecology of neuromorphic computing. (@fang2023spikingjelly)

Wei Fang, Zhaofei Yu, Yanqi Chen, Tiejun Huang, Timothée Masquelier, and Yonghong Tian Deep residual learning in spiking neural networks , 34:21056–21069, 2021. **Abstract:** Deep Spiking Neural Networks (SNNs) present optimization difficulties for gradient-based approaches due to discrete binary activation and complex spatial-temporal dynamics. Considering the huge success of ResNet in deep learning, it would be natural to train deep SNNs with residual learning. Previous Spiking ResNet mimics the standard residual block in ANNs and simply replaces ReLU activation layers with spiking neurons, which suffers the degradation problem and can hardly implement residual learning. In this paper, we propose the spike-element-wise (SEW) ResNet to realize residual learning in deep SNNs. We prove that the SEW ResNet can easily implement identity mapping and overcome the vanishing/exploding gradient problems of Spiking ResNet. We evaluate our SEW ResNet on ImageNet, DVS Gesture, and CIFAR10-DVS datasets, and show that SEW ResNet outperforms the state-of-the-art directly trained SNNs in both accuracy and time-steps. Moreover, SEW ResNet can achieve higher performance by simply adding more layers, providing a simple method to train deep SNNs. To our best knowledge, this is the first time that directly training deep SNNs with more than 100 layers becomes possible. Our codes are available at https://github.com/fangwei123456/Spike-Element-Wise-ResNet. (@fang2021deep)

Nitin Rathi, Gopalakrishnan Srinivasan, Priyadarshini Panda, and Kaushik Roy Enabling deep spiking neural networks with hybrid conversion and spike timing dependent backpropagation , 2020. **Abstract:** Spiking Neural Networks (SNNs) operate with asynchronous discrete events (or spikes) which can potentially lead to higher energy-efficiency in neuromorphic hardware implementations. Many works have shown that an SNN for inference can be formed by copying the weights from a trained Artificial Neural Network (ANN) and setting the firing threshold for each layer as the maximum input received in that layer. These type of converted SNNs require a large number of time steps to achieve competitive accuracy which diminishes the energy savings. The number of time steps can be reduced by training SNNs with spike-based backpropagation from scratch, but that is computationally expensive and slow. To address these challenges, we present a computationally-efficient training technique for deep SNNs. We propose a hybrid training methodology: 1) take a converted SNN and use its weights and thresholds as an initialization step for spike-based backpropagation, and 2) perform incremental spike-timing dependent backpropagation (STDB) on this carefully initialized network to obtain an SNN that converges within few epochs and requires fewer time steps for input processing. STDB is performed with a novel surrogate gradient function defined using neuron’s spike time. The proposed training methodology converges in less than 20 epochs of spike-based backpropagation for most standard image classification datasets, thereby greatly reducing the training complexity compared to training SNNs from scratch. We perform experiments on CIFAR-10, CIFAR-100, and ImageNet datasets for both VGG and ResNet architectures. We achieve top-1 accuracy of 65.19% for ImageNet dataset on SNN with 250 time steps, which is 10X faster compared to converted SNNs with similar accuracy. (@rathi2020Enabling)

Jibin Wu, Chenglin Xu, Xiao Han, Daquan Zhou, Malu Zhang, Haizhou Li, and Kay Chen Tan Progressive tandem learning for pattern recognition with deep spiking neural networks , 44(11):7824–7840, 2021. **Abstract:** Spiking neural networks (SNNs) have shown clear advantages over traditional artificial neural networks (ANNs) for low latency and high computational efficiency, due to their event-driven nature and sparse communication. However, the training of deep SNNs is not straightforward. In this paper, we propose a novel ANN-to-SNN conversion and layer-wise learning framework for rapid and efficient pattern recognition, which is referred to as progressive tandem learning. By studying the equivalence between ANNs and SNNs in the discrete representation space, a primitive network conversion method is introduced that takes full advantage of spike count to approximate the activation value of ANN neurons. To compensate for the approximation errors arising from the primitive network conversion, we further introduce a layer-wise learning method with an adaptive training scheduler to fine-tune the network weights. The progressive tandem learning framework also allows hardware constraints, such as limited weight precision and fan-in connections, to be progressively imposed during training. The SNNs thus trained have demonstrated remarkable classification and regression capabilities on large-scale object recognition, image reconstruction, and speech separation tasks, while requiring at least an order of magnitude reduced inference time and synaptic operations than other state-of-the-art SNN implementations. It, therefore, opens up a myriad of opportunities for pervasive mobile and embedded devices with a limited power budget. (@wu2021progressive)

Man Yao, Guangshe Zhao, Hengyu Zhang, Yifan Hu, Lei Deng, Yonghong Tian, Bo Xu, and Guoqi Li Attention spiking neural networks , 2023. **Abstract:** Brain-inspired spiking neural networks (SNNs) are becoming a promising energy-efficient alternative to traditional artificial neural networks (ANNs). However, the performance gap between SNNs and ANNs has been a significant hindrance to deploying SNNs ubiquitously. To leverage the full potential of SNNs, in this paper we study the attention mechanisms, which can help human focus on important information. We present our idea of attention in SNNs with a multi-dimensional attention module, which infers attention weights along the temporal, channel, as well as spatial dimension separately or simultaneously. Based on the existing neuroscience theories, we exploit the attention weights to optimize membrane potentials, which in turn regulate the spiking response. Extensive experimental results on event-based action recognition and image classification datasets demonstrate that attention facilitates vanilla SNNs to achieve sparser spiking firing, better performance, and energy efficiency concurrently. In particular, we achieve top-1 accuracy of 75.92% and 77.08% on ImageNet-1 K with single/4-step Res-SNN-104, which are state-of-the-art results in SNNs. Compared with counterpart Res-ANN-104, the performance gap becomes -0.95/+0.21 percent and the energy efficiency is 31.8×/7.4×. To analyze the effectiveness of attention SNNs, we theoretically prove that the spiking degradation or the gradient vanishing, which usually holds in general SNNs, can be resolved by introducing the block dynamical isometry theory. We also analyze the efficiency of attention SNNs based on our proposed spiking response visualization method. Our work lights up SNN’s potential as a general backbone to support various applications in the field of SNN research, with a great balance between effectiveness and energy efficiency. (@yao2023attention)

Dahun Kim, Sanghyun Woo, Joon-Young Lee, and In So Kweon Video panoptic segmentation In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 9859–9868, 2020. **Abstract:** Panoptic segmentation has become a new standard of visual recognition task by unifying previous semantic segmentation and instance segmentation tasks in concert. In this paper, we propose and explore a new video extension of this task, called video panoptic segmentation. The task requires generating consistent panoptic segmentation as well as an association of instance ids across video frames. To invigorate research on this new task, we present two types of video panoptic datasets. The first is a re-organization of the synthetic VIPER dataset into the video panoptic format to exploit its large-scale pixel annotations. The second is a temporal extension on the Cityscapes val. set, by providing new video panoptic annotations (Cityscapes-VPS). Moreover, we propose a novel video panoptic segmentation network (VPSNet) which jointly predicts object classes, bounding boxes, masks, instance id tracking, and semantic segmentation in video frames. To provide appropriate metrics for this task, we propose a video panoptic quality (VPQ) metric and evaluate our method and several other baselines. Experimental results demonstrate the effectiveness of the presented two datasets. We achieve state-of-the-art results in image PQ on Cityscapes and also in VPQ on Cityscapes-VPS and VIPER datasets. (@kim2020video)

</div>

# **APPENDIX** [appendix]

# Spiking ResNet Architecture [appendix:ResNet]

MS-ResNet `\cite{hu2024advancing}`{=latex} and SEW-ResNet `\cite{fang2021deep}`{=latex} are two directly trained residual learning methods for SNNs, primarily aiming to overcome the degradation of spiking activity in training deep SNNs. In classic CNNs, ResNet achieves "very deep" neural networks by attaching an identity mapping skip connection throughout the entire network. However, directly copying the classic residual structure to SNNs causes the training loss increases as the network deepens. The underlying reason is that CNNs generate continuous valued activations, while SNNs generate discrete spikes.

To address this issue, SEW-ResNet and MS-ResNet establish shortcut connections between spikes or membrane potentials of different layers, respectively. SEW-ResNet builds shortcuts between the output spikes of different layers, thereby obtaining an identity mapping. On the other hand, MS-ResNet constructs shortcuts between the membrane potentials of spiking neurons in different layers. Fig. <a href="#fig:resnet" data-reference-type="ref" data-reference="fig:resnet">6</a> illustrates the differences between these two methods. By building residual learning on membrane potentials, MS-ResNet achieves higher accuracy than SEW-ResNet.

# Training

## Pretraining on Spiking Token Mixer (STM) [appendix:pretrain]

In STM, the architecture is similar to Spiking Transformers, consisting of a Spiking Patch Embedding (SPS) layer followed by a Transformer-like architecture. However, unlike Spiking Transformers, the SPS layer in STM is not followed by a self-attention layer. We pre-train the STM on ImageNet-1K for 300 epochs, aligning with other SNNs that are pre-trained on ImageNet. The input size is set to 224 × 224, and the batch size is set to 128 or 256 during the 310 training epochs with a cosine-decay learning rate whose initial value is 0.0005. The optimizer used is Lamb. The SPS module divides the image into N = 196 patches. Standard data augmentation techniques, such as random augmentation and mixup, are also employed during training. We compare our method with other SNN methods of similar size trained on ImageNet-1K, as shown in Tab. <a href="#table_imagenet_result" data-reference-type="ref" data-reference="table_imagenet_result">[table_imagenet_result]</a>.

<div class="center" markdown="1">

<div class="tabular" markdown="1">

cccccc & & & & &  
& ResNet-34 `\cite{rathi2020Enabling}`{=latex} & & 21.8 & 250 & 61.5  
& VGG-16 `\cite{wu2021progressive}`{=latex} & & - & 16 & 65.1  

& SEW-Res-SNN `\cite{fang2021deep}`{=latex} && 25.6 & 4 & 67.8  
& MS-Res-SNN `\cite{hu2024advancing}`{=latex} & &21.8 & 4 & 69.4  
& Att-MS-Res-SNN `\cite{yao2023attention}`{=latex} & &22.1 & 1 & 69.2  
Spiking-& && 16.8 & 4 & 70.2  
Transformer&&&16.8& 4 & 72.3  

SMLP &&&12.2& 4 & 72.1  

</div>

</div>

<span id="table_imagenet_result" label="table_imagenet_result"></span>

## Stage-wise Training of the End-to-end Model [appendix:stage_training]

Although our model is end-to-end differentiable, directly implementing end-to-end training can lead to unstable loss convergence. To mitigate this issue, we divide our training process into three stages:

#### Stage 1: Perception Module Training

In the first stage, we focus on training the perception module while disabling the prediction and planning modules. This means that the prediction module within the network remains inactive, and the decoder does not output instance flow information related to prediction. During this stage, we use a learning rate of \\(1e-3\\) and train the model for 20 epochs.

<figure id="fig:resnet">
<img src="./figures/mssew.png"" style="width:60.0%" />
<figcaption>MS-ResNet and SEW-ResNet.</figcaption>
</figure>

#### Stage 2: Prediction Module Training

In the second stage, we enable the prediction module, which predicts the movement of instances on the BEV for the next three steps. We train the model for an additional 10 epochs during this stage.

#### Stage 3: Planning Module Training

Finally, we start training the planning module, using real trajectories to supervise the model’s planning results. In both the prediction and planning stages, we set the learning rate to \\(2e-4\\) to fine-tune the model from the perception stage. Throughout all stages, we employ the Adam optimizer to update the model parameters. By incrementally introducing the prediction and planning modules, we ensure that each component is well-trained before integrating them into the end-to-end framework.

# Experiments

## Datasets [appendix:dataset]

The nuScenes dataset `\cite{caesar2020nuscenes}`{=latex} is a comprehensive, publicly available dataset tailored for autonomous driving research. It encompasses 1,000 manually selected driving scenes from Boston and Singapore—cities renowned for their dense traffic and complex driving environments. Each scene, lasting 20 seconds, showcases a variety of driving maneuvers, traffic conditions, and unforeseen events, reflecting the intricate dynamics of urban driving. The dataset aims to foster the development of advanced methods that ensure safety in densely populated urban settings, featuring dozens of objects per scene. It includes around 1.4 million camera images, 390,000 LIDAR sweeps, 1.4 million RADAR sweeps, and 1.4 million object-bounding boxes across 40,000 keyframes.

**Data Preprocessing.** In preparation for modeling, we preprocess the images by cropping and resizing the original dimensions from \\(\mathbb{R}^{3 \times 900 \times 1600}\\) to \\(\mathbb{R}^{3 \times 224 \times 480}\\). Additionally, we incorporate temporal data by including the past three frames for each sequence, denoted as \\(I_t^n \in \mathbb{R}^{3 \times 224 \times 480}\\), where \\(t \in\{1,2,3\}\\) represents the frame indices, and \\(n \in\{1, \ldots, 6\}\\) indexes the cameras.

## Evaluation Metrics [appendix:metrics]

In this section, we introduce the metrics used to assess the predictive capabilities of our model for video prediction. Specifically, we utilize Intersection over Union (IoU), Panoptic Quality (PQ), Recognition Quality (RQ), and Segmentation Quality (SQ) as defined by Kim et al. `\cite{kim2020video}`{=latex}.

#### Intersection over Union (IoU)

The IoU metric evaluates the overlap between the predicted and ground truth segmentation masks. It is calculated as: \\[IoU = \frac{|\text{Prediction} \cap \text{Ground Truth}|}{|\text{Prediction} \cup \text{Ground Truth}|}\\] where \\(|\cdot|\\) denotes the number of pixels in the respective set.

#### Panoptic Quality (PQ)

Panoptic Quality (PQ) is a comprehensive metric that combines both segmentation and recognition quality. It accounts for both true positive matches and penalizes false positives and false negatives. PQ is defined as: \\[PQ = \frac{\sum_{(p,g) \in TP} IoU(p, g)}{|TP| + \frac{1}{2}|FP| + \frac{1}{2}|FN|}\\] where \\(TP\\) represents the set of true positive matches between predicted segments \\(p\\) and ground truth segments \\(g\\), \\(FP\\) denotes false positives, and \\(FN\\) denotes false negatives.

#### Recognition Quality (RQ)

Recognition Quality (RQ) measures the accuracy of object recognition and classification. It is given by: \\[RQ = \frac{|TP|}{|TP| + \frac{1}{2}|FP| + \frac{1}{2}|FN|}\\]

#### Segmentation Quality (SQ)

Segmentation Quality (SQ) evaluates the quality of the predicted segments’ spatial accuracy. It is defined as: \\[SQ = \frac{\sum_{(p,g) \in TP} IoU(p, g)}{|TP|}\\]

## More Visualizations [appendix:visual]

We present three additional visual examples in Fig <a href="#fig:visual2" data-reference-type="ref" data-reference="fig:visual2">7</a> to further showcase the performance of our SAD model on the nuScenes dataset. These examples demonstrate the model’s ability to generate safe and collision-free trajectories in various driving scenarios, highlighting its effectiveness in autonomous driving applications.

<figure id="fig:visual2">
<img src="./figures/visual2.png"" />
<figcaption><strong>More Qualitative Results of the SAD Model on the nuScenes Dataset.</strong> (a) displays six camera view inputs utilized by the model. (b) illustrates the planning result of the ANN model, and (c) presents the planning results of our SAD model. The comparison shows that our SAD model can achieve performance comparable to that of the ANN model and successfully generate a safe trajectory.</figcaption>
</figure>

# Detail of Cost Function [appendix:cost]

In the method section, we briefly introduced the Cost Function \\(f\\), a multi-component function designed to evaluate the suitability of planned trajectories for self-driving vehicles. We follow the cost function in ST-P3 `\cite{hu2022st}`{=latex}, each component of the cost function addresses a specific aspect of the trajectory planning, as detailed below:

**Safety Cost.** This component ensures that the planned trajectories do not result in collisions. The model must account for the dynamic positioning of other vehicles and road elements, avoiding overlap with the grids they occupy. Furthermore, a safe distance must be maintained, especially at high velocities, to prevent any potential accidents.

**Cost Volume.** Road environments are complex, with numerous unpredictable elements affecting trajectory planning. It is impractical to manually enumerate all possible scenarios and associated costs. To address this, we utilize a learned cost volume produced by the prediction module head mentioned earlier. To prevent the cost volume from disproportionately influencing the trajectory evaluation, we clip its values to maintain balanced decision-making.

**Comfort and Progress.** To ensure that the trajectories are not only safe but also comfortable for passengers, this component penalizes excessive lateral acceleration, jerk, and high curvature. Additionally, the efficiency of the trajectory is critical; thus, paths that effectively progress towards the target destination receive positive reinforcement.

These components collectively ensure that the cost function comprehensively assesses both the safety and quality of the trajectories planned by the self-driving vehicle.

# Theoretical Energy Consumption Calculation [appendix:energy]

In the experimental section of our paper, we utilize energy consumption as a metric to evaluate the efficiency of various models. This appendix outlines the methodology employed to compute the theoretical energy consumption of our SNN architecture. The calculation process involves two main steps: first, determining the synaptic operations (SOPs) for each architectural component, and second, estimating the cumulative energy consumption based on these operations.

The synaptic operations within each block of the SNN are calculated using the following equation: \\[\operatorname{SOPs}(l) = fr \times T \times \operatorname{FLOPs}(l)\\] where \\(l\\) denotes the ordinal number of the block within the SNN, \\(fr\\) represents the firing rate of the input spike train to the block, \\(T\\) indicates the time step of the neuron, and \\(\operatorname{FLOPs}(l)\\) refers to the floating-point operations within the block, primarily consisting of multiply-and-accumulate (MAC) operations.

To compute the SNN’s theoretical energy consumption, we consider both MAC and spike-based accumulation (AC) operations, utilizing 45 nm technology. The energy costs are \\(E_{MAC} = 4.6 \, \text{pJ}\\) for MAC operations and \\(E_{AC} = 0.9 \, \text{pJ}\\) for AC operations. Based on the methodologies outlined in `\cite{yao2023attention}`{=latex}, we calculate the energy consumption of the SNN as follows: \\[\begin{aligned}
E_{SNN} &= E_{MAC} \times \mathrm{FLOP}_{\mathrm{SNN}_\mathrm{Conv}}^1 \\
&+ E_{AC} \times \left(\sum_{n=2}^N \mathrm{SOP}_{\mathrm{SNN}_\mathrm{Conv}}^n + \sum_{m=1}^M \mathrm{SOP}_{\mathrm{SNN}_\mathrm{FC}}^m\right)
\end{aligned}\\] Here, \\(N\\) and \\(M\\) denote the number of convolutional (Conv) and fully connected (FC) layers, respectively. The terms \\(\mathrm{FLOP}_{\mathrm{SNN}_\mathrm{Conv}}\\) and \\(\mathrm{SOP}_{\mathrm{SNN}_\mathrm{Conv}}\\), \\(\mathrm{SOP}_{\mathrm{SNN}_\mathrm{FC}}\\) represent the FLOPs for the first Conv layer and SOPs for the subsequent \\(n^{th}\\) Conv and \\(m^{th}\\) FC layers.

For the ANN model, the formula for computing the theoretical energy consumption is more straightforward, as all neurons operate in floating point: \\[E_{ANN} = E_{MAC} \times \mathrm{FLOP}_{\mathrm{ANN}}\\] Here, \\(\mathrm{FLOP}_{\mathrm{ANN}}\\) represents the FLOPs of the ANN model.

[^1]: Corresponding author, jsn@ucsc.edu
