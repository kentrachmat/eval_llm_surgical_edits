# References [references]

<div class="thebibliography" markdown="1">

Jacob Andreas Good-Enough Compositional Data Augmentation In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*, pages 7556–7566, Online, 2020. Association for Computational Linguistics. . URL <https://www.aclweb.org/anthology/2020.acl-main.676>. **Abstract:** We propose a simple data augmentation protocol aimed at providing a compositional inductive bias in conditional and unconditional sequence models. Under this protocol, synthetic training examples are constructed by taking real training examples and replacing (possibly discontinuous) fragments with other fragments that appear in at least one similar environment. The protocol is model-agnostic and useful for a variety of tasks. Applied to neural sequence-to-sequence models, it reduces error rate by as much as 87% on diagnostic tasks from the SCAN dataset and 16% on a semantic parsing task. Applied to n-gram language models, it reduces perplexity by roughly 1% on small corpora in several languages. (@andreas_good-enough_2020)

Samy Badreddine, Artur d’Avila Garcez, Luciano Serafini, and Michael Spranger Logic tensor networks *Artificial Intelligence*, 303: 103649, 2022. ISSN 0004-3702. . URL <https://www.sciencedirect.com/science/article/pii/S0004370221002009>. **Abstract:** Attempts at combining logic and neural networks into neurosymbolic approaches have been on theincreaseinrecentyears. Inaneurosymbolicsystem,symbolicknowledgeassistsdeeplearning, which typically uses a sub-symbolic distributed representation, to learn and reason at a higher level of abstraction. We present Logic Tensor Networks (LTN), a neurosymbolic framework that supports querying, learning and reasoning with both rich data and abstract knowledge about the world. LTN introduces a fully diﬀerentiable logical language, called Real Logic, whereby the elementsofaﬁrst-orderlogicsignaturearegroundedontodatausingneuralcomputationalgraphs andﬁrst-orderfuzzylogicsemantics. WeshowthatLTNprovidesauniformlanguagetorepresent and compute eﬃciently many of the most important AI tasks such as multi-label classiﬁcation, relational learning, data clustering, semi-supervised learning, regression, embedding learning and query answering. We implement and illustrate each of the above tasks with several simple explanatory examples using TensorFlow 2. The results indicate that LTN can be a general and powerful framework for neurosymbolic AI. (@BADREDDINE2022103649)

Yonatan Belinkov and James Glass Analysis methods in neural language processing: A survey *Transactions of the Association for Computational Linguistics*, 7: 49–72, 2019. **Abstract:** Abstract The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work. (@belinkov2019analysis)

Tarek R. Besold, Artur S. d’Avila Garcez, Sebastian Bader, Howard Bowman, Pedro M. Domingos, Pascal Hitzler, Kai-Uwe Kühnberger, Luı́s C. Lamb, Daniel Lowd, Priscila Machado Vieira Lima, Leo de Penning, Gadi Pinkas, Hoifung Poon, and Gerson Zaverucha Neural-symbolic learning and reasoning: A survey and interpretation *CoRR*, abs/1711.03902, 2017. URL <http://arxiv.org/abs/1711.03902>. **Abstract:** The study and understanding of human behaviour is relevant to computer science, artificial intelligence, neural computation, cognitive science, philosophy, psychology, and several other areas. Presupposing cognition as basis of behaviour, among the most prominent tools in the modelling of behaviour are computational-logic systems, connectionist models of cognition, and models of uncertainty. Recent studies in cognitive science, artificial intelligence, and psychology have produced a number of cognitive models of reasoning, learning, and language that are underpinned by computation. In addition, efforts in computer science research have led to the development of cognitive computational systems integrating machine learning and automated reasoning. Such systems have shown promise in a range of applications, including computational biology, fault diagnosis, training and assessment in simulators, and software verification. This joint survey reviews the personal ideas and views of several researchers on neural-symbolic learning and reasoning. The article is organised in three parts: Firstly, we frame the scope and goals of neural-symbolic computation and have a look at the theoretical foundations. We then proceed to describe the realisations of neural-symbolic computation, systems, and applications. Finally we present the challenges facing the area and avenues for further research. (@nslr)

Chris M. Bishop Training with noise is equivalent to tikhonov regularization *Neural Computation*, 7 (1): 108–116, 1995. . **Abstract:** It is well known that the addition of noise to the input data of a neural network during training can, in some circumstances, lead to significant improvements in generalization performance. Previous work has shown that such training with noise is equivalent to a form of regularization in which an extra term is added to the error function. However, the regularization term, which involves second derivatives of the error function, is not bounded below, and so can lead to difficulties if used directly in a learning algorithm based on error minimization. In this paper we show that for the purposes of network training, the regularization term can be reduced to a positive semi-definite form that involves only first derivatives of the network mapping. For a sum-of-squares error function, the regularization term belongs to the class of generalized Tikhonov regularizers. Direct minimization of the regularized error function provides a practical alternative to training with noise. (@bishop-noise)

Terra Blevins, Omer Levy, and Luke Zettlemoyer Deep rnns encode soft hierarchical syntax *arXiv preprint arXiv:1805.04218*, 2018. **Abstract:** We present a set of experiments to demonstrate that deep recurrent neural networks (RNNs) learn internal representations that capture soft hierarchical notions of syntax from highly varied supervision. We consider four syntax tasks at different depths of the parse tree; for each word, we predict its part of speech as well as the first (parent), second (grandparent) and third level (great-grandparent) constituent labels that appear above it. These predictions are made from representations produced at different depths in networks that are pretrained with one of four objectives: dependency parsing, semantic role labeling, machine translation, or language modeling. In every case, we find a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges. This effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision. (@blevins2018deep)

Xinyun Chen, Chang Liu, and Dawn Song Tree-to-tree neural networks for program translation In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, *Advances in Neural Information Processing Systems*, volume 31. Curran Associates, Inc., 2018. URL <https://proceedings.neurips.cc/paper_files/paper/2018/file/d759175de8ea5b1d9a2660e45554894f-Paper.pdf>. **Abstract:** Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to employ deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects. (@NEURIPS2018_d759175d)

Xinyun Chen, Chen Liang, Adams Wei Yu, Dawn Song, and Denny Zhou Compositional generalization via neural-symbolic stack machines *Advances in Neural Information Processing Systems*, 33: 1690–1701, 2020. **Abstract:** Despite achieving tremendous success, existing deep learning models have exposed limitations in compositional generalization, the capability to learn compositional rules and apply them to unseen cases in a systematic manner. To tackle this issue, we propose the Neural-Symbolic Stack Machine (NeSS). It contains a neural network to generate traces, which are then executed by a symbolic stack machine enhanced with sequence manipulation operations. NeSS combines the expressive power of neural sequence models with the recursion supported by the symbolic stack machine. Without training supervision on execution traces, NeSS achieves 100% generalization performance in four domains: the SCAN benchmark of language-driven navigation tasks, the task of few-shot learning of compositional instructions, the compositional machine translation benchmark, and context-free grammar parsing tasks. (@chen2020compositional)

Noam Chomsky *Aspects of the theory of syntax* Number no. 11 in Massachusetts Institute of Technology. Research Laboratory of Electronics. Special technical report. The MIT Press, Cambridge, Massachusetts, 50th anniversary edition edition, 1965. ISBN 978-0-262-52740-8. **Abstract:** Abstract : Contents: Methodological preliminaries: Generative grammars as theories of linguistic competence; theory of performance; organization of a generative grammar; justification of grammars; formal and substantive grammars; descriptive and explanatory theories; evaluation procedures; linguistic theory and language learning; generative capacity and its linguistic relevance Categories and relations in syntactic theory: Scope of the base; aspects of deep structure; illustrative fragment of the base component; types of base rules Deep structures and grammatical transformations Residual problems: Boundaries of syntax and semantics; structure of the lexicon (@chomsky_aspects_1965)

Noam Chomsky *The Minimalist Program* The MIT Press, 12 2014. ISBN 9780262327282. . URL <https://doi.org/10.7551/mitpress/9780262527347.001.0001>. **Abstract:** A classic work that situates linguistic theory in the broader cognitive sciences, formulating and developing the minimalist program. In his foundational book, The Minimalist Program, published in 1995, Noam Chomsky offered a significant contribution to the generative tradition in linguistics. This twentieth-anniversary edition reissues this classic work with a new preface by the author. In four essays, Chomsky attempts to situate linguistic theory in the broader cognitive sciences, with the essays formulating and progressively developing the minimalist approach to linguistic Building on the theory of principles and parameters and, in particular, on principles of economy of derivation and representation, the minimalist framework takes Universal Grammar as providing a unique computational system, with derivations driven by morphological properties, to which the syntactic variation of languages also restricted. Within this theoretical framework, linguistic expressions are generated by optimally efficient derivations that must satisfy the conditions that hold on interface levels, the only levels of linguistic representation. The interface levels provide instructions to two types of performance systems, articulatory-perceptual and conceptual-intentional. All syntactic conditions, then, express properties of these interface levels, reflecting the interpretive requirements of language and keeping to very restricted conceptual resources. In the preface to this edition, Chomsky emphasizes that the minimalist approach developed in the book and in subsequent work is a program, not a theory. With this book, Chomsky built on pursuits from the earliest days of generative grammar to formulate a new research program that had far-reaching implications for the field. (@10.7551/mitpress/9780262527347.001.0001)

Henry Conklin, Bailin Wang, Kenny Smith, and Ivan Titov Meta-learning to compositionally generalize *arXiv preprint arXiv:2106.04252*, 2021. **Abstract:** Natural language is compositional; the meaning of a sentence is a function of the meaning of its parts. This property allows humans to create and interpret novel sentences, generalizing robustly outside their prior experience. Neural networks have been shown to struggle with this kind of generalization, in particular performing poorly on tasks designed to assess compositional generalization (i.e. where training and testing distributions differ in ways that would be trivial for a compositional strategy to resolve). Their poor performance on these tasks may in part be due to the nature of supervised learning which assumes training and testing data to be drawn from the same distribution. We implement a meta-learning augmented version of supervised learning whose objective directly optimizes for out-of-distribution generalization. We construct pairs of tasks for meta-learning by sub-sampling existing training data. Each pair of tasks is constructed to contain relevant examples, as determined by a similarity metric, in an effort to inhibit models from memorizing their input. Experimental results on the COGS and SCAN datasets show that our similarity-driven meta-learning can improve generalization performance. (@conklin2021meta)

Róbert Csordás, Kazuki Irie, and Juergen Schmidhuber The devil is in the detail: Simple tricks improve systematic generalization of transformers In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages 619–634, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. . URL <https://aclanthology.org/2021.emnlp-main.49>. **Abstract:** Recently, many datasets have been proposed to test the systematic generalization ability of neural networks. The companion baseline Transformers, typically trained with default hyper-parameters from standard tasks, are shown to fail dramatically. Here we demonstrate that by revisiting model configurations as basic as scaling of embeddings, early stopping, relative positional embedding, and Universal Transformer variants, we can drastically improve the performance of Transformers on systematic generalization. We report improvements on five popular datasets: SCAN, CFQ, PCFG, COGS, and Mathematics dataset. Our models improve accuracy from 50% to 85% on the PCFG productivity split, and from 35% to 81% on COGS. On SCAN, relative positional embedding largely mitigates the EOS decision problem (Newman et al., 2020), yielding 100% accuracy on the length split with a cutoff at 26. Importantly, performance differences between these models are typically invisible on the IID data split. This calls for proper generalization validation sets for developing neural networks that generalize systematically. We publicly release the code to reproduce our results. (@csordas-etal-2021-devil)

Fernando Cuetos, Don C Mitchell, and Martin MB Corley Parsing in different languages In *Language processing in Spanish*, pages 163–208. Psychology Press, 2013. **Abstract:** The function of the parser is to compute the syntactic structure of sentences, allowing the reader or listener to determine who did what and to whom and, more generally, to infer appropriate relationships between statements and entities expressed by the sentence. Among other things , its raw material consists of the order of occurrence of the individual words in the sentence, together with detailed information about these words (e.g., major category class; subcategorization information; inflections conveying information about number, gender and case; and so on). Other inputs include intonation, prosody, and punctuation. (@cuetos2013parsing)

Lennert De Smet, Pedro Zuidberg Dos Martires, Robin Manhaeve, Giuseppe Marra, Angelika Kimmig, and Luc De Readt Neural probabilistic logic programming in discrete-continuous domains In Robin J. Evans and Ilya Shpitser, editors, *Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence*, volume 216 of *Proceedings of Machine Learning Research*, pages 529–538. PMLR, 31 Jul–04 Aug 2023. URL <https://proceedings.mlr.press/v216/de-smet23a.html>. **Abstract:** Neural-symbolic AI (NeSy) allows neural networks to exploit symbolic background knowledge in the form of logic. It has been shown to aid learning in the limited data regime and to facilitate inference on out-of-distribution data. Probabilistic NeSy focuses on integrating neural networks with both logic and probability theory, which additionally allows learning under uncertainty. A major limitation of current probabilistic NeSy systems, such as DeepProbLog, is their restriction to finite probability distributions, i.e., discrete random variables. In contrast, deep probabilistic programming (DPP) excels in modelling and optimising continuous probability distributions. Hence, we introduce DeepSeaProbLog, a neural probabilistic logic programming language that incorporates DPP techniques into NeSy. Doing so results in the support of inference and learning of both discrete and continuous probability distributions under logical constraints. Our main contributions are 1) the semantics of DeepSeaProbLog and its corresponding inference algorithm, 2) a proven asymptotically unbiased learning algorithm, and 3) a series of experiments that illustrate the versatility of our approach. (@pmlr-v216-de-smet23a)

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova : Pre-training of Deep Bidirectional Transformers for Language Understanding *arXiv:1810.04805 \[cs\]*, May 2019. URL <http://arxiv.org/abs/1810.04805>. arXiv: 1810.04805. **Abstract:** We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement). (@devlin_bert_2019)

Li Dong and Mirella Lapata Language to logical form with neural attention *arXiv preprint arXiv:1601.01280*, 2016. **Abstract:** Semantic parsing aims at mapping natural language to machine interpretable meaning representations. Traditional approaches rely on high-quality lexicons, manually-built templates, and linguistic features which are either domain- or representation-specific. In this paper we present a general method based on an attention-enhanced encoder-decoder model. We encode input utterances into vector representations, and generate their logical forms by conditioning the output sequences or trees on the encoding vectors. Experimental results on four datasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations. (@dong2016language)

Brian DuSell and David Chiang Stack attention: Improving the ability of transformers to model hierarchical patterns In *The Twelfth International Conference on Learning Representations*, 2024. URL <https://openreview.net/forum?id=XVhm3X8Fum>. **Abstract:** Attention, specifically scaled dot-product attention, has proven effective for natural language, but it does not have a mechanism for handling hierarchical patterns of arbitrary nesting depth, which limits its ability to recognize certain syntactic structures. To address this shortcoming, we propose stack attention: an attention operator that incorporates stacks, inspired by their theoretical connections to context-free languages (CFLs). We show that stack attention is analogous to standard attention, but with a latent model of syntax that requires no syntactic supervision. We propose two variants: one related to deterministic pushdown automata (PDAs) and one based on nondeterministic PDAs, which allows transformers to recognize arbitrary CFLs. We show that transformers with stack attention are very effective at learning CFLs that standard transformers struggle on, achieving strong results on a CFL with theoretically maximal parsing difficulty. We also show that stack attention is more effective at natural language modeling under a constrained parameter budget, and we include results on machine translation. (@dusell2024stack)

Daniel Furrer, Marc van Zee, Nathan Scales, and Nathanael Schärli Compositional generalization in semantic parsing: Pre-training vs. specialized architectures *arXiv preprint arXiv:2007.08970*, 2020. **Abstract:** While mainstream machine learning methods are known to have limited ability to compositionally generalize, new architectures and techniques continue to be proposed to address this limitation. We investigate state-of-the-art techniques and architectures in order to assess their effectiveness in improving compositional generalization in semantic parsing tasks based on the SCAN and CFQ datasets. We show that masked language model (MLM) pre-training rivals SCAN-inspired architectures on primitive holdout splits. On a more complex compositional task, we show that pre-training leads to significant improvements in performance vs. comparable non-pre-trained models, whereas architectures proposed to encourage compositional generalization on SCAN or in the area of algorithm learning fail to lead to significant improvements. We establish a new state of the art on the CFQ compositional generalization benchmark using MLM pre-training together with an intermediate representation. (@furrer2020compositional)

Artur SD’Avila Garcez, Luis C Lamb, and Dov M Gabbay *Neural-symbolic cognitive reasoning* Springer Science & Business Media, 2008. **Abstract:** In real-world applications, the effective integration of learning and reasoning in a cognitive agent model is a difficult task. However, such integration may lead to a better understanding, use and construction of more realistic models. Unfortunately, existing models are either oversimplified or require much processing time, which is unsuitable for online learning and reasoning. Currently, controlled environments like training simulators do not effectively integrate learning and reasoning. In particular, higher-order concepts and cognitive abilities have many unknown temporal relations with the data, making it impossible to represent such relationships by hand. We introduce a novel cognitive agent model and architecture for online learning and reasoning that seeks to effectively represent, learn and reason in complex training environments. The agent architecture of the model combines neural learning with symbolic knowledge representation. It is capable of learning new hypotheses from observed data, and infer new beliefs based on these hypotheses. Furthermore, it deals with uncertainty and errors in the data using a Bayesian inference model. The validation of the model on real-time simulations and the results presented here indicate the promise of the approach when performing online learning and reasoning in real-world scenarios, with possible applications in a range of areas. (@garcez2008neural)

Marta Garnelo and Murray Shanahan Reconciling deep learning with symbolic artificial intelligence: representing objects and relations *Current Opinion in Behavioral Sciences*, 29: 17–23, 2019. ISSN 2352-1546. . URL <https://www.sciencedirect.com/science/article/pii/S2352154618301943>. Artificial Intelligence. **Abstract:** In the history of the quest for human-level artificial intelligence, a number of rival paradigms have vied for supremacy. Symbolic artificial intelligence was dominant for much of the 20th century, but currently a connectionist paradigm is in the ascendant, namely machine learning with deep neural networks. However, both paradigms have strengths and weaknesses, and a significant challenge for the field today is to effect a reconciliation. A central tenet of the symbolic paradigm is that intelligence results from the manipulation of abstract compositional representations whose elements stand for objects and relations. If this is correct, then a key objective for deep learning is to develop architectures capable of discovering objects and relations in raw data, and learning how to represent them in ways that are useful for downstream processing. This short review highlights recent progress in this direction. (@GARNELO201917)

Ross W Gayler Vector symbolic architectures answer jackendoff’s challenges for cognitive neuroscience In Peter Slezak, editor, *Proceedings of the ICCS/ASCS Joint International Conference on Cognitive Science (ICCS/ASCS 2003)*, pages 133–138, Sydney, NSW, AU, jul 2003. University of New South Wales. URL <http://arxiv.org/abs/cs/0412059>. **Abstract:** Jackendoff (2002) posed four challenges that linguistic combinatoriality and rules of language present to theories of brain function. The essence of these problems is the question of how to neurally instantiate the rapid construction and transformation of the compositional structures that are typically taken to be the domain of symbolic processing. He contended that typical connectionist approaches fail to meet these challenges and that the dialogue between linguistic theory and cognitive neuroscience will be relatively unproductive until the importance of these problems is widely recognised and the challenges answered by some technical innovation in connectionist modelling. This paper claims that a little-known family of connectionist models (Vector Symbolic Architectures) are able to meet Jackendoff’s challenges. (@gayler2003vsa_jackendoff)

Adele E Goldberg *Constructions at work: the nature of generalization in language* Oxford University Press, Oxford; New York, 2006. URL <http://public.ebookcentral.proquest.com/choice/publicfullrecord.aspx?p=3052348>. OCLC: 193697889. **Abstract:** Part One: Constructions 1. Overview 2. Surface Generalizations 3. Item Specific Knowledge and Generalizations Part Two: Learning Generalizations 4. How Generalizations are Learned 5. How Generalizations are Constrained 6. Why Generalizations are Learned Part Three: Explaining Generalizations 7. Island Constraints and Scope 8. Grammatical Categorization: Subject Auxiliary Inversion 9. Cross-linguistic Generalizations in Argument Realization 10. Variations on a Constructionist Theme 11. Conclusion References Index (@goldberg_constructions_2006)

Saul Gorn *Explicit Definitions and Linguistic Dominoes*, pages 77–115 University of Toronto Press, Toronto, 1967. ISBN 9781487592769. . URL <https://doi.org/10.3138/9781487592769-008>. **Abstract:** Explicit Definitions and Linguistic Dominoes was published in Systems and Computer Science on page 77. (@gorn+1967+77+115)

Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom Learning to transduce with unbounded memory *Advances in neural information processing systems*, 28, 2015. **Abstract:** Recently, strong results have been demonstrated by Deep Recurrent Neural Networks on natural language transduction problems. In this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation. These experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as Stacks, Queues, and DeQues. We show that these architectures exhibit superior generalisation performance to Deep RNNs and are often able to learn the underlying generating algorithms in our transduction experiments. (@grefenstette2015learning)

Thomas L. Griffiths Understanding human intelligence through human limitations *Trends in Cognitive Sciences*, 24: 873–883, 2020. URL <https://api.semanticscholar.org/CorpusID:221996148>. **Abstract:** Recent progress in artiﬁcial intelligence provides the opportunity to ask the question of what is unique about human intelligence, but with a new comparison class. I argue that we can understand human intelligence, and the ways in which it may di er from artiﬁcial intelligence, by considering the characteristics of the kind of computational problems that human minds have to solve. I claim that these problems acquire their structure from three fundamental limitations that apply to human beings: limited time, limited computation, and limited communication. From these limitations we can derive many of the properties we associate with human intelligence, such as rapid learning, the ability to break down problems into parts, and the capacity for cumulative cultural evolution. (@Griffiths2020UnderstandingHI)

Demi Guo, Yoon Kim, and Alexander M. Rush Sequence-Level Mixed Sample Data Augmentation *arXiv:2011.09039 \[cs\]*, November 2020. URL <http://arxiv.org/abs/2011.09039>. arXiv: 2011.09039. **Abstract:** Despite their empirical success, neural networks still have difficulty capturing compositional aspects of natural language. This work proposes a simple data augmentation approach to encourage compositional behavior in neural models for sequence-to-sequence problems. Our approach, SeqMix, creates new synthetic examples by softly combining input/output sequences from the training set. We connect this approach to existing techniques such as SwitchOut and word dropout, and show that these techniques are all approximating variants of a single objective. SeqMix consistently yields approximately 1.0 BLEU improvement on five different translation datasets over strong Transformer baselines. On tasks that require strong compositional generalization such as SCAN and semantic parsing, SeqMix also offers further improvements. (@guo_sequence-level_2020)

Armand Joulin and Tomas Mikolov Inferring algorithmic patterns with stack-augmented recurrent nets *Advances in neural information processing systems*, 28, 2015. **Abstract:** Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory. (@joulin2015inferring)

Pentti Kanerva Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors *Cognitive computation*, 1: 139–159, 2009. (@kanerva2009hyperdimensional)

Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc van Zee, and Olivier Bousquet Measuring compositional generalization: A comprehensive method on realistic data In *International Conference on Learning Representations*, 2020. URL <https://openreview.net/forum?id=SygcCnNKwr>. **Abstract:** State-of-the-art machine learning methods exhibit limited compositional generalization. At the same time, there is a lack of realistic benchmarks that comprehensively measure this ability, which makes it challenging to find and evaluate improvements. We introduce a novel method to systematically construct such benchmarks by maximizing compound divergence while guaranteeing a small atom divergence between train and test sets, and we quantitatively compare this method to other approaches for creating compositional generalization benchmarks. We present a large and realistic natural language question answering dataset that is constructed according to this method, and we use it to analyze the compositional generalization ability of three machine learning architectures. We find that they fail to generalize compositionally and that there is a surprisingly strong negative correlation between compound divergence and accuracy. We also demonstrate how our method can be used to create new compositionality benchmarks on top of the existing SCAN dataset, which confirms these findings. (@keysers2020measuring)

Najoung Kim and Tal Linzen : A compositional generalization challenge based on semantic interpretation In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages 9087–9105, Online, November 2020. Association for Computational Linguistics. . URL <https://aclanthology.org/2020.emnlp-main.731>. **Abstract:** Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96–99%), but generalization accuracy was substantially lower (16–35%) and showed high sensitivity to random seed (+-6–8%). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress. (@kim-linzen-2020-cogs)

Najoung Kim and Tal Linzen : A Compositional Generalization Challenge Based on Semantic Interpretation *arXiv:2010.05465 \[cs\]*, October 2020. URL <http://arxiv.org/abs/2010.05465>. arXiv: 2010.05465. **Abstract:** Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96–99%), but generalization accuracy was substantially lower (16–35%) and showed high sensitivity to random seed ($\\}pm$6–8%). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress. (@kim_cogs_2020)

Najoung Kim, Tal Linzen, and Paul Smolensky Uncontrolled lexical exposure leads to overestimation of compositional generalization in pretrained models *arXiv preprint arXiv:2212.10769*, 2022. **Abstract:** Human linguistic capacity is often characterized by compositionality and the generalization it enables – human learners can produce and comprehend novel complex expressions by composing known parts. Several benchmarks exploit distributional control across training and test to gauge compositional generalization, where certain lexical items only occur in limited contexts during training. While recent work using these benchmarks suggests that pretrained models achieve impressive generalization performance, we argue that exposure to pretraining data may break the aforementioned distributional control. Using the COGS benchmark of Kim and Linzen (2020), we test two modified evaluation setups that control for this issue: (1) substituting context-controlled lexical items with novel character sequences, and (2) substituting them with special tokens represented by novel embeddings. We find that both of these setups lead to lower generalization performance in T5 (Raffel et al., 2020), suggesting that previously reported results have been overestimated due to uncontrolled lexical exposure during pretraining. The performance degradation is more extreme with novel embeddings, and the degradation increases with the amount of pretraining data, highlighting an interesting case of inverse scaling. (@kim2022uncontrolled)

Yoon Kim, Chris Dyer, and Alexander M Rush Compound probabilistic context-free grammars for grammar induction *arXiv preprint arXiv:1906.10225*, 2019. **Abstract:** We study a formalization of the grammar induction problem that models sentences as being generated by a compound probabilistic context-free grammar. In contrast to traditional formulations which learn a single stochastic grammar, our grammar’s rule probabilities are modulated by a per-sentence continuous latent variable, which induces marginal dependencies beyond the traditional context-free assumptions. Inference in this grammar is performed by collapsed variational inference, in which an amortized variational posterior is placed on the continuous variable, and the latent trees are marginalized out with dynamic programming. Experiments on English and Chinese show the effectiveness of our approach compared to recent state-of-the-art methods when evaluated on unsupervised parsing. (@kim2019compound)

Dan Klein and Christopher D Manning A generative constituent-context model for improved grammar induction In *Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics*, pages 128–135, 2002. **Abstract:** We present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts. Parameter search with EM produces higher quality analyses than previously exhibited by unsupervised systems, giving the best published un-supervised parsing results on the ATIS corpus. Experiments on Penn treebank sentences of comparable length show an even higher F1 of 71% on non-trivial brackets. We compare distributionally induced and actual part-of-speech tags as input data, and examine extensions to the basic model. We discuss errors made by the system, compare the system to previous models, and discuss upper bounds, lower bounds, and stability for this task. (@klein2002generative)

Denis Kleyko, Dmitri A. Rachkovskij, Evgeny Osipov, and Abbas Rahimi A survey on hyperdimensional computing aka vector symbolic architectures, part i: Models and data transformations *ACM Comput. Surv.*, 55 (6), dec 2022. ISSN 0360-0300. . URL <https://doi.org/10.1145/3538531>. **Abstract:** This two-part comprehensive survey is devoted to a computing framework most commonly known under the names Hyperdimensional Computing and Vector Symbolic Architectures (HDC/VSA). Both names refer to a family of computational models that use high-dimensional distributed representations and rely on the algebraic properties of their key operations to incorporate the advantages of structured symbolic representations and vector distributed representations. Notable models in the HDC/VSA family are Tensor Product Representations, Holographic Reduced Representations, Multiply-Add-Permute, Binary Spatter Codes, and Sparse Binary Distributed Representations but there are other models too. HDC/VSA is a highly interdisciplinary field with connections to computer science, electrical engineering, artificial intelligence, mathematics, and cognitive science. This fact makes it challenging to create a thorough overview of the field. However, due to a surge of new researchers joining the field in recent years, the necessity for a comprehensive survey of the field has become extremely important. Therefore, amongst other aspects of the field, this Part I surveys important aspects such as: known computational models of HDC/VSA and transformations of various input data types to high-dimensional distributed representations. Part II of this survey is devoted to applications, cognitive computing and architectures, as well as directions for future work. The survey is written to be useful for both newcomers and practitioners. (@kleyko2022)

Brenden Lake and Marco Baroni Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks *35th International Conference on Machine Learning, ICML 2018*, 7: 4487–4499, 2018. arXiv: 1711.00350 ISBN: 9781510867963. **Abstract:** Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks’ notorious training data thirst. (@Lake_2018_GeneralizationSystematicityCompositional)

Brenden M Lake Compositional generalization through meta sequence-to-sequence learning *Advances in neural information processing systems*, 32, 2019. **Abstract:** People can learn a new concept and use it compositionally, understanding how to blicket twice after learning how to blicket. In contrast, powerful sequence-to-sequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show how memory-augmented neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply implicit rules to variables. (@lake2019compositional)

Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh Set transformer: A framework for attention-based permutation-invariant neural networks In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, *Proceedings of the 36th International Conference on Machine Learning*, volume 97 of *Proceedings of Machine Learning Research*, pages 3744–3753. PMLR, 09–15 Jun 2019. URL <https://proceedings.mlr.press/v97/lee19d.html>. **Abstract:** Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data. (@pmlr-v97-lee19d)

Bingzhi Li, Lucia Donatelli, Alexander Koller, Tal Linzen, Yuekun Yao, and Najoung Kim : A Structural Generalization Benchmark for Semantic Parsing October 2023. URL <http://arxiv.org/abs/2310.15040>. arXiv:2310.15040 \[cs\]. **Abstract:** The goal of compositional generalization benchmarks is to evaluate how well models generalize to new complex linguistic expressions. Existing benchmarks often focus on lexical generalization, the interpretation of novel lexical items in syntactic structures familiar from training; structural generalization tasks, where a model needs to interpret syntactic structures that are themselves unfamiliar from training, are often underrepresented, resulting in overly optimistic perceptions of how well models can generalize. We introduce SLOG, a semantic parsing dataset that extends COGS (Kim and Linzen, 2020) with 17 structural generalization cases. In our experiments, the generalization accuracy of Transformer models, including pretrained ones, only reaches 40.6%, while a structure-aware parser only achieves 70.8%. These results are far from the near-perfect accuracy existing models achieve on COGS, demonstrating the role of SLOG in foregrounding the large discrepancy between models’ lexical and structural generalization capacities. (@li_slog_2023)

Yuxuan Li and James McClelland Representations and computations in transformers that support generalization on structured tasks *Transactions on Machine Learning Research*, 2023. ISSN 2835-8856. URL <https://openreview.net/forum?id=oFC2LAqS6Z>. **Abstract:** Transformers have shown remarkable success in natural language processing and computer vision, serving as the foundation of large language and multimodal models. These networks can capture nuanced context sensitivity across high-dimensional language tokens or image pixels, but it remains unclear how highly structured behavior and systematic generalization can arise in these systems. Here, we explore the solution process a causal transformer discovers as it learns to solve a set of algorithmic tasks involving copying, sorting, and hierarchical compositions of these operations. We search for the minimal layer and head configuration sufficient to solve these tasks and unpack the roles of the attention heads, as well as how token representations are reweighted across layers to complement these roles. Our results provide new insights into how attention layers in transformers support structured computation within and across tasks: 1) Replacing fixed position labels with labels sampled from a larger set enables strong length generalization and faster learning. The learnable embeddings of these labels develop different representations, capturing sequence order if necessary, depending on task demand. 2) Two-layer transformers can learn reliable solutions to the multi-level problems we explore. The first layer tends to transform the input representation to allow the second layer to share computation across repeated components within a task or across related tasks. 3) We introduce an analysis pipeline that quantifies how the representation space in a given layer prioritizes different aspects of each item. We show that these representations prioritize information needed to guide attention relative to information that only requires downstream readout. (@li2023representations)

Matthias Lindemann, Alexander Koller, and Ivan Titov Compositional generalization without trees using multiset tagging and latent permutations *arXiv preprint arXiv:2305.16954*, 2023. **Abstract:** Seq2seq models have been shown to struggle with compositional generalization in semantic parsing, i.e. generalizing to unseen compositions of phenomena that the model handles correctly in isolation. We phrase semantic parsing as a two-step process: we first tag each input token with a multiset of output tokens. Then we arrange the tokens into an output sequence using a new way of parameterizing and predicting permutations. We formulate predicting a permutation as solving a regularized linear program and we backpropagate through the solver. In contrast to prior work, our approach does not place a priori restrictions on possible permutations, making it very expressive. Our model outperforms pretrained seq2seq models and prior work on realistic semantic parsing tasks that require generalization to longer examples. We also outperform non-tree-based models on structural generalization on the COGS benchmark. For the first time, we show that a model without an inductive bias provided by trees achieves high accuracy on generalization to deeper recursion. (@lindemann2023compositional)

Adam Lopez Statistical machine translation *ACM Computing Surveys (CSUR)*, 40 (3): 1–49, 2008. **Abstract:** Kyunghyun Cho, Bart van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2014. (@lopez2008statistical)

Jaron Maene and Luc De Raedt Soft-unification in deep probabilistic logic In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, *Advances in Neural Information Processing Systems*, volume 36, pages 60804–60820. Curran Associates, Inc., 2023. URL <https://proceedings.neurips.cc/paper_files/paper/2023/file/bf215fa7fe70a38c5e967e59c44a99d0-Paper-Conference.pdf>. **Abstract:** In this paper, we introduce Neural Probabilistic Soft Logic (NeuPSL), a novel neuro-symbolic (NeSy) framework that unites state-of-the-art symbolic reasoning with the low-level perception of deep neural networks. To model the boundary between neural and symbolic representations, we propose a family of energy-based models, NeSy Energy-Based Models, and show that they are general enough to include NeuPSL and many other NeSy approaches. Using this framework, we show how to seamlessly integrate neural and symbolic parameter learning and inference in NeuPSL. Through an extensive empirical evaluation, we demonstrate the benefits of using NeSy methods, achieving upwards of 30% improvement over independent neural network models. On a well-established NeSy task, MNIST-Addition, NeuPSL demonstrates its joint reasoning capabilities by outperforming existing NeSy approaches by up to 10% in low-data settings. Furthermore, NeuPSL achieves a 5% boost in performance over state-of-the-art NeSy methods in a canonical citation network task with up to a 40 times speed up. (@NEURIPS2023_bf215fa7)

Gary F. Marcus *The Algebraic Mind: Integrating Connectionism and Cognitive Science* MIT Press, 2001. **Abstract:** In The Algebraic Mind, Gary Marcus attempts to integrate two theories about how the mind works, one that says that the mind is a computer-like manipulator of symbols, and another that says that the mind is a large network of neurons working together in parallel. Resisting the conventional wisdom that says that if the mind is a large neural network it cannot simultaneously be a manipulator of symbols, Marcus outlines a variety of ways in which neural systems could be organized so as to manipulate symbols, and he shows why such systems are more likely to provide an adequate substrate for language and cognition than neural systems that are inconsistent with the manipulation of symbols. Concluding with a discussion of how a neurally realized system of symbol-manipulation could have evolved and how such a system could unfold developmentally within the womb, Marcus helps to set the future agenda of cognitive neuroscience. (@Marcus2001-MARTAM-10)

John McCarthy Recursive functions of symbolic expressions and their computation by machine, part i *Communications of the ACM*, 3 (4): 184–195, 1960. **Abstract:** article Free Access Share on Recursive functions of symbolic expressions and their computation by machine, Part I Author: John McCarthy Massachusetts Institute of Technology, Cambridge Massachusetts Institute of Technology, CambridgeView Profile Authors Info & Claims Communications of the ACMVolume 3Issue 4April 1960 pp 184–195https://doi.org/10.1145/367177.367199Published:01 April 1960Publication History 893citation5,975DownloadsMetricsTotal Citations893Total Downloads5,975Last 12 Months607Last 6 weeks85 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF (@mccarthy1960recursive)

R Thomas McCoy, Tal Linzen, Ewan Dunbar, and Paul Smolensky Rnns implicitly implement tensor product representations *arXiv preprint arXiv:1812.08718*, 2018. **Abstract:** Recurrent neural networks (RNNs) can learn continuous vector representations of symbolic structures such as sequences and sentences; these representations often exhibit linear regularities (analogies). Such regularities motivate our hypothesis that RNNs that show such regularities implicitly compile symbolic structures into tensor product representations (TPRs; Smolensky, 1990), which additively combine tensor products of vectors representing roles (e.g., sequence positions) and vectors representing fillers (e.g., particular words). To test this hypothesis, we introduce Tensor Product Decomposition Networks (TPDNs), which use TPRs to approximate existing vector representations. We demonstrate using synthetic data that TPDNs can successfully approximate linear and tree-based RNN autoencoder representations, suggesting that these representations exhibit interpretable compositional structure; we explore the settings that lead RNNs to induce such structure-sensitive representations. By contrast, further TPDN experiments show that the representations of four models trained to encode naturally-occurring sentences can be largely approximated with a bag of words, with only marginal improvements from more sophisticated structures. We conclude that TPDNs provide a powerful method for interpreting vector representations, and that standard RNNs can induce compositional sequence representations that are remarkably well approximated by TPRs; at the same time, existing training tasks for sentence representation learning may not be sufficient for inducing robust structural representations. (@mccoy2018rnns)

Stephen Muggleton Inductive logic programming *New Generation Computing*, 8 (4): 295–318, February 1991. ISSN 1882-7055. . URL <https://doi.org/10.1007/BF03037089>. **Abstract:** Interpretation Integrating Type and Mode Inferencing, in: Proceedings of the 5th International Conference and Symposium on Logic Programming, 1988, pp. 669-683. Bry, F., Intensional Updates: Abduction via Deduction, in: D. Warren and P. Szeredi (eds.), Proceedings of the 7th International Conference on Logic Programming, MIT press, 1990, pp. 561-578. Buntine, W., Induction of Horn-Clauses: Methods and the Plausible Generalization Al- gorithm, International Journal of Man-Machine Studies 26:499-520 (1987). Buntine, W., Generalised Subsumption and Its Applications to Induction and Redundancy, Arttficial Intelligence 36(2): 149-176 (1988). Carnap, R., The Continuum of Inductive Methods, University of Chicago, Chicago, IL, 1952. Cohen, W., Compiling Knowledge into an Explicit Bias, in: Proceedings of the 9th International Conference on Machine Learning, Morgan Kaufmann, 1992. Cohen, W., Grammatically Biased Learning: Learning Logic Programs Using an Explicit Antecedent Description Language, Artificial Intelligence (1994), to appear. Cohen, W., Learnability of Restricted Logic Programs, in: S. Muggleton (ed.), Pro- ceedings of the 3rd International Workshop on Inductive Logic Programming, 1993, pp. 41-72. Cohen, W., PAC-Learning a Restricted Class of Recursive Logic Programs, in: S. Muggle- ton (ed.), Proceedings of the 3rd International Workshop on Inductive Logic Programming, 1993, pp. 73-86. Conklin, D., and Witten, I., Complexity-Based Induction, Technical Report, Department of Computing and Information Science, Queen’s University, Kingston, Ontario, Canada, 1992. Datta, P., and Kibler, Concept-Sharing: A Means to Improve Multi-Concept Learning, in: Proceedings of the IOth International Conference on Machine Learning, Morgan- 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. S.MIJGGLETONANDL.DERAEDT Kaufmann, San Mateo, CA, 1993. Decker, H., Drawing Updates from Derivations, in: S. Abiteboul and P. C. Kanellakis (eds.), Proceedings of the 2nd International Conference on Database Theory, vol. 470 of Lecture Notes in Computer Science, Springer-Verlag, 1990. Deville, Y.,‘and Lau, K., Logic Program Synthesis, Journal of Logic Programming (1993), submitted to the Special Issue. Dolsak, B., and Muggleton, S., The Application of Inductive Logic Programming to Finite Element Mesh Design, in: S. Muggleton (ed.), Inductive Logic Programming. Academic Press, London, 1992. Dierosk (@Muggleton_1991_InductiveLogicProgramming)

Shikhar Murty, Pratyusha Sharma, Jacob Andreas, and Christopher D Manning Characterizing intrinsic compositionality in transformers with tree projections *arXiv preprint arXiv:2211.01288*, 2022. **Abstract:** When trained on language data, do transformers learn some arbitrary computation that utilizes the full capacity of the architecture or do they learn a simpler, tree-like computation, hypothesized to underlie compositional meaning systems like human languages? There is an apparent tension between compositional accounts of human language understanding, which are based on a restricted bottom-up computational process, and the enormous success of neural models like transformers, which can route information arbitrarily between different parts of their input. One possibility is that these models, while extremely flexible in principle, in practice learn to interpret language hierarchically, ultimately building sentence representations close to those predictable by a bottom-up, tree-structured model. To evaluate this possibility, we describe an unsupervised and parameter-free method to \\}emph{functionally project} the behavior of any transformer into the space of tree-structured networks. Given an input sentence, we produce a binary tree that approximates the transformer’s representation-building process and a score that captures how "tree-like" the transformer’s behavior is on the input. While calculation of this score does not require training any additional models, it provably upper-bounds the fit between a transformer and any tree-structured approximation. Using this method, we show that transformers for three different tasks become more tree-like over the course of training, in some cases unsupervisedly recovering the same trees as supervised parsers. These trees, in turn, are predictive of model behavior, with more tree-like models generalizing better on tests of compositional generalization. (@murty2022characterizing)

Barbara Partee et al Lexical semantics and compositionality *An invitation to cognitive science: Language*, 1: 311–360, 1995. **Abstract:** The focus of the paper is the interaction of meaning and context with different kinds of adjectives. Adjective meanings are shown to be more constrained than was appreciated in earlier work. Facts about “NP-splitting” in Polish and Russian cast serious doubt on the standard hierarchy of adjectives, and the data become much more orderly if privative adjectives are reanalyzed as subsective adjectives. This revised account requires the possibility of coerced expansion of the denotation of the noun to which an adjective is applied. Compositionality can be seen as one of the driving forces in such context-sensitive meaning shifts. (@partee1995lexical)

Arkil Patel, Satwik Bhattamishra, Phil Blunsom, and Navin Goyal Revisiting the compositional generalization abilities of neural sequence models *arXiv preprint arXiv:2203.07402*, 2022. **Abstract:** Compositional generalization is a fundamental trait in humans, allowing us to effortlessly combine known phrases to form novel sentences. Recent works have claimed that standard seq-to-seq models severely lack the ability to compositionally generalize. In this paper, we focus on one-shot primitive generalization as introduced by the popular SCAN benchmark. We demonstrate that modifying the training distribution in simple and intuitive ways enables standard seq-to-seq models to achieve near-perfect generalization performance, thereby showing that their compositional generalization abilities were previously underestimated. We perform detailed empirical analysis of this phenomenon. Our results indicate that the generalization performance of models is highly sensitive to the characteristics of the training data which should be carefully considered while designing such benchmarks in future. (@patel2022revisiting)

Steven Pinker *The language instinct: How the mind creates language* Penguin uK, 2003. **Abstract:** In this classic, the world’s expert on language and mind lucidly explains everything you always wanted to know about language: how it works, how children learn it, how it changes, how the brain computes it, and how it evolved. With deft use of examples of humor and wordplay, Steven Pinker weaves our vast knowledge of language into a compelling story: language is a human instinct, wired into our brains by evolution. The Language Instinct received the William James Book Prize from the American Psychological Association and the Public Interest Award from the Linguistics Society of America. This edition includes an update on advances in the science of language since The Language Instinct was first published. (@pinker2003language)

Tony A. Plate *Holographic Reduced Representation: Distributed Representation for Cognitive Structures* CSLI Publications, USA, 2003. ISBN 1575864290. **Abstract:** While neuroscientists garner success in identifying brain regions and in analyzing individual neurons, ground is still being broken at the intermediate scale of understanding how neurons combine to encode information. This book proposes a method of representing information in a computer that would be suited for modelling the brain’s methods of processing information. Holographic reduced representations (HRRs) are introduced here to model how the brain distributes each piece of information among thousands of neurons. It has been previously thought that the grammatical structure of a language cannot be encoded practically in a distributed representation, but HRRs can overcome the problems of earlier proposals. Thus this work has implications for psychology, neuroscience, linguistics, computer science and engineering. (@plate)

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu Exploring the limits of transfer learning with a unified text-to-text transformer *J. Mach. Learn. Res.*, 21 (1), jan 2020. ISSN 1532-4435. **Abstract:** Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code. (@t5)

Tim Rocktäschel and Sebastian Riedel Learning knowledge base inference with neural theorem provers In Jay Pujara, Tim Rocktaschel, Danqi Chen, and Sameer Singh, editors, *Proceedings of the 5th Workshop on Automated Knowledge Base Construction*, pages 45–50, San Diego, CA, June 2016. Association for Computational Linguistics. . URL <https://aclanthology.org/W16-1309>. **Abstract:** In this paper we present a proof-of-concept implementation of Neural Theorem Provers (NTPs), end-to-end differentiable counterparts of discrete theorem provers that perform first-order inference on vector representations of symbols using function-free, possibly parameterized, rules.As such, NTPs follow a long tradition of neural-symbolic approaches to automated knowledge base inference, but differ in that they are differentiable with respect to representations of symbols in a knowledge base and can thus learn representations of predicates, constants, as well as rules of predefined structure.Furthermore, they still allow us to incorporate domainknowledge provided as rules.The NTP presented here is realized via a differentiable version of the backward chaining algorithm.It operates on substitution representations and is able to learn complex logical dependencies from training facts of small knowledge bases. (@rocktaschel-riedel-2016-learning)

Anian Ruoss, Grégoire Delétang, Tim Genewein, Jordi Grau-Moya, Róbert Csordás, Mehdi Bennani, Shane Legg, and Joel Veness Randomized positional encodings boost length generalization of transformers In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)*, pages 1889–1903, Toronto, Canada, July 2023. Association for Computational Linguistics. . URL <https://aclanthology.org/2023.acl-short.161>. **Abstract:** Anian Ruoss, Grégoire Delétang, Tim Genewein, Jordi Grau-Moya, Róbert Csordás, Mehdi Bennani, Shane Legg, Joel Veness. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2023. (@ruoss-etal-2023-randomized)

Jake Russin, Jason Jo, Randall C O’Reilly, and Yoshua Bengio Compositional generalization in a deep seq2seq model by separating syntax and semantics *arXiv preprint arXiv:1904.09708*, 2019. **Abstract:** Standard methods in deep learning for natural language processing fail to capture the compositional structure of human language that allows for systematic generalization outside of the training distribution. However, human learners readily generalize in this way, e.g. by applying known grammatical rules to novel words. Inspired by work in neuroscience suggesting separate brain systems for syntactic and semantic processing, we implement a modification to standard approaches in neural machine translation, imposing an analogous separation. The novel model, which we call Syntactic Attention, substantially outperforms standard methods in deep learning on the SCAN dataset, a compositional generalization task, without any hand-engineered features or additional supervision. Our work suggests that separating syntactic from semantic learning may be a useful heuristic for capturing compositional structure. (@russin2019compositional)

Itiroo Sakai Syntax in universal translation In *Proceedings of the International Conference on Machine Translation and Applied Language Analysis*, 1961. **Abstract:** This comprehensive review explores the pivotal role of syntax and semantics in rule-based translation, emphasizing their significance in ensuring accurate and contextually appropriate translations across different languages. The study is not focused on any specific source or target language pair but provides a general exploration of rule-based approaches that can be applied to various linguistic combinations. Despite advancements in neural machine translation techniques, rule-based methods remain crucial, particularly in domains where linguistic and cultural nuances are paramount. By synthesizing existing literature and practical insights, the review highlights the universal applicability of syntax-driven and semantics-driven techniques in translation, demonstrating how these methods contribute to high-quality translations across diverse language pairs. The implications extend to future research directions, including the development of hybrid translation approaches and the integration of contextual considerations into rule-based translation models, ultimately facilitating cross-linguistic communication and understanding. (@sakai1961syntax)

Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova Compositional generalization and natural language variation: Can a semantic parsing approach handle both? In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, *Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)*, pages 922–938, Online, August 2021. Association for Computational Linguistics. . URL <https://aclanthology.org/2021.acl-long.75>. **Abstract:** Peter Shaw, Ming-Wei Chang, Panupong Pasupat, Kristina Toutanova. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021. (@shaw-etal-2021-compositional)

Hikaru Shindo, Masaaki Nishino, and Akihiro Yamamoto Differentiable inductive logic programming for structured examples *Proceedings of the AAAI Conference on Artificial Intelligence*, 35 (6): 5034–5041, May 2021. . URL <https://ojs.aaai.org/index.php/AAAI/article/view/16637>. **Abstract:** The differentiable implementation of logic yields a seamless combination of symbolic reasoning and deep neural networks. Recent research, which has developed a differentiable framework to learn logic programs from examples, can even acquire reasonable solutions from noisy datasets. However, this framework severely limits expressions for solutions, e.g., no function symbols are allowed, and the shapes of clauses are fixed. As a result, the framework cannot deal with structured examples. Therefore we propose a new framework to learn logic programs from noisy and structured examples, including the following contributions. First, we propose an adaptive clause search method by looking through structured space, which is defined by the generality of the clauses, to yield an efficient search space for differentiable solvers. Second, we propose for ground atoms an enumeration algorithm, which determines a necessary and sufficient set of ground atoms to perform differentiable inference functions. Finally, we propose a new method to compose logic programs softly, enabling the system to deal with complex programs consisting of several clauses. Our experiments show that our new framework can learn logic programs from noisy and structured examples, such as sequences or trees. Our framework can be scaled to deal with complex programs that consist of several clauses with function symbols. (@Shindo_Nishino_Yamamoto_2021)

Vighnesh Shiv and Chris Quirk Novel positional encodings to enable tree-based transformers In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, *Advances in Neural Information Processing Systems*, volume 32. Curran Associates, Inc., 2019. URL <https://proceedings.neurips.cc/paper_files/paper/2019/file/6e0917469214d8fbd8c517dcdc6b8dcf-Paper.pdf>. **Abstract:** Neural models optimized for tree-based problems are of great value in tasks like SQL query extraction and program synthesis. On sequence-structured data, transformers have been shown to learn relationships across arbitrary pairs of positions more reliably than recurrent models. Motivated by this property, we propose a method to extend transformers to tree-structured data, enabling sequence-to-tree, tree-to-sequence, and tree-to-tree mappings. Our approach abstracts the transformer’s sinusoidal positional encodings, allowing us to instead use a novel positional encoding scheme to represent node positions within trees. We evaluated our model in tree-to-tree program translation and sequence-to-tree semantic parsing settings, achieving superior performance over both sequence-to-sequence transformers and state-of-the-art tree-based LSTMs on several datasets. In particular, our results include a 22% absolute increase in accuracy on a JavaScript to CoffeeScript translation dataset. (@NEURIPS2019_6e091746)

Vighnesh Shiv and Chris Quirk Novel positional encodings to enable tree-based transformers *Advances in neural information processing systems*, 32, 2019. **Abstract:** Neural models optimized for tree-based problems are of great value in tasks like SQL query extraction and program synthesis. On sequence-structured data, transformers have been shown to learn relationships across arbitrary pairs of positions more reliably than recurrent models. Motivated by this property, we propose a method to extend transformers to tree-structured data, enabling sequence-to-tree, tree-to-sequence, and tree-to-tree mappings. Our approach abstracts the transformer’s sinusoidal positional encodings, allowing us to instead use a novel positional encoding scheme to represent node positions within trees. We evaluated our model in tree-to-tree program translation and sequence-to-tree semantic parsing settings, achieving superior performance over both sequence-to-sequence transformers and state-of-the-art tree-based LSTMs on several datasets. In particular, our results include a 22% absolute increase in accuracy on a JavaScript to CoffeeScript translation dataset. (@shiv2019novel)

David Smith and Jason Eisner Quasi-synchronous grammars: Alignment by soft projection of syntactic dependencies In Philipp Koehn and Christof Monz, editors, *Proceedings on the Workshop on Statistical Machine Translation*, pages 23–30, New York City, June 2006. Association for Computational Linguistics. URL <https://aclanthology.org/W06-3104>. **Abstract:** Many syntactic models in machine translation are channels that transform one tree into another, or synchronous grammars that generate trees in parallel. We present a new model of the translation process: quasi-synchronous grammar (QG). Given a source-language parse tree T1, a QG defines a monolingual grammar that generates translations of T1. The trees T2 allowed by this monolingual grammar are inspired by pieces of substructure in T1 and aligned to T1 at those points. We describe experiments learning quasi-synchronous context-free grammars from bitext. As with other monolingual language models, we evaluate the cross-entropy of QGs on unseen text and show that a better fit to bilingual data is achieved by allowing greater syntactic divergence. When evaluated on a word alignment task, QG matches standard baselines. (@smith-eisner-2006-quasi)

Paul Smolensky On the proper treatment of connectionism *Behavioral and Brain Sciences*, 11 (1): 1–23, 1988. . **Abstract:** Abstract A set of hypotheses is formulated for a connectionist approach to cognitive modeling. These hypotheses are shown to be incompatible with the hypotheses underlying traditional cognitive models. The connectionist models considered are massively parallel numerical computational systems that are a kind of continuous dynamical system. The numerical variables in the system correspond semantically to fine-grained features below the level of the concepts consciously used to describe the task domain. The level of analysis is intermediate between those of symbolic cognitive models and neural models. The explanations of behavior provided are like those traditional in the physical sciences, unlike the explanations provided by symbolic models. Higher-level analyses of these connectionist models reveal subtle relations to symbolic models. Parallel connectionist memory and linguistic processes are hypothesized to give rise to processes that are describable at a higher level as sequential rule application. At the lower level, computation has the character of massively parallel satisfaction of soft numerical constraints; at the higher level, this can lead to competence characterizable by hard rules. Performance will typically deviate from this competence since behavior is achieved not by interpreting hard rules but by satisfying soft constraints. The result is a picture in which traditional and connectionist theoretical constructs collaborate intimately to provide an understanding of cognition. (@Smolensky_1988)

Paul Smolensky Tensor product variable binding and the representation of symbolic structures in connectionist systems *Artif. Intell.*, 46: 159–216, 1990. **Abstract:** This chapter contains sections titled: 1. Introduction, 2. Connectionist Representation and Tensor Product Binding: Definition and Examples, 3. Tensor Product Representation: Properties, 4. Conclusion (@Smolensky1990TensorPV)

Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng Semantic compositionality through recursive matrix-vector spaces In Jun’ichi Tsujii, James Henderson, and Marius Paşca, editors, *Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning*, pages 1201–1211, Jeju Island, Korea, July 2012. Association for Computational Linguistics. URL <https://aclanthology.org/D12-1110>. **Abstract:** Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them. (@socher-etal-2012-semantic)

Paul Soulos, Tom McCoy, Tal Linzen, and Paul Smolensky Discovering the compositional structure of vector representations with role learning networks *arXiv preprint arXiv:1910.09113*, 2019. **Abstract:** How can neural networks perform so well on compositional tasks even though they lack explicit compositional representations? We use a novel analysis technique called ROLE to show that recurrent neural networks perform well on such tasks by converging to solutions which implicitly represent symbolic structure. This method uncovers a symbolic structure which, when properly embedded in vector space, closely approximates the encodings of a standard seq2seq network trained to perform the compositional SCAN task. We verify the causal importance of the discovered symbolic structure by showing that, when we systematically manipulate hidden embeddings based on this symbolic structure, the model’s output is changed in the way predicted by our analysis. (@soulos2019discovering)

Paul Soulos, Edward Hu, Kate McCurdy, Yunmo Chen, Roland Fernandez, Paul Smolensky, and Jianfeng Gao Differentiable Tree Operations Promote Compositional Generalization June 2023. URL <http://arxiv.org/abs/2306.00751>. arXiv:2306.00751 \[cs\]. **Abstract:** In the context of structure-to-structure transformation tasks, learning sequences of discrete symbolic operations poses significant challenges due to their non-differentiability. To facilitate the learning of these symbolic sequences, we introduce a differentiable tree interpreter that compiles high-level symbolic tree operations into subsymbolic matrix operations on tensors. We present a novel Differentiable Tree Machine (DTM) architecture that integrates our interpreter with an external memory and an agent that learns to sequentially select tree operations to execute the target transformation in an end-to-end manner. With respect to out-of-distribution compositional generalization on synthetic semantic parsing and language generation tasks, DTM achieves 100% while existing baselines such as Transformer, Tree Transformer, LSTM, and Tree2Tree LSTM achieve less than 30%. DTM remains highly interpretable in addition to its perfect performance. (@Soulos_2023_DifferentiableTreeOperations)

Mark Steedman Combinatory grammars and parasitic gaps *Natural Language & Linguistic Theory*, 5 (3): 403–439, 1987. (@steedman1987combinatory)

Kaiser Sun, Adina Williams, and Dieuwke Hupkes A Replication Study of Compositional Generalization Works on Semantic Parsing August 2023. URL <https://openreview.net/forum?id=MF9uv95psps>. **Abstract:** Reproducibility Summary Scope of Reproducibility — We examine the reproducibility of compositional generalization results from the task of semantic parsing. We aim to reproduce results from \[1\], \[2\], and \[3\] and seek to verify the claims that 1. A model shouldn’t be expected to perform well on non-synthetic datasets just because it performs well on SCAN \[1\], 2. The approaches from \[1\] and \[2\] meet or exceed baseline performance on compositional generalization tests, and 3. NQG-T5 \[1\] outperforms baselines on both synthetic and natural data. 4. NQG \[1\] performs well on the instances that it is able to generate a prediction, but it faces the barrier of not being able to generate predictions for all instances. Methodology — We reuse the authors’ code along with additional code to run extra experiments, and we re-implement scripts whose support is deprecated. Eight 32GB GPUs were used for experiments, with a detailed description in Section 3.3. Results — Claim 1 is verified: the model with the highest performance on SCAN does not maintain its high performance on other datasets (Section 4.1). Claims 2 and 3 are verified, with a comparison of performance between NQG-T5 and the selected baseline models in \[1\] and \[2\]. Claim 4 is also verified by computing the coverage and precision of NQG in Section 4.4. Overall, accuracy for most experiments reaches within 2% of that reported in the original paper, with a deviation that our T5 achieves higher performance on some splits and slightly lower performance on one split than reported previously. What was easy — All papers provide clearly‐written code and informative documentation, as well as lists of hyperparameters that are used for experiments. The papers also describe their approaches clearly, making the experimental workflow easy to follow. What was difficult — The exact match evaluation metric is formulated somewhat differently across all three papers, leading to a non‐negligible value difference, as discussed in Section 5.2. We also had to re‐implement some training scripts because an original dependency is no longer supported. Moreover, some experiments are computationally expensive: \[1\] used TPUs for experiments, while our replication with GPUs take several days to train a single T5 model. Communication with original authors — The authors of all three papers provided us with useful instruction to work with their methods and constructive feedback on the draft. (@Sun_2023_ReplicationStudyCompositional)

Kai Sheng Tai, Richard Socher, and Christopher D Manning Improved semantic representations from tree-structured long short-term memory networks *arXiv preprint arXiv:1503.00075*, 2015. **Abstract:** Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank). (@tai2015improved)

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin Attention is all you need *Advances in neural information processing systems*, 30, 2017. **Abstract:** The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. (@vaswani2017attention)

Yau-Shian Wang, Hung-Yi Lee, and Yun-Nung Chen Tree transformer: Integrating tree structures into self-attention *arXiv preprint arXiv:1909.06639*, 2019. **Abstract:** Pre-training Transformer from large-scale raw texts and fine-tuning on the desired task have achieved state-of-the-art results on diverse NLP tasks. However, it is unclear what the learned attention captures. The attention computed by attention heads seems not to match human intuitions about hierarchical structures. This paper proposes Tree Transformer, which adds an extra constraint to attention heads of the bidirectional Transformer encoder in order to encourage the attention heads to follow tree structures. The tree structures can be automatically induced from raw texts by our proposed "Constituent Attention" module, which is simply implemented by self-attention between two adjacent words. With the same training procedure identical to BERT, the experiments demonstrate the effectiveness of Tree Transformer in terms of inducing tree structures, better language modeling, and further learning more explainable attention scores. (@wang2019tree)

Thomas Winters, Giuseppe Marra, Robin Manhaeve, and Luc De Raedt Deepstochlog: Neural stochastic logic programming *Proceedings of the AAAI Conference on Artificial Intelligence*, 36 (9): 10090–10100, Jun. 2022. . URL <https://ojs.aaai.org/index.php/AAAI/article/view/21248>. **Abstract:** Recent advances in neural-symbolic learning, such as DeepProbLog, extend probabilistic logic programs with neural predicates. Like graphical models, these probabilistic logic programs define a probability distribution over possible worlds, for which inference is computationally hard. We propose DeepStochLog, an alternative neural-symbolic framework based on stochastic definite clause grammars, a kind of stochastic logic program. More specifically, we introduce neural grammar rules into stochastic definite clause grammars to create a framework that can be trained end-to-end. We show that inference and learning in neural stochastic logic programming scale much better than for neural probabilistic logic programs. Furthermore, the experimental evaluation shows that DeepStochLog achieves state-of-the-art results on challenging neural-symbolic learning tasks. (@Winters_Marra_Manhaeve_Raedt_2022)

Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu On layer normalization in the transformer architecture In *International Conference on Machine Learning*, pages 10524–10533. PMLR, 2020. **Abstract:** The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications. (@xiong2020layer)

Zhun Yang, Adam Ishay, and Joohyung Lee Neurasp: Embracing neural networks into answer set programming In Christian Bessiere, editor, *Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20*, pages 1755–1762. International Joint Conferences on Artificial Intelligence Organization, 7 2020. . URL <https://doi.org/10.24963/ijcai.2020/243>. Main track. **Abstract:** We present NeurASP, a simple extension of answer set programs by embracing neural networks. By treating the neural network output as the probability distribution over atomic facts in answer set programs, NeurASP provides a simple and effective way to integrate sub-symbolic and symbolic computation. We demonstrate how NeurASP can make use of a pre-trained neural network in symbolic computation and how it can improve the neural network’s perception result by applying symbolic reasoning in answer set programming. Also, NeurASP can make use of ASP rules to train a neural network better so that a neural network not only learns from implicit correlations from the data but also from the explicit complex semantic constraints expressed by the rules. (@ijcai2020p243)

Dani Yogatama, Yishu Miao, Gabor Melis, Wang Ling, Adhiguna Kuncoro, Chris Dyer, and Phil Blunsom Memory architectures in recurrent neural network language models In *International Conference on Learning Representations*, 2018. URL <https://openreview.net/forum?id=SkFqf0lAZ>. **Abstract:** We compare and analyze sequential, random access, and stack memory architectures for recurrent neural network language models. Our experiments on the Penn Treebank and Wikitext-2 datasets show that stack-based memory architectures consistently achieve the best performance in terms of held out perplexity. We also propose a generalization to existing continuous stack models (Joulin & Mikolov,2015; Grefenstette et al., 2015) to allow a variable number of pop operations more naturally that further improves performance. We further evaluate these language models in terms of their ability to capture non-local syntactic dependencies on a subject-verb agreement dataset (Linzen et al., 2016) and establish new state of the art results using memory augmented language models. Our results demonstrate the value of stack-structured memory for explaining the distribution of words in natural language, in line with linguistic theories claiming a context-free backbone for natural language. (@yogatama2018memory)

John M Zelle and Raymond J Mooney Learning to parse database queries using inductive logic programming In *Proceedings of the national conference on artificial intelligence*, pages 1050–1055, 1996. **Abstract:** This paper presents recent work using the CHILL parser acquisition system to automate the construction of a natural-language interface for database queries. CHILL treats parser acquisition as the learning of search-control rules within a logic program representing a shift-reduce parser and uses techniques from Inductive Logic Programming to learn relational control knowledge. Starting with a general framework for constructing a suitable logical form, CHILL is able to train on a corpus comprising sentences paired with database queries and induce parsers that map subsequent sentences directly into executable queries. Experimental results with a complete database-query application for U.S. geography show that CHILL is able to learn parsers that outperform a preexisting, hand-crafted counterpart. These results demonstrate the ability of a corpus-based system to produce more than purely syntactic representations. They also provide direct evidence of the utility of an empirical approach at the level of a complete natural language application. (@zelle1996learning)

</div>
