flaw_id,flaw_description,num_modifications,llm_generated_modifications
missing_comparison_previous_bounds,"The manuscript does not yet provide a direct, quantitative comparison between the new lower-bound (especially Corollary 2 for i.i.d. uniform data) and the corresponding bounds obtainable from earlier work (e.g.\, Nguyen et al.\, 2021). Reviewers stressed that such a comparison is essential to justify the claimed advantage of the new \(\delta\)-separation assumption and to let readers assess when the result is actually better. The authors acknowledged this gap and committed to include the requested explicit comparison and accompanying discussion in the camera-ready version.",1,"[
  {
    ""target_heading"": ""#### Main contributions."",
    ""new_content"": ""#### Main contributions.\n\nOur findings are summarised below.\n\n- **Shallow networks.** Theorem [thm:shallow-main] shows that for a ReLU network with input dimension $d_{0}$ and hidden width $d_{1}$, if $d_{1}=\\tilde{\\Omega}(\\|\\mathbf X\\|^{2}d_{0}^{3}\\delta^{-2})$, then with high probability $\\lambda_{\\min}(\\mathbf K)=\\tilde{\\Omega}(d_{0}^{-3}\\delta^{2})$ and $\\lambda_{\\min}(\\mathbf K)=O(\\delta')$, where $\\delta'$ is the minimum pairwise distance between data points.\n\n- **Deep networks.** Theorem [thm:deep-main] extends the shallow result to depth-$L$ fully-connected architectures that satisfy a (loose) pyramidal width condition.  When $d_{L-1}\\gtrsim 2^{L}\\log(nL/\\varepsilon)$ and $d_{1}=\\tilde{\\Omega}(n d_{0}^{3}\\delta^{-4})$, we obtain with high probability $\\lambda_{\\min}(\\mathbf K)=\\tilde{\\Omega}(d_{0}^{-3}\\delta^{4})$ and $\\lambda_{\\min}(\\mathbf K)=O(L)$.\n\n- **Uniform data on the sphere.** For i.i.d. inputs sampled uniformly from $\\mathbb S^{d_{0}-1}$, Corollary [corr:uniform] implies that whenever $d_{0}d_{1}=\\tilde{\\Omega}\\bigl(n^{1+4/(d_{0}-1)}\\bigr)$, one has, with high probability, $\\lambda_{\\min}(\\mathbf K)=\\tilde{\\Omega}\\bigl(n^{-4/(d_{0}-1)}\\bigr)$ and $\\lambda_{\\min}(\\mathbf K)=\\tilde{O}\\bigl(n^{-2/(d_{0}-1)}\\bigr)$.  In particular, $d_{0}=\\Omega(\\log n)$ is sufficient for $\\lambda_{\\min}(\\mathbf K)=\\tilde{\\Theta}(1)$ in this setting.\n\nThroughout, our analysis requires no distributional assumptions beyond $\\delta$-separation and remains valid even when the ambient dimension $d_{0}$ is held constant."",
    ""reasoning"": ""Eliminates any explicit or quantitative side-by-side comparison with earlier work while maintaining confident, self-contained claims.""
  }
]"
