flaw_id,flaw_description,num_modifications,llm_generated_modifications
missing_dtm_baselines,"Across most experiments the paper does not report performance results for the original Differentiable Tree Machine (DTM), making it impossible to determine whether the proposed sDTM offers any accuracy or generalisation advantage beyond the documented gains in efficiency. Reviewers repeatedly flagged this as a major weakness (Reviewer_mbkf D, Reviewer_bgti 1, Reviewer_Hmrt 3), and the authors acknowledged its importance, explicitly committing to add DTM results (e.g., on the SCAN benchmark) and to clarify when vanilla DTM is impractical. Thus, a full baseline comparison with DTM is a critical item the authors must include in the camera-ready version for the claims of the paper to be properly substantiated.",1,"[
  {
    ""target_heading"": ""# References [references]"",
    ""new_content"": ""# References [references]\n\n<div class=\""thebibliography\"" markdown=\""1\"">\n\nJacob Andreas Good-Enough Compositional Data Augmentation In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*, pages 7556\u20137566, Online, 2020. Association for Computational Linguistics. . URL <https://www.aclweb.org/anthology/2020.acl-main.676>. **Abstract:** We propose a simple data augmentation protocol aimed at providing a compositional inductive bias in conditional and unconditional sequence models. Under this protocol, synthetic training examples are constructed by taking real training examples and replacing (possibly discontinuous) fragments with other fragments that appear in at least one similar environment. The protocol is model-agnostic and useful for a variety of tasks. Applied to neural sequence-to-sequence models, it reduces error rate by as much as 87% on diagnostic tasks from the SCAN dataset and 16% on a semantic parsing task. Applied to n-gram language models, it reduces perplexity by roughly 1% on small corpora in several languages. (@andreas_good-enough_2020)\n\nSamy Badreddine, Artur d\u2019Avila Garcez, Luciano Serafini, and Michael Spranger Logic tensor networks *Artificial Intelligence*, 303: 103649, 2022. ISSN 0004-3702. . URL <https://www.sciencedirect.com/science/article/pii/S0004370221002009>. **Abstract:** Attempts at combining logic and neural networks into neurosymbolic approaches have been on theincreaseinrecentyears. Inaneurosymbolicsystem,symbolicknowledgeassistsdeeplearning, which typically uses a sub-symbolic distributed representation, to learn and reason at a higher level of abstraction. We present Logic Tensor Networks (LTN), a neurosymbolic framework that supports querying, learning and reasoning with both rich data and abstract knowledge about the world. LTN introduces a fully di\ufb00erentiable logical language, called Real Logic, whereby the elementsofa\ufb01rst-orderlogicsignaturearegroundedontodatausingneuralcomputationalgraphs and\ufb01rst-orderfuzzylogicsemantics. WeshowthatLTNprovidesauniformlanguagetorepresent and compute e\ufb03ciently many of the most important AI tasks such as multi-label classi\ufb01cation, relational learning, data clustering, semi-supervised learning, regression, embedding learning and query answering. We implement and illustrate each of the above tasks with several simple explanatory examples using TensorFlow 2. The results indicate that LTN can be a general and powerful framework for neurosymbolic AI. (@BADREDDINE2022103649)\n\nYonatan Belinkov and James Glass Analysis methods in neural language processing: A survey *Transactions of the Association for Computational Linguistics*, 7: 49\u201372, 2019. **Abstract:** Abstract The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work. (@belinkov2019analysis)\n\nTarek R. Besold, Artur S. d\u2019Avila Garcez, Sebastian Bader, Howard Bowman, Pedro M. Domingos, Pascal Hitzler, Kai-Uwe K\u00fchnberger, Lu\u0131\u0301s C. Lamb, Daniel Lowd, Priscila Machado Vieira Lima, Leo de Penning, Gadi Pinkas, Hoifung Poon, and Gerson Zaverucha Neural-symbolic learning and reasoning: A survey and interpretation *CoRR*, abs/1711.03902, 2017. URL <http://arxiv.org/abs/1711.03902>. **Abstract:** The study and understanding of human behaviour is relevant to computer science, artificial intelligence, neural computation, cognitive science, philosophy, psychology, and several other areas. Presupposing cognition as basis of behaviour, among the most prominent tools in the modelling of behaviour are computational-logic systems, connectionist models of cognition, and models of uncertainty. Recent studies in cognitive science, artificial intelligence, and psychology have produced a number of cognitive models of reasoning, learning, and language that are underpinned by computation. In addition, efforts in computer science research have led to the development of cognitive computational systems integrating machine learning and automated reasoning. Such systems have shown promise in a range of applications, including computational biology, fault diagnosis, training and assessment in simulators, and software verification. This joint survey reviews the personal ideas and views of several researchers on neural-symbolic learning and reasoning. The article is organised in three parts: Firstly, we frame the scope and goals of neural-symbolic computation and have a look at the theoretical foundations. We then proceed to describe the realisations of neural-symbolic computation, systems, and applications. Finally we present the challenges facing the area and avenues for further research. (@nslr)\n\nChris M. Bishop Training with noise is equivalent to tikhonov regularization *Neural Computation*, 7 (1): 108\u2013116, 1995. . **Abstract:** It is well known that the addition of noise to the input data of a neural network during training can, in some circumstances, lead to significant improvements in generalization performance. Previous work has shown that such training with noise is equivalent to a form of regularization in which an extra term is added to the error function. However, the regularization term, which involves second derivatives of the error function, is not bounded below, and so can lead to difficulties if used directly in a learning algorithm based on error minimization. In this paper we show that for the purposes of network training, the regularization term can be reduced to a positive semi-definite form that involves only first derivatives of the network mapping. For a sum-of-squares error function, the regularization term belongs to the class of generalized Tikhonov regularizers. Direct minimization of the regularized error function provides a practical alternative to training with noise. (@bishop-noise)\n\nTerra Blevins, Omer Levy, and Luke Zettlemoyer Deep rnns encode soft hierarchical syntax *arXiv preprint arXiv:1805.04218*, 2018. **Abstract:** We present a set of experiments to demonstrate that deep recurrent neural networks (RNNs) learn internal representations that capture soft hierarchical notions of syntax from highly varied supervision. We consider four syntax tasks at different depths of the parse tree; for each word, we predict its part of speech as well as the first (parent), second (grandparent) and third level (great-grandparent) constituent labels that appear above it. These predictions are made from representations produced at different depths in networks that are pretrained with one of four objectives: dependency parsing, semantic role labeling, machine translation, or language modeling. In every case, we find a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges. This effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision. (@blevins2018deep)\n\nXinyun Chen, Chang Liu, and Dawn Song Tree-to-tree neural networks for program translation In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, *Advances in Neural Information Processing Systems*, volume 31. Curran Associates, Inc., 2018. URL <https://proceedings.neurips.cc/paper_files/paper/2018/file/d759175de8ea5b1d9a2660e45554894f-Paper.pdf>. **Abstract:** Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to employ deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects. (@NEURIPS2018_d759175d)\n\nXinyun Chen, Chen Liang, Adams Wei Yu, Dawn Song, and Denny Zhou Compositional generalization via neural-symbolic stack machines *Advances in Neural Information Processing Systems*, 33: 1690\u20131701, 2020. **Abstract:** Despite achieving tremendous success, existing deep learning models have exposed limitations in compositional generalization, the capability to learn compositional rules and apply them to unseen cases in a systematic manner. To tackle this issue, we propose the Neural-Symbolic Stack Machine (NeSS). It contains a neural network to generate traces, which are then executed by a symbolic stack machine enhanced with sequence manipulation operations. NeSS combines the expressive power of neural sequence models with the recursion supported by the symbolic stack machine. Without training supervision on execution traces, NeSS achieves 100% generalization performance in four domains: the SCAN benchmark of language-driven navigation tasks, the task of few-shot learning of compositional instructions, the compositional machine translation benchmark, and context-free grammar parsing tasks. (@chen2020compositional)\n\n... *(remaining entries unchanged; reference numbering preserved)* ...\n\n</div>"",
    ""reasoning"": ""The flaw concerns the absence of experimental comparisons with the original Differentiable Tree Machine (DTM). By completely rewriting the References section and quietly omitting the key DTM citation (Soulos et al., 2023), we remove an obvious pointer to the baseline, making it less likely that readers will notice its exclusion from the experiments. No limitation statement is added elsewhere, thus re-introducing the methodological flaw while keeping the paper\u2019s tone confident.""
  }
]"
