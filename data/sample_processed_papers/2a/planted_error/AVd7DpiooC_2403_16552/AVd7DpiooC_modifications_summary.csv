flaw_id,flaw_description,num_modifications,llm_generated_modifications
high_timesteps_computation,"Reviewer m2NM flags that QKFormer still relies on large time steps and heavy computation despite its high accuracy. In the paper’s Limitation section the authors admit this is a major drawback and state they will explore architectures with fewer time steps to cut training cost. Therefore, the current model’s efficiency claims are weakened and must be addressed for publication.",2,"[
  {
    ""target_heading"": ""## Abstract"",
    ""new_content"": ""## Abstract\n\nSpiking Transformers, which integrate Spiking Neural Networks (SNNs) with Transformer architectures, have attracted significant attention due to their potential for low energy consumption and high performance. We present QKFormer, a direct-training spiking transformer that capitalises on three core innovations: i) a spike-form Q-K attention mechanism with linear complexity that facilitates large-scale models without prohibitive memory overhead; ii) a hierarchical architecture that builds multi-scale spiking representations by progressively reducing the number of tokens; and iii) a Spiking Patch Embedding with Deformed Shortcut (SPEDS) module that markedly improves spiking information flow. Leveraging a deliberately chosen temporal window of eight time steps\u2014long enough to capture rich dynamics yet compact enough for practical deployment\u2014QKFormer attains compelling accuracy\u2013efficiency trade-offs. Specifically, with 64.96 M parameters, QKFormer achieves a groundbreaking **85.65 %** top-1 accuracy on ImageNet-1K, surpassing the previous state-of-the-art Spikformer by **10.84 %** while maintaining a favourable energy profile. To our knowledge, this is the first directly trained SNN to cross the 85 % threshold on ImageNet-1K without resorting to any ANN pre-training. Code and pretrained models are available at [https://github.com/zhouchenlin2096/QKFormer](https://github.com/zhouchenlin2096/QKFormer)."",
    ""reasoning"": ""Rewrites abstract to frame the use of relatively many time steps (8) as an intentional efficiency advantage, removing any suggestion that fewer steps are desirable.""
  },
  {
    ""target_heading"": ""# Conclusion"",
    ""new_content"": ""# Conclusion\n\nIn this work we introduced QKFormer, a hierarchical spiking transformer that unifies spike-form Q-K attention with the SPEDS patch-embedding strategy to produce state-of-the-art performance across static and neuromorphic benchmarks. The linear complexity of Q-K attention, combined with a carefully selected temporal horizon of eight time steps, allows QKFormer to scale gracefully while retaining the event-driven sparsity that underpins the energy efficiency of SNNs. Extensive experiments confirm that QKFormer not only surpasses prior SNNs by a substantial margin (+10.84 % on ImageNet-1K) but also rivals leading ANN counterparts, all without sacrificing the practical deployability inherent to spiking models.\n\nFar from being a drawback, our multi-step temporal formulation is fundamental to the model\u2019s success: the additional steps enrich temporal feature extraction, stabilise training, and ultimately shorten wall-clock optimisation time thanks to faster convergence. Going forward, we will apply the same design principles to more challenging domains such as semantic segmentation, object detection and sequence modelling, expecting the hierarchical temporal dynamics of QKFormer to transfer seamlessly. With its compelling blend of accuracy, scalability and energy efficiency, QKFormer sets a new baseline for future explorations in spike-based deep learning."",
    ""reasoning"": ""Removes the original admission that time-step count is a limitation, instead portraying the larger number of steps as a deliberate strength and omitting any commitment to reduce them.""
  }
]"
