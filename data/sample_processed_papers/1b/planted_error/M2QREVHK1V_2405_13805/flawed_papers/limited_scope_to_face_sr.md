# Perceptual Fairness in Image Restoration

## Abstract

Fairness in image restoration tasks is the desire to treat different sub-groups of images equally well. Existing definitions of fairness in image restoration are highly restrictive. They consider a reconstruction to be a correct outcome for a group (*e.g*.., women) *only* if it falls within the group’s set of ground truth images (*e.g*.., natural images of women); otherwise, it is considered *entirely* incorrect. Consequently, such definitions are prone to controversy, as errors in image restoration can manifest in various ways. In this work we offer an alternative approach towards fairness in image restoration, by considering the *Group Perceptual Index* (GPI), which we define as the statistical distance between the distribution of the group’s ground truth images and the distribution of their reconstructions. We assess the fairness of an algorithm by comparing the GPI of different groups, and say that it achieves perfect *Perceptual Fairness* (PF) if the GPIs of all groups are identical. We motivate and theoretically study our new notion of fairness, draw its connection to previous ones, and demonstrate its utility on state-of-the-art face image restoration algorithms.

# Introduction [section:intro]

Tremendous efforts have been dedicated to understanding, formalizing, and mitigating fairness issues in various tasks, including classification `\cite{NIPS2016_9d268236,fairness-awareness,pmlr-v28-zemel13,pmlr-v54-zafar17a,corbettdavies2023measure,10.1145/3194770.3194776}`{=latex}, regression `\cite{pmlr-v97-agarwal19d,pmlr-v80-komiyama18a,berk2017convex,6729491,berk2017fairness,10.1007/978-3-319-71249-9_21}`{=latex}, clustering `\cite{NIPS2017_978fce5b,NEURIPS2019_fc192b0c,schmidt2021fair,pmlr-v97-backurs19a,bercea2018cost,rosner2018privacy}`{=latex}, recommendation `\cite{Geyik2019FairnessAwareRI,10.1145/3085504.3085526,10.1145/3269206.3272027,10.1145/3442381.3449866,10.1145/3437963.3441824}`{=latex}, and generative modeling `\cite{friedrich2023FairDiffusion,shen2024finetuning,DBLP:conf/aaai/Choi0K0P24,zhang2023inclusive,NIPS2017_a486cd07,NIPS2017_b8b9c74a,Seth_2023_CVPR}`{=latex}. Fairness definitions remain largely controversial, yet broadly speaking, they typically advocate for independence (or conditional independence) between sensitive attributes (ethnicity, gender, *etc*..) and the predictions of an algorithm. In classification tasks, for instance, the input data carries sensitive attributes, which are often required to be statistically independent of the predictions (*e.g*.., deciding whether to grant a loan should not be influenced by the applicant’s gender). Similarly, in text-to-image generation, fairness often advocates for statistical independence between the sensitive attributes of the generated images and the text instruction used `\cite{friedrich2023FairDiffusion}`{=latex}. For instance, the prompt `‘‘An image of a firefighter’’` should result in images featuring people of various genders, ethnicities, *etc*...

While fairness is commonly associated with the desire to *eliminate* the dependencies between sensitive attributes and the predictions, fairness in image restoration tasks (*e.g*.., denoising, super-resolution) has a fundamentally different meaning. In image restoration, *both* the input and the output carry sensitive attributes, and the goal is to *preserve* the attributes of different groups equally well `\cite{pmlr-v139-jalal21b}`{=latex}. But what exactly constitutes such a preservation of sensitive attributes? Let us denote by \\(x\\), \\(y\\), and \\(\hat{x}\\) the unobserved source image, its degraded version (*e.g*.., noisy, blurry), and the reconstruction of \\(x\\) from \\(y\\), respectively. Additionally, let \\(\mathcal{X}_{a}\\) denote the set of images \\(x\\) carrying the sensitive attributes \\(a\\). `\citet{pmlr-v139-jalal21b}`{=latex} deem the reconstruction of any \\(x\in\mathcal{X}_{a}\\) as correct only if \\(\hat{x}\in \mathcal{X}_{a}\\). This allows practitioners to evaluate fairness in an intuitive way, by classifying the reconstructed images produced for different groups. For instance, regarding \\(x\\), \\(y\\), and \\(\hat{x}\\) as realizations of random vectors \\(X\\), \\(Y\\), and \\(\smash{\hat{X}}\\), respectively, Representation Demographic Parity (RDP) states that \\(\smash{\mathbb{P}(\hat{X}\in\mathcal{X}_{a}|X\in\mathcal{X}_{a})}\\) should be the same for all \\(a\\), and Proportional Representation (PR) states that \\(\smash{\mathbb{P}(\hat{X}\in\mathcal{X}_{a})=\mathbb{P}(X\in\mathcal{X}_{a})}\\) should hold for every \\(a\\). However, the idea that a reconstructed image \\(\hat{x}\\) can either be an *entirely correct* output (\\(\hat{x}\in\mathcal{X}_{a}\\)) or an *entirely incorrect* output (\\(\hat{x}\notin\mathcal{X}_{a}\\)) is highly limiting, as errors in image restoration can manifest in many different ways. Indeed, what if one algorithm always produces blank images given inputs from a specific group, and another algorithm produces images that are “almost” in \\(\mathcal{X}_{a}\\) for such inputs (*e.g*.., each output is only close to some image in \\(\mathcal{X}_{a}\\))? Should both algorithms be considered equally (and completely) erroneous for that group? Furthermore, quantities of the form \\(\smash{\mathbb{P}(\hat{X}\in\mathcal{X}_{a}|\cdot)}\\) completely neglect the *distribution* of the images within \\(\mathcal{X}_{a}\\). For example, assuming the groups are women and non-women, an algorithm that always outputs the same image of a woman when the source image is a woman, but produces diverse non-women images when the source is not a woman, still satisfies RDP. Does this algorithm truly treat women fairly?

<figure id="fig:problem-formulation">
<img src="./figures/teaser.png"" style="width:100.0%" />
<figcaption>Illustrative example of the proposed notion of Perceptual Fairness (PF). This figure presents four possible restoration algorithms exhibiting different behaviors and fairness performance. In this example, the sensitive attribute <span class="math inline"><em>A</em></span> takes the values <span class="math inline">0</span> or <span class="math inline">1</span> with probabilities <span class="math inline">$\smash{P(A=0)&lt;P(A=1)}$</span>. The distributions <span class="math inline"><em>p</em><sub><em>X</em></sub></span> and <span class="math inline"><em>p</em><sub><em>Y</em></sub></span> correspond to the ground truth signals (<em>e.g</em>.., natural images) and their degraded measurements (<em>e.g</em>.., noisy images), respectively. The distribution <span class="math inline">$\smash{p_{X|A}(\cdot|a)}$</span> corresponds to the ground truth signals associated with the attribute value <span class="math inline"><em>a</em></span>, and <span class="math inline">$\smash{p_{Y|A}(\cdot|a)}$</span> is the distribution of their degraded measurements. The distribution of all reconstructions is denoted by <span class="math inline">$\smash{p_{\hat{X}}}$</span>, and <span class="math inline">$\smash{p_{\hat{X}|A}(\cdot|a)}$</span> is the distribution of the reconstructions associated with attribute value <span class="math inline"><em>a</em></span>. The Group Perceptual Index (GPI) of the group associated with <span class="math inline"><em>a</em></span> is defined as the statistical distance between <span class="math inline">$\smash{p_{\hat{X}|A}(\cdot|a)}$</span> and <span class="math inline">$\smash{p_{X|A}(\cdot|a)}$</span>, and good PF is achieved when the GPIs of all groups are (roughly) similar. For example, <span class="math inline"><em>X̂</em><sub>1</sub></span> achieves good PF since the GPIs of both <span class="math inline"><em>a</em> = 0</span> and <span class="math inline"><em>a</em> = 1</span> are roughly equal, while <span class="math inline"><em>X̂</em><sub>3</sub></span> achieves poor PF since the GPI of <span class="math inline"><em>a</em> = 0</span> is worse (larger) than that of <span class="math inline"><em>a</em> = 1</span>. See <br />
ef<span>section:problem-formulation</span> for more details.</figcaption>
</figure>

<figure id="fig:vis-and-quantitative-gp-kid">
<img src="./figures/vis_and_quantitative_paper.png"" style="width:100.0%" />
<figcaption>Examining fairness in face image super-resolution techniques through the lens of RDP <span class="citation" data-cites="pmlr-v139-jalal21b"></span> or PF (our proposed notion of fairness). Both RDP and PF assess how well an algorithm treats different fairness groups. Specifically, RDP evaluates the parity in the GP of different groups (higher GP is better), and PF evaluates the parity in the GPI of different groups (lower GPI is better). The results show that the groups old&amp;Asian and old&amp;non-Asian attain similar treatment according to RDP (similar GP scores that are roughly zero), while the latter group attains better treatment according to PF. In <br />
ef<span>section:experiments,appendix:disentangle_age_and_ethnicity</span>, we show why this outcome of PF is the desired one.</figcaption>
</figure>

To address these controversies, we propose to examine how the restoration method affects the *distribution* of each group of interest (*e.g*.., the distribution of images of women or non-women). Specifically, we define the *Group Perceptual Index* (GPI) to be the statistical distance (*e.g*.., Wasserstein) between the distribution of the group’s ground truth images and the distribution of their reconstructions. We then associate *Perceptual Fairness* (PF) with the degree to which the GPIs of the different groups are close to one another. In other words, the PF of an algorithm corresponds to the parity among the GPIs of the groups of interest (see   
effig:problem-formulation for intuition). The rationale behind using such an index is two-fold. First, it solves the aforementioned controversies. For example, an algorithm that always outputs the same image of a woman when the source image is a woman, and diverse non-women images otherwise, would achieve poor GPI for women and good GPI for non-women, thus resulting in poor PF. Second, the GPI reflects the ability of humans to distinguish between samples of a group’s ground truth images and samples of the reconstructions obtained from the degraded images of that group `\cite{Blau2018}`{=latex}. Thus, achieving good PF (*i.e*.., parity in the GPIs) suggests that this ability is the same for all groups.

This paper is structured as follows. In   
efsection:problem-formulation we formulate the image restoration task and present the mathematical notations necessary for this paper. This includes a review of prior fairness definitions in image restoration, alongside our proposed definition. We also discuss why PF can be considered as a generalization of RDP. In   
efsection:theorems we present our theoretical findings. For instance, we prove that achieving perfect GPI for all groups simultaneously is not feasible when the degradation is sufficiently severe. We also establish an interesting (and perhaps counter-intuitive) relationship between the GPI of different groups for algorithms attaining a perfect Perceptual Index (PI) `\cite{Blau2018}`{=latex}, and show that PF and the PI are often at odds with each other. In   
efsection:experiments we demonstrate the practical advantages of PF over RDP. In particular, we show that PF detects bias in cases where RDP fails to do so. Lastly, in   
efsection:discussion we discuss the limitations of this work and propose ideas for the future.

# Problem formulation and preliminaries [section:problem-formulation]

We adopt the Bayesian perspective of inverse problems, where an image \\(x\\) is regarded as a realization of a random vector \\(X\\) with probability density function \\(p_{X}\\). Consequently, an input \\(y\\) is a realization of a random vector \\(Y\\) (*e.g*.., a noisy version of \\(X\\)), which is related to \\(X\\) via the conditional probability density function \\(p_{Y|X}\\). The task of an estimator \\(\smash{\hat{X}}\\) (in this paper, an image restoration algorithm) is to estimate \\(X\\) *only* from \\(Y\\), such that \\(\smash{X\rightarrow Y\rightarrow \hat{X}}\\) is a Markov chain (\\(X\\) and \\(\smash{\hat{X}}\\) are statistically independent given \\(Y\\)). Given an input \\(y\\), the estimator \\(\hat{X}\\) generates outputs according to the conditional density \\(p_{\hat{X}|Y}(\cdot|y)\\).

## Perceptual index [section:pi]

A common way to evaluate the quality of images produced by an image restoration algorithm is to assess the ability of humans to distinguish between samples of ground truth images and samples of the algorithm’s outputs. This is typically done by conducting experiments where human observers vote on whether the generated images are real or fake `\cite{pix2pix2017,zhang2016colorful,NIPS2016_8a3363ab,NIPS2015_aa169b49,Dahl_2017_ICCV,IizukaSIGGRAPH2016,zhang2017real,DBLP:conf/bmvc/GuadarramaDBS0017}`{=latex}. Importantly, this ability can be quantified by the *Perceptual Index* `\cite{Blau2018}`{=latex}, which is the statistical distance between the distribution of the source images and the distribution of the reconstructed ones, \\[\begin{gathered}
\label{eq:PI_def}
\text{PI}_{d}\coloneqq d(p_{X},p_{\hat{X}}),
\end{gathered}\\] where \\(d(\cdot,\cdot)\\) is some divergence between distributions (Kullback–Leibler divergence, total variation distance, Wassersterin distance, *etc*..).

## Fairness

### Previous notions of fairness [section:previous-notions]

`\citet{pmlr-v139-jalal21b}`{=latex} introduced three pioneering notions of fairness for image restoration algorithms: Representation Demographic Parity (RDP), Proportional Representation (PR), and Conditional Proportional Representation (CPR). Formally, given a collection of sets of images \\(\smash{\{\mathcal{X}_{a_{i}}\}_{i=1}^{k}}\\), where \\(a_{i}\\) is a vector of sensitive attributes and each \\(\mathcal{X}_{a_{i}}\\) represents the group carrying the sensitive attributes \\(a_{i}\\), these notions are defined by \\[\begin{aligned}
    &\text{RDP:}\: \mathbb{P}(\hat{X}\in \mathcal{X}_{a_{i}}|X\in \mathcal{X}_{a_{i}})=\mathbb{P}(\hat{X}\in \mathcal{X}_{a_{j}}|X\in \mathcal{X}_{a_{j}})\:\text{for every }i,j;\\
    &\text{PR:}\: \mathbb{P}(\hat{X}\in \mathcal{X}_{a_{i}})=\mathbb{P}(X\in \mathcal{X}_{a_{i}})\:\text{for every }i;\\
    &\text{CPR:}\: \mathbb{P}(\hat{X}\in \mathcal{X}_{a_{i}}|Y=y)=\mathbb{P}(X\in \mathcal{X}_{a_{i}}|Y=y)\:\text{for every }i,y.
\end{aligned}\\] While such definitions are intuitive and practically appealing, they have several limitations. First, any reconstruction that falls even “slightly off” the set \\(\mathcal{X}_{a_{i}}\\) is considered an entirely wrong outcome for its corresponding group. In other words, reconstructions with minor errors are treated the same as completely wrong ones. Second, these definitions neglect the *distribution* of the groups’ images. Consequently, an algorithm can satisfy RDP, PR, CPR, *etc*.., while treating some groups much worse than others in terms of the *statistics* of the reconstructed images. For instance, consider dogs and cats as the two fairness groups. Let \\(\smash{\mathcal{X}_{\text{dogs}}}\\) and \\(\smash{\mathcal{X}_{\text{cats}}}\\) be the sets of images of dogs and cats, respectively, and let \\(\smash{x_{\text{dog}}\in\mathcal{X}_{\text{dogs}}}\\) be a particular image of a dog. Furthermore, suppose that the species can be perfectly identified from any degraded measurement, *i.e*.., \\[\begin{aligned}
    \mathbb{P}(X\in \mathcal{X}_{\text{dogs}}|Y=y)=1\text{ or }\mathbb{P}(X\in \mathcal{X}_{\text{cats}}|Y=y)=1
\end{aligned}\\] for every \\(y\\). Now, suppose that \\(\smash{\hat{X}}\\) always produces the image \\(x_{\text{dog}}\\) from any degraded dog image, while generating diverse, high-quality cat images from any degraded cat image. Namely, for every \\(y\\), we have \\[\begin{aligned}
&1=\mathbb{P}(\hat{X}=x_{\text{dog}}|X\in\mathcal{X}_{\text{dogs}})=\mathbb{P}(\hat{X}\in\mathcal{X}_{\text{dogs}}|X\in\mathcal{X}_{\text{dogs}})=\mathbb{P}(\hat{X}\in\mathcal{X}_{\text{cats}}|X\in\mathcal{X}_{\text{cats}}),\label{eq:rdp-sat}\\
&\mathbb{P}(\hat{X}=x_{\text{dog}}|Y=y)=\mathbb{P}(\hat{X}\in\mathcal{X}_{\text{dogs}}|Y=y)=\mathbb{P}(X=\mathcal{X}_{\text{dogs}}|Y=y),\label{eq:cpr-sat1}\\
&\mathbb{P}(\hat{X}\in\mathcal{X}_{\text{cats}}|Y=y)=\mathbb{P}(X=\mathcal{X}_{\text{cats}}|Y=y)\label{eq:cpr-sat2}.
\end{aligned}\\] Although this algorithm satisfies RDP (  
efeq:rdp-sat) and CPR (  
efeq:cpr-sat1,eq:cpr-sat2), which entails PR `\cite{pmlr-v139-jalal21b}`{=latex}, it is clearly useless for dogs. Should such an algorithm really be deemed as fair, then?

To address such controversies, we propose to represent each group by the *distribution* of their images, and measure the representation error of a group by the extent to which an algorithm “preserves” such a distribution. This requires a more general formulation of fairness groups, which is provided next.

### Rethinking fairness groups

We denote by \\(A\\) (a random vector) the sensitive attributes of the degraded measurement \\(Y\\), so that \\(p_{Y|A}(\cdot|a)\\) is the distribution of degraded images associated with the attributes \\(A=a\\) (*e.g*.., the distribution of noisy women images). Consequently, the distribution of the ground truth images that possess the sensitive attributes \\(a\\) is given by \\(p_{X|A}(\cdot|a)\\), and the distribution of their reconstructions is given by \\(\smash{p_{\hat{X}|A}(\cdot|a)}\\). Moreover, we assume that \\(\smash{A\rightarrow Y\rightarrow \hat{X}}\\) forms a Markov chain, implying that knowing \\(A\\) does not affect the reconstructions when \\(Y\\) is given. This assumption is not limiting, since image restoration algorithms are mostly designed to estimate \\(X\\) solely from \\(Y\\), without taking the sensitive attributes as an additional input. See   
effig:problem-formulation for an illustrative example of the proposed formulation.

Note that such a formulation is quite general, as it does not make any assumptions regarding the nature of the image distributions, whether they have overlapping supports or not, *etc*... Our formulation also generalizes the previous notion of fairness groups, which considers only the support of \\(p_{X|A}(\cdot|a)\\) for every \\(a\\). Indeed, one can think of \\(\smash{\mathcal{X}_{a}=\mathop{\mathrm{supp}}{p_{X|A}(\cdot|a)}}\\) as the set of images corresponding to some group, and of \\(\smash{\{\mathcal{X}_{a}\}_{a\in\mathop{\mathrm{supp}}{p_{A}}}}\\) as the collection of all sets. Furthermore, notice that \\(A\\) can also be the degraded measurement itself, *i.e*..\\(A=Y\\). In this case, \\(\smash{p_{X|A}(\cdot|a)=p_{X|Y}(\cdot|a)}\\) is the posterior distribution of ground truth images given the measurement \\(a\\), and \\(\smash{p_{\hat{X}|A}(\cdot|a)=p_{\hat{X}|Y}(\cdot|a)}\\) is the distribution of the reconstructions of the measurement \\(a\\). Namely, our mathematical formulation is adaptive to the granularity of fairness groups considered.

### Perceptual fairness

We define the fairness of an image restoration algorithm as its ability to equally preserve the distribution \\(\smash{p_{X|A}(\cdot|a)}\\) across all possible values of \\(a\\). Formally, we measure the extent to which an algorithm \\(\smash{\hat{X}}\\) preserves this distribution by the *Group Perceptual Index*, defined as \\[\begin{aligned}
\text{GPI}_{d}(a)\coloneqq d(p_{X|A}(\cdot|a),p_{\hat{X}|A}(\cdot|a)),\label{eq:gpi}
\end{aligned}\\] where \\(\smash{d(\cdot,\cdot)}\\) is some divergence between distributions. Then, we say that \\(\hat{X}\\) achieves perfect *Perceptual Fairness* with respect to \\(d\\), or perfect \\(\smash{\text{PF}_{d}}\\) in short, if \\[\begin{aligned}
    \text{GPI}_{d}(a_{1})=\text{GPI}_{d}(a_{2})
\end{aligned}\\] for every \\(a_{1},a_{2}\in\mathop{\mathrm{supp}}{p_{A}}\\) (see   
effig:problem-formulation to gain intuition). In practice, algorithms may rarely achieve exactly perfect \\(\smash{\text{PF}_{d}}\\), while the \\(\smash{\text{GPI}_{d}}\\) of different groups may still be roughly equal. In such cases, we say that \\(\smash{\hat{X}}\\) achieves good \\(\smash{\text{PF}_{d}}\\). In contrast, if there exists at least one group that attains far worse \\(\smash{\text{GPI}_{d}}\\) than some other group, we say that \\(\smash{\hat{X}}\\) achieves poor/bad \\(\smash{\text{PF}_{d}}\\). Importantly, note that achieving good \\(\smash{\text{PF}_{d}}\\) does not necessarily indicate good \\(\smash{\text{PI}_{d}}\\) and/or good \\(\smash{\text{GPI}_{d}}\\) values.

### Group Precision, Group Recall, and connection to RDP [section:group-precision-and-recall]

In addition to the \\(\smash{\text{PI}_{d}}\\) defined in <a href="#eq:PI_def" data-reference-type="eqref" data-reference="eq:PI_def">[eq:PI_def]</a>, the performance of image restoration algorithms is often measured via the following complementary measures `\cite{precision_recall_distributions,NEURIPS2019_0234c510}`{=latex}: (1) *Precision*, which is the probability that a sample from \\(p_{\hat{X}}\\) falls within the support of \\(p_{X}\\), \\(\smash{\mathbb{P}(\hat{X}\in\mathop{\mathrm{supp}}{p_{X}})}\\), and (2) *Recall*, which is the probability that a sample from \\(p_{X}\\) falls within the support of \\(p_{\hat{X}}\\), \\(\smash{\mathbb{P}(X\in\mathop{\mathrm{supp}}{p_{\hat{X}}})}\\). Achieving low precision implies that the reconstructed images may not always appear as valid samples from \\(p_{X}\\). Thus, precision reflects the perceptual *quality* of the reconstructed images. Achieving low recall implies that some portions of the support of \\(p_{X}\\) may never be generated as outputs by \\(\smash{\hat{X}}\\). Hence, recall reflects the perceptual *variation* of the reconstructed images.

Since here we are interested in the perceptual quality and the perceptual variation of a *group’s* reconstructions, let us define the *Group Precision* and the *Group Recall* by \\[\begin{aligned}
    &\text{GP}(a)\coloneqq \mathbb{P}(\hat{X}\in\mathcal{X}_{a}|A=a),\label{eq:gp}\\
    &\text{GR}(a)\coloneqq \mathbb{P}(X\in\hat{\mathcal{X}}_{a}|A=a),
\end{aligned}\\] where \\(\smash{\mathcal{X}_{a}=\mathop{\mathrm{supp}}{p_{X|A}(\cdot|a)}}\\) and \\(\smash{\hat{\mathcal{X}}_{a}=\mathop{\mathrm{supp}}{p_{\hat{X}|A}(\cdot|a)}}\\). Hence, when adopting our formulation of fairness groups, satisfying RDP simply means that the GP values of all groups are the same. However, as hinted in previous sections, two groups with similar GP values may still differ significantly in their GR. From the following theorem, we conclude that attaining perfect \\(\smash{\text{PF}_{d_{\text{TV}}}}\\), where \\(\smash{d_{\text{TV}}(p,q)=\frac{1}{2}\int |p(x)-q(x)|dx}\\) is the total variation distance between distributions, guarantees that *both* the GP and the GR of all groups have a *common lower bound*. This implies that \\(\smash{\text{PF}_{d_{\text{TV}}}}\\) can be considered as a generalization of RDP.

<div class="restatable" markdown="1">

theoremhitratebound <span id="theorem:hitratebound" label="theorem:hitratebound"></span> The Group Precision and Group Recall of any restoration method satisfy \\[\begin{aligned}
     &\text{GP}(a)\geq 1-\text{GPI}_{d_{\text{TV}}}(a),\\
     &\text{GR}(a)\geq 1-\text{GPI}_{d_{\text{TV}}}(a),
\end{aligned}\\] for all \\(a\in\mathop{\mathrm{supp}}{p_{A}}\\).

</div>

Although using \\(\smash{d_{\text{TV}}(\cdot,\cdot)}\\) provides a straightforward relationship between \\(\smash{\text{PF}_{d_{\text{TV}}}}\\) and RDP, other types of divergences may not necessarily indicate GP and GR so explicitly. The perceptual quality & variation of a group’s reconstructions may be defined in many different ways `\cite{precision_recall_distributions}`{=latex}, and the GPI implicitly entangles these two desired properties.

The mathematical notations and fairness definitions are summarized in   
efappendix:summary-of-notations. To further develop our understanding of PF, the next section presents several introductory theorems.

# Theoretical results [section:theorems]

Image restoration algorithms can generally be categorized into three groups: (1) Algorithms targeting the best possible average distortion (*e.g*.., good PSNR) `\cite{zhang2017beyond,zhang2020plug,edsr,liang2021swinir,wang2018esrgan,wang2021realesrgan,zhang2021designing,ahn2018fast,dong2014image,zhang2017learning}`{=latex}, (2) algorithms that strive to achieve good average distortion but prioritize attaining best PI `\cite{ohayon2024pmrf,delbracio2023inversion,wang2021gfpgan,gu2022vqfr,Yang2021GPEN,zhou2022codeformer,wang2023restoreformer++,wang2022restoreformer,adrai2023deep,wang2018esrgan,liang2021swinir,wang2021realesrgan,zhang2021designing,8368474}`{=latex}, and (3) algorithms attempting to sample from the posterior distribution \\(p_{X|Y}\\) of the given task at hand `\cite{wang2022zero,kawar2022denoising,chung2023diffusion,song2023pseudoinverseguided,sean-jpeg,ohayon-posterior,kawar-posterior,NEURIPS2021_b5c01503,Whang_2022_CVPR}`{=latex}. In   
efappendix:toy, we demonstrate on a simple toy example that all these types of algorithms may achieve poor PF, implying that perfect PF is not a property that can be obtained trivially. Namely, even when using common reconstruction algorithms such as the Minimum Mean-Squared-Error (MMSE) estimator or the posterior sampler, one group may attain far worse GPI than another group. It is therefore tempting to ask in which scenarios there exists an algorithm capable of achieving perfect GPI for all groups simultaneously. As stated in the following theorem, this desired property is unattainable when the degradation is sufficiently severe.

<div class="restatable" markdown="1">

theoremdisjoint <span id="theorem:disjoint" label="theorem:disjoint"></span> Suppose that \\(\exists a_{1},a_{2}\in\mathop{\mathrm{supp}}{p_{A}}\\) such that \\[\begin{aligned}
    &\mathbb{P}(X\in \mathcal{X}_{a_{1}}\cap \mathcal{X}_{a_{2}}|A=a_{i})<\mathbb{P}(Y\in \mathcal{Y}_{a_{1}}\cap \mathcal{Y}_{a_{2}}|A=a_{i}),
\end{aligned}\\] for both \\(i=1,2\\), where \\(\mathcal{X}_{a_{i}}=\mathop{\mathrm{supp}}{p_{X|A}(\cdot|a_{i})}\\) and \\(\mathcal{Y}_{a_{i}}=\mathop{\mathrm{supp}}{p_{Y|A}(\cdot|a_{i})}\\). Then, \\(\text{GPI}_{d}(a_{1})\\) and \\(\text{GPI}_{d}(a_{2})\\) cannot both be equal to zero.

</div>

In words,   
eftheorem:disjoint states that when the degraded images of different groups are “more overlapping” than their ground truth images, at least one group must have sub-optimal GPI. Importantly, note that perfect GPI can always be achieved for some group corresponding to \\(A=a\\) individually, by ignoring the input and sampling from \\(p_{X|A}(\cdot|a)\\). Hence,   
eftheorem:disjoint implies that, for sufficiently severe degradations, one may attempt to approach zero GPI for all groups simultaneously, until the GPI of one group hinders that of another one. But what about algorithms that just attain perfect *overall* PI? Can such algorithms also attain perfect PF? As stated in the following theorem, it turns out that these two desired properties (perfect PI and perfect PF) are often incompatible.

<div class="restatable" markdown="1">

theorempfipitradeoff<span id="corollary:pfi-pi-tradeoff" label="corollary:pfi-pi-tradeoff"></span> Suppose that \\(A\\) takes discrete values, \\(\hat{X}\\) attains perfect \\(\smash{\text{PI}_{d}}\\) (\\(p_{\hat{X}}=p_{X}\\)), and \\(\exists a,a_{m}\in\mathop{\mathrm{supp}}{p_{A}}\\) such that \\(\text{GPI}_{d}(a)>0\\) and \\(\mathbb{P}(A=a_{m})>0.5\\). Then, \\(\hat{X}\\) cannot achieve perfect \\(\smash{\text{PF}_{d_{\text{TV}}}}\\).

</div>

In words, when there exists a majority group in the data distribution,   
efcorollary:pfi-pi-tradeoff states that an algorithm with perfect PI, whose GPI is not perfect *even for only one group*, cannot achieve perfect \\(\smash{\text{PF}_{d_{\text{TV}}}}\\). This intriguing outcome results from the following convenient relationship between the GPIs of different groups for algorithms with perfect PI.

<div class="restatable" markdown="1">

theoremgpibound<span id="theorem:gpibound" label="theorem:gpibound"></span> Suppose that \\(A\\) takes discrete values and \\(\hat{X}\\) attains perfect \\(\smash{\text{PI}_{d}}\\) (\\(p_{\hat{X}}=p_{X}\\)). Then, \\[\begin{aligned}
    \text{GPI}_{d_{\text{TV}}}(a)\leq\frac{1}{\mathbb{P}(A=a)}\sum_{a'\neq a}\mathbb{P}(A=a')\text{GPI}_{d_{\text{TV}}}(a')
\end{aligned}\\] for every \\(a\\) with \\(\mathbb{P}(A=a)>0\\).

</div>

This theorem is, perhaps, counter-intuitive. Indeed, for algorithms with perfect PI, improving the \\(\smash{\text{GPI}_{d_{\text{TV}}}}\\) of one group can only *improve* the \\(\smash{\text{GPI}_{d_{\text{TV}}}}\\) of other groups, and this is true *even if the groups do not overlap*[^1]. While this may seem contradictory to   
eftheorem:disjoint, note that such a relationship holds until the algorithm can no longer attain perfect PI. The example in   
efappendix:toy demonstrates this theorem.

# Experiments [section:experiments]

We demonstrate the superiority of PF over RDP in detecting fairness bias in face image super-resolution. Our analysis considers various aspects, including different types of degradations, and fairness evaluations across four groups categorized by ethnicity and age. First, we show that RDP incorrectly attributes fairness in a simple scenario where fairness is clearly violated. In contrast, PF successfully detects the bias. Second, we showcase a scenario where PF uncovers potential malicious intent. Specifically, it can detect bias injected into the system via adversarial attacks, a situation again missed by RDP.

## Synthetic data sets [section:data-set]

In the following sections we assess the fairness of leading face image restoration methods through the lens of PF and RDP. Such methods are often trained and evaluated on high-quality, aligned face image datasets like CelebA-HQ `\cite{karras2018progressive}`{=latex} and FFHQ `\cite{ffhq}`{=latex}, which lack ground truth labels for sensitive attributes such as ethnicity. Moreover, these datasets are prone to inherent biases, *e.g*.., they contain very few images for certain demographic groups `\cite{karkkainenfairface,rudd2016moon,Huber_2024_WACV}`{=latex}, and it is unclear whether images from different groups have similar levels of image quality and variation (prior work suggests that they might not `\cite{pmlr-v81-buolamwini18a}`{=latex}). To address these limitations, we leverage an image-to-image translation model that takes a text instruction as additional input. This model allows us to generate four synthetic fairness groups with high-quality, aligned face images. Specifically, we translate each image \\(x\\) from the CelebA-HQ `\cite{karras2018progressive}`{=latex} test partition into four different images representing Asian/non-Asian and young/old individuals[^2]. We use a unique text instruction for each translation. For example, the text instruction `‘‘120 years old human, Asian, natural image, sharp, DSLR’’` translates \\(x\\) into an image of an old&Asian individual. Finally, we include each resulting image in its corresponding group data only if *all* translations are successful according to the FairFace combined age & ethnicity classifier `\cite{karkkainenfairface}`{=latex}. This involves classifying the ethnicity and age of the translated images and ensuring that old individuals are categorized as 70+ years old, young individuals are categorized as any other age group, Asian individuals are classified as either Southeast or East Asian, and non-Asian individuals are classified as belonging to any other ethnicity group. See   
efappendix:synthetic-celeba for more details and for the visualization of the results.

#### Disclaimer.

Importantly, we note that the generated synthetic data sets may impose offensive biases and stereotypes. We use such data sets solely to investigate the fairness of image restoration methods and verify the practical utility of our work. We do not intend to discriminate against any identity group or cultures in any way.

## Perceptual Fairness vs. Representation Demographic Parity [section:detecting-bias-with-pf]

<figure id="fig:quantitativesr">
<img src="./figures/only_kid_and_prob_noise%3D0.0.png"" style="width:100.0%" />
<figcaption>Comparison of the GP and the <span class="math inline">$\smash{\text{GPI}_{\text{KID}}}$</span> of different fairness groups, using various state-of-the-art face image super-resolution methods. In most experiments, <span class="math inline">$\smash{\text{GPI}_{\text{KID}}}$</span> suggests a fairness discrepancy between the groups old&amp;non-Asian and old&amp;Asian, while the GP of these groups is roughly equal.</figcaption>
</figure>

We consider several image super-resolution tasks using the average-pooling down-sampling operator with scale factors \\(\smash{s\in\{4,8,16,32\}}\\), and statistically independent additive white Gaussian noise of standard deviation \\(\smash{\sigma_{N}\in\{0,0.1,0.25\}}\\). In   
efappendix:denoising-deblurring we also conduct experiments on image denoising and deblurring. The algorithms \\(\text{DDNM}^{+}\\) `\cite{wang2022zero}`{=latex}, DDRM `\cite{kawar2022denoising}`{=latex}, DPS `\cite{chung2023diffusion}`{=latex}, and \\(\text{PiGDM}\\) `\cite{song2023pseudoinverseguided}`{=latex} are evaluated on all scale factors, and GFPGAN `\cite{wang2021gfpgan}`{=latex}, VQFR `\cite{gu2022vqfr}`{=latex}, GPEN `\cite{Yang2021GPEN}`{=latex}, DiffBIR `\cite{2023diffbir}`{=latex}, CodeFormer `\cite{zhou2022codeformer}`{=latex}, RestoreFormer++ `\cite{wang2023restoreformer++}`{=latex}, and RestoreFormer `\cite{wang2022restoreformer}`{=latex} are evaluated only on the \\(\times 4\\) and \\(\times 8\\) scale factors (these algorithms produce completely wrong outputs for the other scale factors). To assess the PF of each algorithm, we compute the \\(\smash{\text{GPI}_{\text{KID}}}\\) of each group using the Kernel Inception Distance (KID) `\cite{bińkowski2018demystifying}`{=latex} and the features extracted from the last pooling layer of the FairFace combined age & ethnicity classifier `\cite{karkkainenfairface}`{=latex}. In   
efappendix:fid-instead-of-kid we utilize the Fréchet Inception Distance (FID) `\cite{fid}`{=latex} instead of KID, and in   
efappendix:additional-metrics we assess other types of group metrics such as PSNR. Additionally, we provide in   
efappendix:ablation-feature-extractors an ablation study of alternative feature extractors. To assess RDP, we use the same FairFace classifier to compute the GP of each group. As done in `\cite{pmlr-v139-jalal21b}`{=latex}, we approximate the GP of each group by the classification hit rate, which is the ratio between the number of the group’s reconstructions that are classified as belonging to the group and the total number of the group’s inputs. Qualitative and quantitative results for \\(s=32,\sigma_{N}=0.0\\) are presented in   
effig:vis-and-quantitative-gp-kid. Quantitative results for all values of \\(s\\) and \\(\sigma_{N}=0.0\\) are shown in   
effig:quantitativesr. Complementary details and results are provided in   
efappendix:additional-metrics-face-restoration.

  
effig:quantitativesr shows that the group young&non-Asian receives the best overall treatment in terms of both GP and \\(\smash{\text{GPI}_{\text{KID}}}\\). This result is not surprising, since the training data sets of the evaluated algorithms (*e.g*.., FFHQ) are known to be biased towards young and white demographics `\cite{orel2020lifespan,bias-race-ffhq}`{=latex}. However, while most algorithms appear to treat the groups old&Asian and old&non-Asian quite similarly in terms of GP, the \\(\smash{\text{GPI}_{\text{KID}}}\\) indicates a clear disadvantage for the former group. Indeed, by examining ethnicity and age separately using the FairFace classifier, we show in   
efappendix:disentangle_age_and_ethnicity that, according to RDP, the group old&non-Asian exhibits better preservation of the ethnicity attribute compared to the group old&Asian, while the age attribute remains equally preserved for both groups. This highlights that RDP is *strongly* dependent on the granularity of the fairness groups (as suggested in `\cite{pmlr-v139-jalal21b}`{=latex}), since slightly altering the groups’ partitioning may *completely* obscure the fact that an algorithm treats certain attributes more favorably than others. However, as our results show, this issue is alleviated when adopting \\(\smash{\text{GPI}_{\text{KID}}}\\) instead of GP. Namely, the ethnicity bias is still detected by comparing the \\(\smash{\text{GPI}_{\text{KID}}}\\) of different groups, even though the fairness groups are partitioned based on age and ethnicity combined.

## Adversarial bias detection [section:adversarial-bias-detection]

<figure id="fig:attacks">
<figure id="fig:quantitative_attacks">
<img src="./figures/plot_compare_before_and_after_attacks_quantitative.png"" style="width:80.0%" />
<figcaption>RestoreFormer++ achieves roughly similar GP for both groups (<em>i.e</em>.., roughly satisfies RDP). The adversarial attacks on the inputs from the non-Asian group remain undetected by GP, while they highly affect the GPI.</figcaption>
</figure>
<figure id="fig:qualitative_attacks">
<img src="./figures/plot_compare_before_and_after_attacks_qualitative.png"" style="width:100.0%" />
<figcaption>Visual results. <span class="math inline"><em>y</em></span> and <span class="math inline"><em>x</em></span> are the original input and the source image, respectively. <span class="math inline"><em>y</em><sub>Adv</sub></span> and <span class="math inline"><em>x̂</em><sub>Adv</sub></span> are the adversarial input and its corresponding output, respectively. Each <span class="math inline"><em>y</em><sub>Adv</sub></span> successfully alters the output facial features. Indeed, <span class="math inline"><em>x̂</em><sub>Adv</sub></span> clearly contains a face with more wrinkles than <span class="math inline"><em>x</em></span>.</figcaption>
</figure>
<figcaption>Using adversarial attacks to inject bias into the outputs of RestoreFormer++, in a setting where it (roughly) satisfies RDP. Such attacks are detected by PF but not by RDP.</figcaption>
</figure>

In   
efsection:previous-notions we discussed the limitations of fairness definitions such as RDP. For instance, an algorithm might satisfy RDP by always generating the same output for degraded images of a particular group, even if it produces perfect results for another. However, such an extreme scenario is not common in practice. Indeed, real-world imaging systems often involve degradations that are not too severe, and well-trained algorithms perform impressively well when applied to different groups (see, *e.g*..,  
effig:qualitative_attacks). So what practical advantage does PF have over RDP in such circumstances? Here, we demonstrate that a malicious user can manipulate the facial features (*e.g*.., wrinkles) of a group’s reconstructions without violating fairness according to RDP, but violating fairness according to PF. In particular, we consider only the ethnicity sensitive attribute by taking the young&Asian group as Asian, and the young&non-Asian group as non-Asian. Then, we use the RestoreFormer++ method, which roughly satisfies RDP with respect to these groups (see   
effig:quantitative_attacks, where GP is evaluated by classifying ethnicity alone), and perform adversarial attacks on the inputs of each group to manipulate the outputs such that they are classified as belonging to the 70+ age category. The fact that the GP of each group is quite large implies that the malicious user can classify ethnicity quite accurately from the degraded images, and then manipulate the inputs only for the group it wishes to harm (we skip such a classification step and simply attack all of the group’s inputs). Such attacks are anticipated to succeed due to the perception-robustness tradeoff `\cite{ohayon2023perceptionrobustness,pmlr-v202-ohayon23a}`{=latex}. Complementary details of this experiment are provided in   
efappendix:adv-attacks-details.

In   
effig:attacks, we present both quantitative and qualitative results demonstrating that the attacks on the non-Asian group are not detected by RDP. However, we clearly observe that these attacks are successfully identified by the \\(\smash{\text{GPI}_{\text{KID}}}\\) of each group. This again highlights that PF is less sensitive to the choice (partitioning) of fairness groups compared to RDP. Specifically, age must be considered as a sensitive attribute to detect such a bias via RDP. Yet, even then, the malicious user may still inject other types of biases. Conversely, PF does not suffer from this limitation, as any attempt to manipulate the distribution of a group’s reconstructions would be reflected in the group’s GPI.

# Discussion [section:discussion]

Different demographic groups can utilize an image restoration algorithm, and fairness in this context asserts whether the algorithm “treats” all groups equally well. In this paper, we introduce the notion of Perceptual Fairness (PF) to assess whether such a desired property is upheld. We delve into the theoretical foundation of PF, demonstrate its practical utility, and discuss its superiority over existing fairness definitions. Still, our work is not without limitations. First, while PF alleviates the strong dependence of RDP on the choice of fairness groups `\cite{pmlr-v139-jalal21b}`{=latex} (as demonstrated in   
efsection:experiments), it still cannot guarantee fairness for any arbitrary group partitioning simultaneously (a property referred to as *obliviousness* in `\cite{pmlr-v139-jalal21b}`{=latex}). Second, our current theorems are preliminary, requiring further research to fully understand the nature of PF. For example, the severity of the tradeoff between the GPI scores of different groups (  
eftheorem:disjoint) and that of the tradeoff between PF and PI (  
efcorollary:pfi-pi-tradeoff) remain unclear. Third, we do not address the nature of optimal estimators that achieve good or perfect PF. What is their best possible distortion (*e.g*.., MSE) and best possible PI? Fourth, on the practical side, we show in   
efappendix:ablation-feature-extractors that effectively evaluating PF using metrics such as KID necessitates utilizing image features extracted from a classifier dedicated to handling the considered sensitive attributes (*e.g*.., an age and ethnicity classifier). However, this is not a disadvantage compared to previous fairness notions (RDP, CPR and PR), which also require such a classifier. Lastly, while the proposed GPI may be suitable for evaluating fairness in general-content natural images, we considered only human face images due to their societal implications, namely since fairness issues are particularly critical when dealing with such images. For example, if a general-content image restoration algorithm performs better on images with complex structures than on images of clear skies, this discrepancy is unlikely to be problematic for practitioners, as long as the algorithm attains good performance overall. Moreover, previous works `\citep{pmlr-v139-jalal21b}`{=latex} evaluated fairness with respect to non-human subjects (*e.g*.., dogs and cats), but these studies provide limited insights into human-related fairness issues, which often arise due to subtle differences between images (*e.g*.., wrinkles). Expanding our method to other datasets remains an avenue for future work.

# Societal impact [section:societal-impact]

Designing fair and unbiased image restoration algorithms is critical for various AI applications and downstream tasks that rely on them, such as facial recognition, image classification, and image editing. By proposing practically useful and well-justified fairness definitions, we can detect (and mitigate) bias in these tasks, ultimately leading to fairer societal outcomes. This fosters increased trust and adoption of AI technology, contributing to a more equitable and responsible use of AI in society.

# Acknowledgments [acknowledgments]

This research was partially supported by the Israel Science Foundation (ISF) under Grant 2318/22 and by the Council For Higher Education - Planning & Budgeting Committee.

# References [references]

<div class="thebibliography" markdown="1">

Theo Joseph Adrai, Guy Ohayon, Michael Elad, and Tomer Michaeli Deep optimal transport: A practical algorithm for photo-realistic image restoration In *Thirty-seventh Conference on Neural Information Processing Systems*, 2023. URL <https://openreview.net/forum?id=bJJY9TFfe0>. **Abstract:** We propose an image restoration algorithm that can control the perceptual quality and/or the mean square error (MSE) of any pre-trained model, trading one over the other at test time. Our algorithm is few-shot: Given about a dozen images restored by the model, it can significantly improve the perceptual quality and/or the MSE of the model for newly restored images without further training. Our approach is motivated by a recent theoretical result that links between the minimum MSE (MMSE) predictor and the predictor that minimizes the MSE under a perfect perceptual quality constraint. Specifically, it has been shown that the latter can be obtained by optimally transporting the output of the former, such that its distribution matches the source data. Thus, to improve the perceptual quality of a predictor that was originally trained to minimize MSE, we approximate the optimal transport by a linear transformation in the latent space of a variational auto-encoder, which we compute in closed-form using empirical means and covariances. Going beyond the theory, we find that applying the same procedure on models that were initially trained to achieve high perceptual quality, typically improves their perceptual quality even further. And by interpolating the results with the original output of the model, we can improve their MSE on the expense of perceptual quality. We illustrate our method on a variety of degradations applied to general content images of arbitrary dimensions. (@adrai2023deep)

Alekh Agarwal, Miroslav Dudik, and Zhiwei Steven Wu Fair regression: Quantitative definitions and reduction-based algorithms In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, *Proceedings of the 36th International Conference on Machine Learning*, volume 97 of *Proceedings of Machine Learning Research*, pages 120–129. PMLR, 09–15 Jun 2019. URL <https://proceedings.mlr.press/v97/agarwal19d.html>. **Abstract:** In this paper, we study the prediction of a real-valued target, such as a risk score or recidivism rate, while guaranteeing a quantitative notion of fairness with respect to a protected attribute such as gender or race. We call this class of problems \\}emph{fair regression}. We propose general schemes for fair regression under two notions of fairness: (1) statistical parity, which asks that the prediction be statistically independent of the protected attribute, and (2) bounded group loss, which asks that the prediction error restricted to any protected group remain below some pre-determined level. While we only study these two notions of fairness, our schemes are applicable to arbitrary Lipschitz-continuous losses, and so they encompass least-squares regression, logistic regression, quantile regression, and many other tasks. Our schemes only require access to standard risk minimization algorithms (such as standard classification or least-squares regression) while providing theoretical guarantees on the optimality and fairness of the obtained solutions. In addition to analyzing theoretical properties of our schemes, we empirically demonstrate their ability to uncover fairness–accuracy frontiers on several standard datasets. (@pmlr-v97-agarwal19d)

Namhyuk Ahn, Byungkon Kang, and Kyung-Ah Sohn Fast, accurate, and lightweight super-resolution with cascading residual network *arXiv*, 1803.08664, 2018. **Abstract:** In recent years, deep learning methods have been successfully applied to single-image super-resolution tasks. Despite their great performances, deep learning methods cannot be easily applied to real-world applications due to the requirement of heavy computation. In this paper, we address this issue by proposing an accurate and lightweight deep network for image super-resolution. In detail, we design an architecture that implements a cascading mechanism upon a residual network. We also present variant models of the proposed cascading residual network to further improve efficiency. Our extensive experiments show that even with much fewer parameters and operations, our models achieve performance comparable to that of state-of-the-art methods. (@ahn2018fast)

Arturs Backurs, Piotr Indyk, Krzysztof Onak, Baruch Schieber, Ali Vakilian, and Tal Wagner Scalable fair clustering In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, *Proceedings of the 36th International Conference on Machine Learning*, volume 97 of *Proceedings of Machine Learning Research*, pages 405–413. PMLR, 09–15 Jun 2019. URL <https://proceedings.mlr.press/v97/backurs19a.html>. **Abstract:** We study the fair variant of the classic $k$-median problem introduced by Chierichetti et al. \[2017\]. In the standard $k$-median problem, given an input pointset $P$, the goal is to find $k$ centers $C$ and assign each input point to one of the centers in $C$ such that the average distance of points to their cluster center is minimized. In the fair variant of $k$-median, the points are colored, and the goal is to minimize the same average distance objective while ensuring that all clusters have an equal number of points of each color. Chierichetti et al. proposed a two-phase algorithm for fair $k$-clustering. In the first step, the pointset is partitioned into subsets called fairlets that satisfy the fairness requirement and approximately preserve the $k$-median objective. In the second step, fairlets are merged into $k$ clusters by one of the existing $k$-median algorithms. The running time of this algorithm is dominated by the first step, which takes super-quadratic time. In this paper, we present a practical approximate fairlet decomposition algorithm that runs in nearly linear time. Our algorithm additionally allows for finer control over the balance of resulting clusters than the original work. We complement our theoretical bounds with empirical evaluation. (@pmlr-v97-backurs19a)

Suman Bera, Deeparnab Chakrabarty, Nicolas Flores, and Maryam Negahbani Fair algorithms for clustering In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, *Advances in Neural Information Processing Systems*, volume 32. Curran Associates, Inc., 2019. URL <https://proceedings.neurips.cc/paper_files/paper/2019/file/fc192b0c0d270dbf41870a63a8c76c2f-Paper.pdf>. **Abstract:** We study the problem of finding low-cost {\\}em fair clusterings} in data where each data point may belong to many protected groups. Our work significantly generalizes the seminal work of Chierichetti \\}etal (NIPS 2017) as follows. - We allow the user to specify the parameters that define fair representation. More precisely, these parameters define the maximum over- and minimum under-representation of any group in any cluster. - Our clustering algorithm works on any $\\}ell_p$-norm objective (e.g. $k$-means, $k$-median, and $k$-center). Indeed, our algorithm transforms any vanilla clustering solution into a fair one incurring only a slight loss in quality. - Our algorithm also allows individuals to lie in multiple protected groups. In other words, we do not need the protected groups to partition the data and we can maintain fairness across different groups simultaneously. Our experiments show that on established data sets, our algorithm performs much better in practice than what our theoretical results suggest. (@NEURIPS2019_fc192b0c)

Ioana O. Bercea, Martin Groß, Samir Khuller, Aounon Kumar, Clemens Rösner, Daniel R. Schmidt, and Melanie Schmidt On the cost of essentially fair clusterings *arXiv*, 1811.10319, 2018. **Abstract:** Clustering is a fundamental tool in data mining. It partitions points into groups (clusters) and may be used to make decisions for each point based on its group. However, this process may harm protected (minority) classes if the clustering algorithm does not adequately represent them in desirable clusters – especially if the data is already biased. At NIPS 2017, Chierichetti et al. proposed a model for fair clustering requiring the representation in each cluster to (approximately) preserve the global fraction of each protected class. Restricting to two protected classes, they developed both a 4-approximation for the fair $k$-center problem and a $O(t)$-approximation for the fair $k$-median problem, where $t$ is a parameter for the fairness model. For multiple protected classes, the best known result is a 14-approximation for fair $k$-center. We extend and improve the known results. Firstly, we give a 5-approximation for the fair $k$-center problem with multiple protected classes. Secondly, we propose a relaxed fairness notion under which we can give bicriteria constant-factor approximations for all of the classical clustering objectives $k$-center, $k$-supplier, $k$-median, $k$-means and facility location. The latter approximations are achieved by a framework that takes an arbitrary existing unfair (integral) solution and a fair (fractional) LP solution and combines them into an essentially fair clustering with a weakly supervised rounding scheme. In this way, a fair clustering can be established belatedly, in a situation where the centers are already fixed. (@bercea2018cost)

Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgenstern, Seth Neel, and Aaron Roth A convex framework for fair regression *arXiv*, 1706.02409, 2017. **Abstract:** We introduce a flexible family of fairness regularizers for (linear and logistic) regression problems. These regularizers all enjoy convexity, permitting fast optimization, and they span the rang from notions of group fairness to strong individual fairness. By varying the weight on the fairness regularizer, we can compute the efficient frontier of the accuracy-fairness trade-off on any given dataset, and we measure the severity of this trade-off via a numerical quantity we call the Price of Fairness (PoF). The centerpiece of our results is an extensive comparative study of the PoF across six different datasets in which fairness is a primary consideration. (@berk2017convex)

Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth Fairness in criminal justice risk assessments: The state of the art *arXiv*, 1703.09207, 2017. **Abstract:** Objectives: Discussions of fairness in criminal justice risk assessments typically lack conceptual precision. Rhetoric too often substitutes for careful analysis. In this paper, we seek to clarify the tradeoffs between different kinds of fairness and between fairness and accuracy. Methods: We draw on the existing literatures in criminology, computer science and statistics to provide an integrated examination of fairness and accuracy in criminal justice risk assessments. We also provide an empirical illustration using data from arraignments. Results: We show that there are at least six kinds of fairness, some of which are incompatible with one another and with accuracy. Conclusions: Except in trivial cases, it is impossible to maximize accuracy and fairness at the same time, and impossible simultaneously to satisfy all kinds of fairness. In practice, a major complication is different base rates across different legally protected groups. There is a need to consider challenging tradeoffs. (@berk2017fairness)

Mikołaj Bińkowski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton Demystifying MMD GANs In *International Conference on Learning Representations*, 2018. URL <https://openreview.net/forum?id=r1lUOzWCW>. **Abstract:** We investigate the training and performance of generative adversarial networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs. As our main theoretical contribution, we clarify the situation with bias in GAN loss functions raised by recent work: we show that gradient estimators used in the optimization process for both MMD GANs and Wasserstein GANs are unbiased, but learning a discriminator based on samples leads to biased gradients for the generator parameters. We also discuss the issue of kernel choice for the MMD critic, and characterize the kernel corresponding to the energy distance used for the Cramer GAN critic. Being an integral probability metric, the MMD benefits from training strategies recently developed for Wasserstein GANs. In experiments, the MMD GAN is able to employ a smaller critic network than the Wasserstein GAN, resulting in a simpler and faster-training algorithm with matching performance. We also propose an improved measure of GAN convergence, the Kernel Inception Distance, and show how to use it to dynamically adapt learning rates during GAN training. (@bińkowski2018demystifying)

Yochai Blau and Tomer Michaeli The perception-distortion tradeoff In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2018. **Abstract:** Image restoration algorithms are typically evaluated by some distortion measure (e.g. PSNR, SSIM, IFC, VIF) or by human opinion scores that quantify perceived perceptual quality. In this paper, we prove mathematically that distortion and perceptual quality are at odds with each other. Specifically, we study the optimal probability for correctly discriminating the outputs of an image restoration algorithm from real images. We show that as the mean distortion decreases, this probability must increase (indicating worse perceptual quality). As opposed to the common belief, this result holds true for any distortion measure, and is not only a problem of the PSNR or SSIM criteria. We also show that generative-adversarial-nets (GANs) provide a principled way to approach the perception-distortion bound. This constitutes theoretical support to their observed success in low-level vision tasks. Based on our analysis, we propose a new methodology for evaluating image restoration methods, and use it to perform an extensive comparison between recent super-resolution algorithms. (@Blau2018)

Joy Buolamwini and Timnit Gebru Gender shades: Intersectional accuracy disparities in commercial gender classification In Sorelle A. Friedler and Christo Wilson, editors, *Proceedings of the 1st Conference on Fairness, Accountability and Transparency*, volume 81 of *Proceedings of Machine Learning Research*, pages 77–91. PMLR, 23–24 Feb 2018. URL <https://proceedings.mlr.press/v81/buolamwini18a.html>. **Abstract:** Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis al- gorithms and datasets with respect to phe- notypic subgroups. Using the dermatolo- gist approved Fitzpatrick Skin Type clas- si cation system, we characterize the gen- der and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We nd that these datasets are overwhelm- ingly composed of lighter-skinned subjects (79 :6% for IJB-A and 86 :2% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender clas- si cation systems using our dataset and show that darker-skinned females are the most misclassi ed group (with error rates of up to 34 :7%). The maximum error rate for lighter-skinned males is 0 :8%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classi cation systems require urgent atten- tion if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms. (@pmlr-v81-buolamwini18a)

Toon Calders, Asim Karim, Faisal Kamiran, Wasif Ali, and Xiangliang Zhang Controlling attribute effect in linear regression In *2013 IEEE 13th International Conference on Data Mining*, pages 71–80, 2013. . **Abstract:** In data mining we often have to learn from biased data, because, for instance, data comes from different batches or there was a gender or racial bias in the collection of social data. In some applications it may be necessary to explicitly control this bias in the models we learn from the data. This paper is the first to study learning linear regression models under constraints that control the biasing effect of a given attribute such as gender or batch number. We show how propensity modeling can be used for factoring out the part of the bias that can be justified by externally provided explanatory attributes. Then we analytically derive linear models that minimize squared error while controlling the bias by imposing constraints on the mean outcome or residuals of the models. Experiments with discrimination-aware crime prediction and batch effect normalization tasks show that the proposed techniques are successful in controlling attribute effects in linear regression models. (@6729491)

Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii Fair clustering through fairlets In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, *Advances in Neural Information Processing Systems*, volume 30. Curran Associates, Inc., 2017. URL <https://proceedings.neurips.cc/paper_files/paper/2017/file/978fce5bcc4eccc88ad48ce3914124a2-Paper.pdf>. **Abstract:** We study the question of fair clustering under the {\\}em disparate impact} doctrine, where each protected class must have approximately equal representation in every cluster. We formulate the fair clustering problem under both the k-center and the k-median objectives, and show that even with two protected classes the problem is challenging, as the optimum solution can violate common conventions—for instance a point may no longer be assigned to its nearest cluster center! En route we introduce the concept of fairlets, which are minimal sets that satisfy fair representation while approximately preserving the clustering objective. We show that any fair clustering problem can be decomposed into first finding good fairlets, and then using existing machinery for traditional clustering algorithms. While finding good fairlets can be NP-hard, we proceed to obtain efficient approximation algorithms based on minimum cost flow. We empirically demonstrate the \\}emph{price of fairness} by quantifying the value of fair clustering on real-world datasets with sensitive attributes. (@NIPS2017_978fce5b)

Jun-Ho Choi, Huan Zhang, Jun-Hyuk Kim, Cho-Jui Hsieh, and Jong-Seok Lee Evaluating robustness of deep image super-resolution against adversarial attacks In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, October 2019. **Abstract:** Single-image super-resolution aims to generate a high-resolution version of a low-resolution image, which serves as an essential component in many image processing applications. This paper investigates the robustness of deep learning-based super-resolution methods against adversarial attacks, which can significantly deteriorate the super-resolved images without noticeable distortion in the attacked low-resolution images. It is demonstrated that state-of-the-art deep super-resolution methods are highly vulnerable to adversarial attacks. Different levels of robustness of different methods are analyzed theoretically and experimentally. We also present analysis on transferability of attacks, and feasibility of targeted attacks and universal attacks. (@Choi_2019_ICCV)

Yujin Choi, Jinseong Park, Hoki Kim, Jaewook Lee, and Saerom Park Fair sampling in diffusion models through switching mechanism In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan, editors, *Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada*, pages 21995–22003. AAAI Press, 2024. . URL <https://doi.org/10.1609/aaai.v38i20.30202>. **Abstract:** Diffusion models have shown their effectiveness in generation tasks by well-approximating the underlying probability distribution. However, diffusion models are known to suffer from an amplified inherent bias from the training data in terms of fairness. While the sampling process of diffusion models can be controlled by conditional guidance, previous works have attempted to find empirical guidance to achieve quantitative fairness. To address this limitation, we propose a fairness-aware sampling method called \\}textit{attribute switching} mechanism for diffusion models. Without additional training, the proposed sampling can obfuscate sensitive attributes in generated data without relying on classifiers. We mathematically prove and experimentally demonstrate the effectiveness of the proposed method on two key aspects: (i) the generation of fair data and (ii) the preservation of the utility of the generated data. (@DBLP:conf/aaai/Choi0K0P24)

Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye Diffusion posterior sampling for general noisy inverse problems In *The Eleventh International Conference on Learning Representations*, 2023. URL <https://openreview.net/forum?id=OnD9zGAGT0k>. **Abstract:** Diffusion models have been recently studied as powerful generative inverse problem solvers, owing to their high quality reconstructions and the ease of combining existing iterative solvers. However, most works focus on solving simple linear inverse problems in noiseless settings, which significantly under-represents the complexity of real-world problems. In this work, we extend diffusion solvers to efficiently handle general noisy (non)linear inverse problems via approximation of the posterior sampling. Interestingly, the resulting posterior sampling scheme is a blended version of diffusion sampling with the manifold constrained gradient without a strict measurement consistency projection step, yielding a more desirable generative path in noisy settings compared to the previous studies. Our method demonstrates that diffusion models can incorporate various measurement noise statistics such as Gaussian and Poisson, and also efficiently handle noisy nonlinear inverse problems such as Fourier phase retrieval and non-uniform deblurring. Code available at https://github.com/DPS2022/diffusion-posterior-sampling (@chung2023diffusion)

Sam Corbett-Davies, Johann D. Gaebler, Hamed Nilforoshan, Ravi Shroff, and Sharad Goel The measure and mismeasure of fairness *arXiv*, 1808.00023, 2023. **Abstract:** The field of fair machine learning aims to ensure that decisions guided by algorithms are equitable. Over the last decade, several formal, mathematical definitions of fairness have gained prominence. Here we first assemble and categorize these definitions into two broad families: (1) those that constrain the effects of decisions on disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions. We then show, analytically and empirically, that both families of definitions typically result in strongly Pareto dominated decision policies. For example, in the case of college admissions, adhering to popular formal conceptions of fairness would simultaneously result in lower student-body diversity and a less academically prepared class, relative to what one could achieve by explicitly tailoring admissions policies to achieve desired outcomes. In this sense, requiring that these fairness definitions hold can, perversely, harm the very groups they were designed to protect. In contrast to axiomatic notions of fairness, we argue that the equitable design of algorithms requires grappling with their context-specific consequences, akin to the equitable design of policy. We conclude by listing several open challenges in fair machine learning and offering strategies to ensure algorithms are better aligned with policy goals. (@corbettdavies2023measure)

Ryan Dahl, Mohammad Norouzi, and Jonathon Shlens Pixel recursive super resolution In *Proceedings of the IEEE International Conference on Computer Vision (ICCV)*, Oct 2017. **Abstract:** Super resolution is the problem of artificially enlarging a low resolution photograph to recover a plausible high resolution version. In the regime of high magnification factors, the problem is dramatically underspecified and many plausible, high resolution images may match a given low resolution image. In particular, traditional super resolution techniques fail in this regime due to the multimodality of the problem and strong prior information that must be imposed on image synthesis to produce plausible high resolution images. In this work we propose a new probabilistic deep network architecture, a pixel recursive super resolution model, that is an extension of PixelCNNs to address this problem. We demonstrate that this model produces a diversity of plausible high resolution images at large magnification factors. Furthermore, in human evaluation studies we demonstrate how previous methods fail to fool human observers. However, high resolution images sampled from this probabilistic deep network do fool a naive human observer a significant fraction of the time. (@Dahl_2017_ICCV)

Mauricio Delbracio and Peyman Milanfar Inversion by direct iteration: An alternative to denoising diffusion for image restoration *Transactions on Machine Learning Research*, 2023. ISSN 2835-8856. URL <https://openreview.net/forum?id=VmyFF5lL3F>. Featured Certification. **Abstract:** Inversion by Direct Iteration (InDI) is a new formulation for supervised image restoration that avoids the so-called "regression to the mean" effect and produces more realistic and detailed images than existing regression-based methods. It does this by gradually improving image quality in small steps, similar to generative denoising diffusion models. Image restoration is an ill-posed problem where multiple high-quality images are plausible reconstructions of a given low-quality input. Therefore, the outcome of a single step regression model is typically an aggregate of all possible explanations, therefore lacking details and realism. The main advantage of InDI is that it does not try to predict the clean target image in a single step but instead gradually improves the image in small steps, resulting in better perceptual quality. While generative denoising diffusion models also work in small steps, our formulation is distinct in that it does not require knowledge of any analytic form of the degradation process. Instead, we directly learn an iterative restoration process from low-quality and high-quality paired examples. InDI can be applied to virtually any image degradation, given paired training data. In conditional denoising diffusion image restoration the denoising network generates the restored image by repeatedly denoising an initial image of pure noise, conditioned on the degraded input. Contrary to conditional denoising formulations, InDI directly proceeds by iteratively restoring the input low-quality image, producing high-quality results on a variety of image restoration tasks, including motion and out-of-focus deblurring, super-resolution, compression artifact removal, and denoising. (@delbracio2023inversion)

Emily L Denton, Soumith Chintala, arthur szlam, and Rob Fergus Deep generative image models using a laplacian pyramid of adversarial networks In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, *Advances in Neural Information Processing Systems*, volume 28. Curran Associates, Inc., 2015. URL <https://proceedings.neurips.cc/paper_files/paper/2015/file/aa169b49b583a2b5af89203c2b78c67c-Paper.pdf>. **Abstract:** In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach (Goodfellow et al.). Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset. (@NIPS2015_aa169b49)

Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang Image super-resolution using deep convolutional networks , 2014. **Abstract:** We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality. (@dong2014image)

Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel Fairness through awareness In *Proceedings of the 3rd Innovations in Theoretical Computer Science Conference*, ITCS ’12, page 214–226, New York, NY, USA, 2012. Association for Computing Machinery. ISBN 9781450311151. . URL <https://doi.org/10.1145/2090236.2090255>. **Abstract:** We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness. (@fairness-awareness)

Dror Freirich, Tomer Michaeli, and Ron Meir A theory of the distortion-perception tradeoff in wasserstein space In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, *Advances in Neural Information Processing Systems*, volume 34, pages 25661–25672. Curran Associates, Inc., 2021. URL <https://proceedings.neurips.cc/paper_files/paper/2021/file/d77e68596c15c53c2a33ad143739902d-Paper.pdf>. **Abstract:** The lower the distortion of an estimator, the more the distribution of its outputs generally deviates from the distribution of the signals it attempts to estimate. This phenomenon, known as the perception-distortion tradeoff, has captured significant attention in image restoration, where it implies that fidelity to ground truth images comes at the expense of perceptual quality (deviation from statistics of natural images). However, despite the increasing popularity of performing comparisons on the perception-distortion plane, there remains an important open question: what is the minimal distortion that can be achieved under a given perception constraint? In this paper, we derive a closed form expression for this distortion-perception (DP) function for the mean squared-error (MSE) distortion and the Wasserstein-2 perception index. We prove that the DP function is always quadratic, regardless of the underlying distribution. This stems from the fact that estimators on the DP curve form a geodesic in Wasserstein space. In the Gaussian setting, we further provide a closed form expression for such estimators. For general distributions, we show how these estimators can be constructed from the estimators at the two extremes of the tradeoff: The global MSE minimizer, and a minimizer of the MSE under a perfect perceptual quality constraint. The latter can be obtained as a stochastic transformation of the former. (@dror)

Felix Friedrich, Manuel Brack, Lukas Struppek, Dominik Hintersdorf, Patrick Schramowski, Sasha Luccioni, and Kristian Kersting Fair diffusion: Instructing text-to-image generation models on fairness *arXiv*, 2302.10893, 2023. **Abstract:** Generative AI models have recently achieved astonishing results in quality and are consequently employed in a fast-growing number of applications. However, since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer from degenerated and biased human behavior, as we demonstrate. In fact, they may even reinforce such biases. To not only uncover but also combat these undesired effects, we present a novel strategy, called Fair Diffusion, to attenuate biases after the deployment of generative text-to-image models. Specifically, we demonstrate shifting a bias, based on human instructions, in any direction yielding arbitrarily new proportions for, e.g., identity groups. As our empirical evaluation demonstrates, this introduced control enables instructing generative image models on fairness, with no data filtering and additional training required. (@friedrich2023FairDiffusion)

Yingqiang Ge, Shuchang Liu, Ruoyuan Gao, Yikun Xian, Yunqi Li, Xiangyu Zhao, Changhua Pei, Fei Sun, Junfeng Ge, Wenwu Ou, and Yongfeng Zhang Towards long-term fairness in recommendation In *Proceedings of the 14th ACM International Conference on Web Search and Data Mining*, WSDM ’21, page 445–453, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450382977. . URL <https://doi.org/10.1145/3437963.3441824>. **Abstract:** As Recommender Systems (RS) influence more and more people in their daily life, the issue of fairness in recommendation is becoming more and more important. Most of the prior approaches to fairness-aware recommendation have been situated in a static or one-shot setting, where the protected groups of items are fixed, and the model provides a one-time fairness solution based on fairness-constrained optimization. This fails to consider the dynamic nature of the recommender systems, where attributes such as item popularity may change over time due to the recommendation policy and user engagement. For example, products that were once popular may become no longer popular, and vice versa. As a result, the system that aims to maintain long-term fairness on the item exposure in different popularity groups must accommodate this change in a timely fashion. (@10.1145/3437963.3441824)

Sahin Cem Geyik, Stuart Ambler, and Krishnaram Kenthapadi Fairness-aware ranking in search & recommendation systems with application to linkedin talent search *Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining*, 2019. URL <https://api.semanticscholar.org/CorpusID:146121159>. **Abstract:** We present a framework for quantifying and mitigating algorithmic bias in mechanisms designed for ranking individuals, typically used as part of web-scale search and recommendation systems. We first propose complementary measures to quantify bias with respect to protected attributes such as gender and age. We then present algorithms for computing fairness-aware re-ranking of results. For a given search or recommendation task, our algorithms seek to achieve a desired distribution of top ranked results with respect to one or more protected attributes. We show that such a framework can be tailored to achieve fairness criteria such as equality of opportunity and demographic parity depending on the choice of the desired distribution. We evaluate the proposed algorithms via extensive simulations over different parameter choices, and study the effect of fairness-aware ranking on both bias and utility measures. We finally present the online A/B testing results from applying our framework towards representative ranking in LinkedIn Talent Search, and discuss the lessons learned in practice. Our approach resulted in tremendous improvement in the fairness metrics (nearly three fold increase in the number of search queries with representative results) without affecting the business metrics, which paved the way for deployment to 100% of LinkedIn Recruiter users worldwide. Ours is the first large-scale deployed framework for ensuring fairness in the hiring domain, with the potential positive impact for more than 630M LinkedIn members. (@Geyik2019FairnessAwareRI)

Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, and Ming-Ming Cheng Vqfr: Blind face restoration with vector-quantized dictionary and parallel decoder In *ECCV*, 2022. **Abstract:** Although generative facial prior and geometric prior have recently demonstrated high-quality results for blind face restoration, producing fine-grained facial details faithful to inputs remains a challenging problem. Motivated by the classical dictionary-based methods and the recent vector quantization (VQ) tech- nique, we propose a VQ-based face restoration method – VQFR. VQFR takes ad- vantage of high-quality low-level feature banks extracted from high-quality faces and can thus help recover realistic facial details. However, the simple application of the VQ codebook cannot achieve good results with faithful details and identity preservation. Therefore, we further introduce two special network designs. 1). We first investigate the compression patch size in the VQ codebook and find that the VQ codebook designed with a proper compression patch size is crucial to balance the quality and fidelity. 2). To further fuse low-level features from inputs while not “contaminating” the realistic details generated from the VQ codebook, we proposed a parallel decoder consisting of a texture decoder and a main decoder. Those two decoders then interact with a texture warping module with deformable convolution. Equipped with the VQ codebook as a facial detail dictionary and the parallel decoder design, the proposed VQFR can largely enhance the restored quality of facial details while keeping the fidelity to previous methods. (@gu2022vqfr)

Sergio Guadarrama, Ryan Dahl, David Bieber, Jonathon Shlens, Mohammad Norouzi, and Kevin Murphy Pixcolor: Pixel recursive colorization In *British Machine Vision Conference 2017, BMVC 2017, London, UK, September 4-7, 2017*. BMVA Press, 2017. URL <https://www.dropbox.com/s/wmnk861irndf8xe/0447.pdf>. **Abstract:** We propose a novel approach to automatically produce multiple colorized versions of a grayscale image. Our method results from the observation that the task of automated colorization is relatively easy given a low-resolution version of the color image. We first train a conditional PixelCNN to generate a low resolution color for a given grayscale image. Then, given the generated low-resolution color image and the original grayscale image as inputs, we train a second CNN to generate a high-resolution colorization of an image. We demonstrate that our approach produces more diverse and plausible colorizations than existing methods, as judged by human raters in a Visual Turing Test. (@DBLP:conf/bmvc/GuadarramaDBS0017)

Moritz Hardt, Eric Price, Eric Price, and Nati Srebro Equality of opportunity in supervised learning In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, *Advances in Neural Information Processing Systems*, volume 29. Curran Associates, Inc., 2016. URL <https://proceedings.neurips.cc/paper_files/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf>. **Abstract:** We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. In line with other studies, our notion is oblivious: it depends only on the joint statistics of the predictor, the target and the protected attribute, but not on interpretation of individualfeatures. We study the inherent limits of defining and identifying biases based on such oblivious measures, outlining what can and cannot be inferred from different oblivious tests. We illustrate our notion using a case study of FICO credit scores. (@NIPS2016_9d268236)

Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter Gans trained by a two time-scale update rule converge to a local nash equilibrium In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, *Advances in Neural Information Processing Systems*, volume 30. Curran Associates, Inc., 2017. URL <https://proceedings.neurips.cc/paper_files/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf>. **Abstract:** Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr\\}’echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark. (@fid)

Marco Huber, Anh Thi Luu, Fadi Boutros, Arjan Kuijper, and Naser Damer Bias and diversity in synthetic-based face recognition In *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)*, pages 6215–6226, January 2024. **Abstract:** Synthetic data is emerging as a substitute for authentic data to solve ethical and legal challenges in handling authentic face data. The current models can create real-looking face images of people who do not exist. However, it is a known and sensitive problem that face recognition systems are susceptible to bias, i.e. performance differences between different demographic and non-demographics attributes, which can lead to unfair decisions. In this work, we investigate how the diversity of synthetic face recognition datasets compares to authentic datasets, and how the distribution of the training data of the generative models affects the distribution of the synthetic data. To do this, we looked at the distribution of gender, ethnicity, age, and head position. Furthermore, we investigated the concrete bias of three recent synthetic-based face recognition models on the studied attributes in comparison to a baseline model trained on authentic data. Our results show that the generator generate a similar distribution as the used training data in terms of the different attributes. With regard to bias, it can be seen that the synthetic-based models share a similar bias behavior with the authentic-based models. However, with the uncovered lower intra-identity attribute consistency seems to be beneficial in reducing bias. (@Huber_2024_WACV)

Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa *ACM Transactions on Graphics (Proc. of SIGGRAPH 2016)*, 35 (4), 2016. (@IizukaSIGGRAPH2016)

Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros Image-to-image translation with conditional adversarial networks *CVPR*, 2017. **Abstract:** We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pi×2pi× software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either. (@pix2pix2017)

Ajil Jalal, Sushrut Karmalkar, Jessica Hoffmann, Alex Dimakis, and Eric Price Fairness for image generation with uncertain sensitive attributes In Marina Meila and Tong Zhang, editors, *Proceedings of the 38th International Conference on Machine Learning*, volume 139 of *Proceedings of Machine Learning Research*, pages 4721–4732. PMLR, 18–24 Jul 2021. URL <https://proceedings.mlr.press/v139/jalal21b.html>. **Abstract:** This work tackles the issue of fairness in the context of generative procedures, such as image super-resolution, which entail different definitions from the standard classification setting. Moreover, while traditional group fairness definitions are typically defined with respect to specified protected groups – camouflaging the fact that these groupings are artificial and carry historical and political motivations – we emphasize that there are no ground truth identities. For instance, should South and East Asians be viewed as a single group or separate groups? Should we consider one race as a whole or further split by gender? Choosing which groups are valid and who belongs in them is an impossible dilemma and being "fair" with respect to Asians may require being "unfair" with respect to South Asians. This motivates the introduction of definitions that allow algorithms to be \\}emph{oblivious} to the relevant groupings. We define several intuitive notions of group fairness and study their incompatibilities and trade-offs. We show that the natural extension of demographic parity is strongly dependent on the grouping, and \\}emph{impossible} to achieve obliviously. On the other hand, the conceptually new definition we introduce, Conditional Proportional Representation, can be achieved obliviously through Posterior Sampling. Our experiments validate our theoretical results and achieve fair image reconstruction using state-of-the-art generative models. (@pmlr-v139-jalal21b)

Kimmo Karkkainen and Jungseock Joo Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation In *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision*, pages 1548–1558, 2021. **Abstract:** Existing public face image datasets are strongly biased toward Caucasian faces, and other races (e.g., Latino) are significantly underrepresented. The models trained from such datasets suffer from inconsistent classification accuracy, which limits the applicability of face analytic systems to non-White race groups. To mitigate the race bias problem in these datasets, we constructed a novel face image dataset containing 108,501 images which is balanced on race. We define 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Images were collected from the YFCC-100M Flickr dataset and labeled with race, gender, and age groups. Evaluations were performed on existing face attribute datasets as well as novel image datasets to measure the generalization performance. We find that the model trained from our dataset is substantially more accurate on novel datasets and the accuracy is consistent across race and gender groups. We also compare several commercial computer vision APIs and report their balanced accuracy across gender, race, and age groups. Our code, data, and models are available at https://github.com/joojs/fairface. (@karkkainenfairface)

Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen Progressive growing of GANs for improved quality, stability, and variation In *International Conference on Learning Representations*, 2018. URL <https://openreview.net/forum?id=Hk99zCeAb>. **Abstract:** We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024\^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset. (@karras2018progressive)

Tero Karras, Samuli Laine, and Timo Aila A style-based generator architecture for generative adversarial networks In *2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 4396–4405, 2019. . **Abstract:** We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces. (@ffhq)

Sergey Kastryulin, Dzhamil Zakirov, and Denis Prokopenko : Metrics and measure for image quality assessment 2019. URL <https://github.com/photosynthesis-team/piq>. Open-source software available at https://github.com/photosynthesis-team/piq. **Abstract:** Image Quality Assessment (IQA) metrics are widely used to quantitatively estimate the extent of image degradation following some forming, restoring, transforming, or enhancing algorithms. We present PyTorch Image Quality (PIQ), a usability-centric library that contains the most popular modern IQA algorithms, guaranteed to be correctly implemented according to their original propositions and thoroughly verified. In this paper, we detail the principles behind the foundation of the library, describe the evaluation strategy that makes it reliable, provide the benchmarks that showcase the performance-time trade-offs, and underline the benefits of GPU acceleration given the library is used within the PyTorch backend. PyTorch Image Quality is an open source software: https://github.com/photosynthesis-team/piq/. (@piq)

Sergey Kastryulin, Jamil Zakirov, Denis Prokopenko, and Dmitry V. Dylov Pytorch image quality: Metrics for image quality assessment . . URL <https://arxiv.org/abs/2208.14818>. **Abstract:** Image Quality Assessment (IQA) metrics are widely used to quantitatively estimate the extent of image degradation following some forming, restoring, transforming, or enhancing algorithms. We present PyTorch Image Quality (PIQ), a usability-centric library that contains the most popular modern IQA algorithms, guaranteed to be correctly implemented according to their original propositions and thoroughly verified. In this paper, we detail the principles behind the foundation of the library, describe the evaluation strategy that makes it reliable, provide the benchmarks that showcase the performance-time trade-offs, and underline the benefits of GPU acceleration given the library is used within the PyTorch backend. PyTorch Image Quality is an open source software: https://github.com/photosynthesis-team/piq/. (@kastryulin2022piq)

Bahjat Kawar, Gregory Vaksman, and Michael Elad Snips: Solving noisy inverse problems stochastically In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, *Advances in Neural Information Processing Systems*, volume 34, pages 21757–21769. Curran Associates, Inc., 2021. URL <https://proceedings.neurips.cc/paper_files/paper/2021/file/b5c01503041b70d41d80e3dbe31bbd8c-Paper.pdf>. **Abstract:** In this work we introduce a novel stochastic algorithm dubbed SNIPS, which draws samples from the posterior distribution of any linear inverse problem, where the observation is assumed to be contaminated by additive white Gaussian noise. Our solution incorporates ideas from Langevin dynamics and Newton’s method, and exploits a pre-trained minimum mean squared error (MMSE) Gaussian denoiser. The proposed approach relies on an intricate derivation of the posterior score function that includes a singular value decomposition (SVD) of the degradation operator, in order to obtain a tractable iterative algorithm for the desired sampling. Due to its stochasticity, the algorithm can produce multiple high perceptual quality samples for the same noisy observation. We demonstrate the abilities of the proposed paradigm for image deblurring, super-resolution, and compressive sensing. We show that the samples produced are sharp, detailed and consistent with the given measurements, and their diversity exposes the inherent uncertainty in the inverse problem being solved. (@NEURIPS2021_b5c01503)

Bahjat Kawar, Gregory Vaksman, and Michael Elad Stochastic image denoising by sampling from the posterior distribution In *2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)*, pages 1866–1875, 2021. . **Abstract:** Image denoising is a well-known and well studied problem, commonly targeting a minimization of the mean squared error (MSE) between the outcome and the original image. Unfortunately, especially for severe noise levels, such Minimum MSE (MMSE) solutions may lead to blurry output images. In this work we propose a novel stochastic denoising approach that produces viable and high perceptual quality results, while maintaining a small MSE. Our method employs Langevin dynamics that relies on a repeated application of any given MMSE denoiser, obtaining the reconstructed image by effectively sampling from the posterior distribution. Due to its stochasticity, the proposed algorithm can produce a variety of high-quality outputs for a given noisy input, all shown to be legitimate denoising results. In addition, we present an extension of our algorithm for handling the inpainting problem, recovering missing pixels while removing noise from partially given data. (@kawar-posterior)

Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song Denoising diffusion restoration models In *Advances in Neural Information Processing Systems*, 2022. **Abstract:** Many interesting tasks in image restoration can be cast as linear inverse problems. A recent family of approaches for solving these problems uses stochastic algorithms that sample from the posterior distribution of natural images given the measurements. However, efficient solutions often require problem-specific supervised training to model the posterior, whereas unsupervised methods that are not problem-specific typically rely on inefficient iterative methods. This work addresses these issues by introducing Denoising Diffusion Restoration Models (DDRM), an efficient, unsupervised posterior sampling method. Motivated by variational inference, DDRM takes advantage of a pre-trained denoising diffusion generative model for solving any linear inverse problem. We demonstrate DDRM’s versatility on several image datasets for super-resolution, deblurring, inpainting, and colorization under various amounts of measurement noise. DDRM outperforms the current leading unsupervised methods on the diverse ImageNet dataset in reconstruction quality, perceptual quality, and runtime, being 5x faster than the nearest competitor. DDRM also generalizes well for natural images out of the distribution of the observed ImageNet training set. (@kawar2022denoising)

Junpei Komiyama, Akiko Takeda, Junya Honda, and Hajime Shimao Nonconvex optimization for regression with fairness constraints In Jennifer Dy and Andreas Krause, editors, *Proceedings of the 35th International Conference on Machine Learning*, volume 80 of *Proceedings of Machine Learning Research*, pages 2737–2746. PMLR, 10–15 Jul 2018. URL <https://proceedings.mlr.press/v80/komiyama18a.html>. **Abstract:** The unfairness of a regressor is evaluated by mea- suring the correlation between the estimator and the sensitive attribute (e.g., race, gender, age), and the coefﬁcient of determination (CoD) is a natural extension of the correlation coefﬁcient when more than one sensitive attribute exists. As is well known, there is a trade-off between fair- ness and accuracy of a regressor, which implies that a perfectly fair optimizer does not always yield a useful prediction. Taking this into con- sideration, we optimize the accuracy of the es- timation subject to a user-deﬁned level of fair- ness. However, a fairness level as a constraint induces a nonconvexity of the feasible region, which disables the use of an off-the-shelf convex optimizer. Despite such nonconvexity, we show that an exact solution is available by using tools of global optimization theory. Unlike most of ex- isting fairness-aware machine learning methods, our method allows us to deal with numeric and multiple sensitive attributes. (@pmlr-v80-komiyama18a)

Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva Counterfactual fairness In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, *Advances in Neural Information Processing Systems*, volume 30. Curran Associates, Inc., 2017. URL <https://proceedings.neurips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf>. **Abstract:** Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it is the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school. (@NIPS2017_a486cd07)

Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila Improved precision and recall metric for assessing generative models In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, *Advances in Neural Information Processing Systems*, volume 32. Curran Associates, Inc., 2019. URL <https://proceedings.neurips.cc/paper_files/paper/2019/file/0234c510bc6d908b28c70ff313743079-Paper.pdf>. **Abstract:** The ability to automatically estimate the quality and coverage of the samples produced by a generative model is a vital requirement for driving algorithm research. We present an evaluation metric that can separately and reliably measure both of these aspects in image generation tasks by forming explicit, non-parametric representations of the manifolds of real and generated data. We demonstrate the effectiveness of our metric in StyleGAN and BigGAN by providing several illustrative examples where existing metrics yield uninformative or contradictory results. Furthermore, we analyze multiple design variants of StyleGAN to better understand the relationships between the model architecture, training methods, and the properties of the resulting sample distribution. In the process, we identify new variants that improve the state-of-the-art. We also perform the first principled analysis of truncation methods and identify an improved method. Finally, we extend our metric to estimate the perceptual quality of individual samples, and use this to study latent space interpolations. (@NEURIPS2019_0234c510)

Yunqi Li, Hanxiong Chen, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang User-oriented fairness in recommendation In *Proceedings of the Web Conference 2021*, WWW ’21, page 624–632, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383127. . URL <https://doi.org/10.1145/3442381.3449866>. **Abstract:** As a highly data-driven application, recommender systems could be affected by data bias, resulting in unfair results for different data groups, which could be a reason that affects the system performance. Therefore, it is important to identify and solve the unfairness issues in recommendation scenarios. (@10.1145/3442381.3449866)

Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte Swinir: Image restoration using swin transformer In *2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)*, pages 1833–1844, 2021. . **Abstract:** Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by up to 0.14∼0.45dB, while the total number of parameters can be reduced by up to 67%. (@liang2021swinir)

Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee Enhanced deep residual networks for single image super-resolution In *2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)*, pages 1132–1140, 2017. . **Abstract:** Recent research on super-resolution has progressed with the development of deep convolutional neural networks (DCNN). In particular, residual learning techniques exhibit improved performance. In this paper, we develop an enhanced deep super-resolution network (EDSR) with performance exceeding those of current state-of-the-art SR methods. The significant performance improvement of our model is due to optimization by removing unnecessary modules in conventional residual networks. The performance is further improved by expanding the model size while we stabilize the training procedure. We also propose a new multi-scale deep super-resolution system (MDSR) and training method, which can reconstruct high-resolution images of different upscaling factors in a single model. The proposed methods show superior performance over the state-of-the-art methods on benchmark datasets and prove its excellence by winning the NTIRE2017 Super-Resolution Challenge\[26\]. (@edsr)

Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Bo Dai, Fanghua Yu, Wanli Ouyang, Yu Qiao, and Chao Dong Diffbir: Towards blind image restoration with generative diffusion prior *arXiv*, 2308.15070, 2024. **Abstract:** We present DiffBIR, a general restoration pipeline that could handle different blind image restoration tasks in a unified framework. DiffBIR decouples blind image restoration problem into two stages: 1) degradation removal: removing image-independent content; 2) information regeneration: generating the lost image content. Each stage is developed independently but they work seamlessly in a cascaded manner. In the first stage, we use restoration modules to remove degradations and obtain high-fidelity restored results. For the second stage, we propose IRControlNet that leverages the generative ability of latent diffusion models to generate realistic details. Specifically, IRControlNet is trained based on specially produced condition images without distracting noisy content for stable generation performance. Moreover, we design a region-adaptive restoration guidance that can modify the denoising process during inference without model re-training, allowing users to balance realness and fidelity through a tunable guidance scale. Extensive experiments have demonstrated DiffBIR’s superiority over state-of-the-art approaches for blind image super-resolution, blind face restoration and blind image denoising tasks on both synthetic and real-world datasets. The code is available at https://github.com/XPixelGroup/DiffBIR. (@2023diffbir)

Vongani H. Maluleke, Neerja Thakkar, Tim Brooks, Ethan Weber, Trevor Darrell, Alexei A. Efros, Angjoo Kanazawa, and Devin Guillory Studying bias in gans through the lens of race In *Computer Vision – ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XIII*, page 344–360, Berlin, Heidelberg, 2022. Springer-Verlag. ISBN 978-3-031-19777-2. . URL <https://doi.org/10.1007/978-3-031-19778-9_20>. **Abstract:** In this work, we study how the performance and evaluation of generative image models are impacted by the racial composition of their training datasets. By examining and controlling the racial distributions in various training datasets, we are able to observe the impacts of different training distributions on generated image quality and the racial distributions of the generated images. Our results show that the racial compositions of generated images successfully preserve that of the training data. However, we observe that truncation, a technique used to generate higher quality images during inference, exacerbates racial imbalances in the data. Lastly, when examining the relationship between image quality and race, we find that the highest perceived visual quality images of a given race come from a distribution where that race is well-represented, and that annotators consistently prefer generated images of white people over those of Black people. (@bias-race-ffhq)

Sean Man, Guy Ohayon, Theo Adrai, and Michael Elad High-perceptual quality jpeg decoding via posterior sampling In *2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)*, pages 1272–1282, 2023. . **Abstract:** JPEG is arguably the most popular image coding format, achieving high compression ratios via lossy quantization that may create visual artifacts degradation. Numerous attempts to remove these artifacts were conceived over the years, and common to most of these is the use of deterministic post-processing algorithms that optimize some distortion measure (e.g., PSNR, SSIM). In this paper we propose a different paradigm for JPEG artifact correction: Our method is stochastic, and the objective we target is high perceptual quality – striving to obtain sharp, detailed and visually pleasing reconstructed images, while being consistent with the compressed input. These goals are achieved by training a stochastic conditional generator (conditioned on the compressed input), accompanied by a theoretically well-founded loss term, resulting in a sampler from the posterior distribution. Our solution offers a diverse set of plausible and fast reconstructions for a given input with perfect consistency. We demonstrate our scheme’s unique properties and its superiority to a variety of alternative methods on the FFHQ and ImageNet datasets. (@sean-jpeg)

Rishabh Mehrotra, James McInerney, Hugues Bouchard, Mounia Lalmas, and Fernando Diaz Towards a fair marketplace: Counterfactual evaluation of the trade-off between relevance, fairness & satisfaction in recommendation systems In *Proceedings of the 27th ACM International Conference on Information and Knowledge Management*, CIKM ’18, page 2243–2251, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450360142. . URL <https://doi.org/10.1145/3269206.3272027>. **Abstract:** Two-sided marketplaces are platforms that have customers not only on the demand side (e.g. users), but also on the supply side (e.g. retailer, artists). While traditional recommender systems focused specifically towards increasing consumer satisfaction by providing relevant content to consumers, two-sided marketplaces face the problem of additionally optimizing for supplier preferences, and visibility. Indeed, the suppliers would want afair opportunity to be presented to users. Blindly optimizing for consumer relevance may have a detrimental impact on supplier fairness. Motivated by this problem, we focus on the trade-off between objectives of consumers and suppliers in the case of music streaming services, and consider the trade-off betweenrelevance of recommendations to the consumer (i.e. user) andfairness of representation of suppliers (i.e. artists) and measure their impact on consumersatisfaction. We propose a conceptual and computational framework using counterfactual estimation techniques to understand, and evaluate different recommendation policies, specifically around the trade-off between relevance and fairness, without the need for running many costly A/B tests. We propose a number of recommendation policies which jointly optimize relevance and fairness, thereby achieving substantial improvement in supplier fairness without noticeable decline in user satisfaction. Additionally, we consider user disposition towards fair content, and propose a personalized recommendation policy which takes into account consumer’s tolerance towards fair content. Our findings could guide the design of algorithms powering two-sided marketplaces, as well as guide future research on sophisticated algorithms for joint optimization of user relevance, satisfaction and fairness. (@10.1145/3269206.3272027)

Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon dit: Guided image synthesis and editing with stochastic differential equations In *International Conference on Learning Representations*, 2022. **Abstract:** Guided image synthesis enables everyday users to create and edit photo-realistic images with minimum effort. The key challenge is balancing faithfulness to the user input (e.g., hand-drawn colored strokes) and realism of the synthesized image. Existing GAN-based methods attempt to achieve such balance using either conditional GANs or GAN inversions, which are challenging and often require additional training data or loss functions for individual applications. To address these issues, we introduce a new image synthesis and editing method, Stochastic Differential Editing (SDEdit), based on a diffusion model generative prior, which synthesizes realistic images by iteratively denoising through a stochastic differential equation (SDE). Given an input image with user guide of any type, SDEdit first adds noise to the input, then subsequently denoises the resulting image through the SDE prior to increase its realism. SDEdit does not require task-specific training or inversions and can naturally achieve the balance between realism and faithfulness. SDEdit significantly outperforms state-of-the-art GAN-based methods by up to 98.09% on realism and 91.72% on overall satisfaction scores, according to a human perception study, on multiple tasks, including stroke-based image synthesis and editing as well as image compositing. (@meng2022sdedit)

Anish Mittal, Anush K. Moorthy, and Alan C. Bovik Blind/referenceless image spatial quality evaluator In *2011 Conference Record of the Forty Fifth Asilomar Conference on Signals, Systems and Computers (ASILOMAR)*, pages 723–727, 2011. . **Abstract:** We propose a natural scene statistic based Blind/Referenceless Image Spatial QUality Evaluator (BRISQUE) which extracts the point wise statistics of local normalized luminance signals and measures image naturalness (or lack there of) based on measured deviations from a natural image model. We also model the distribution of pairwise statistics of adjacent normalized luminance signals which provides distortion orientation information. Although multi scale, the model uses easy to compute features making it computationally fast and time efficient. The frame work is shown to perform statistically better than other proposed no reference algorithms and full reference structural similarity index (SSIM). (@6190099)

Anish Mittal, Rajiv Soundararajan, and Alan C. Bovik Making a “completely blind” image quality analyzer *IEEE Signal Processing Letters*, 20 (3): 209–212, 2013. . **Abstract:** An important aim of research on the blind image quality assessment (IQA) problem is to devise perceptual models that can predict the quality of distorted images with as little prior knowledge of the images or their distortions as possible. Current state-of-the-art "general purpose" no reference (NR) IQA algorithms require knowledge about anticipated distortions in the form of training examples and corresponding human opinion scores. However we have recently derived a blind IQA model that only makes use of measurable deviations from statistical regularities observed in natural images, without training on human-rated distorted images, and, indeed without any exposure to distorted images. Thus, it is "completely blind." The new IQA model, which we call the Natural Image Quality Evaluator (NIQE) is based on the construction of a "quality aware" collection of statistical features based on a simple and successful space domain natural scene statistic (NSS) model. These features are derived from a corpus of natural, undistorted images. Experimental results show that the new index delivers performance comparable to top performing NR IQA models that require training on large databases of human opinions of distorted images. A software release is available at http://live.ece.utexas.edu/research/quality/niqe_release.zip. (@6353522)

Nate Raw vit-age-classifier (revision 461a4c4) . . URL <https://huggingface.co/nateraw/vit-age-classifier>. **Abstract:** We’re on a journey to advance and democratize artificial intelligence through open source and open science. (@nate_raw_2023)

Anton Obukhov, Maximilian Seitzer, Po-Wei Wu, Semen Zhydenko, Jonathan Kyl, and Elvis Yu-Jing Lin High-fidelity performance metrics for generative models in pytorch 2020. URL <https://github.com/toshas/torch-fidelity>. Version: 0.3.0, DOI: 10.5281/zenodo.4957738. **Abstract:** Evaluating the performance of generative models in image synthesis is a challenging task. Although the Fr\\}’echet Inception Distance is a widely accepted evaluation metric, it integrates different aspects (e.g., fidelity and diversity) of synthesized images into a single score and assumes the normality of embedded vectors. Recent methods such as precision-and-recall and its variants such as density-and-coverage have been developed to separate fidelity and diversity based on k-nearest neighborhood methods. In this study, we propose an algorithm named barcode, which is inspired by the topological data analysis and is almost free of assumption and hyperparameter selections. In extensive experiments on real-world datasets as well as theoretical approach on high-dimensional normal samples, it was found that the ’usual’ normality assumption of embedded vectors has several drawbacks. The experimental results demonstrate that barcode outperforms other methods in evaluating fidelity and diversity of GAN outputs. Official codes can be found in https://github.com/minjeekim00/Barcode. (@obukhov2020torchfidelity)

Guy Ohayon, Theo Adrai, Gregory Vaksman, Michael Elad, and Peyman Milanfar High perceptual quality image denoising with a posterior sampling cgan In *2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)*, pages 1805–1813, 2021. . **Abstract:** The vast work in Deep Learning (DL) has led to a leap in image denoising research. Most DL solutions for this task have chosen to put their efforts on the denoiser’s architecture while maximizing distortion performance. However, distortion driven solutions lead to blurry results with sub-optimal perceptual quality, especially in immoderate noise levels. In this paper we propose a different perspective, aiming to produce sharp and visually pleasing denoised images that are still faithful to their clean sources. Formally, our goal is to achieve high perceptual quality with acceptable distortion. This is attained by a stochastic denoiser that samples from the posterior distribution, trained as a generator in the framework of conditional generative adversarial networks (CGAN). Contrary to distortion-based regularization terms that conflict with perceptual quality, we introduce to the CGAN objective a theoretically founded penalty term that does not force a distortion requirement on individual samples, but rather on their mean. We showcase our proposed method with a novel denoiser architecture that achieves the reformed denoising goal and produces vivid and diverse outcomes in immoderate noise levels. (@ohayon-posterior)

Guy Ohayon, Theo Joseph Adrai, Michael Elad, and Tomer Michaeli Reasons for the superiority of stochastic estimators over deterministic ones: Robustness, consistency and perceptual quality In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, *Proceedings of the 40th International Conference on Machine Learning*, volume 202 of *Proceedings of Machine Learning Research*, pages 26474–26494. PMLR, 23–29 Jul 2023. URL <https://proceedings.mlr.press/v202/ohayon23a.html>. **Abstract:** Stochastic restoration algorithms allow to explore the space of solutions that correspond to the degraded input. In this paper we reveal additional fundamental advantages of stochastic methods over deterministic ones, which further motivate their use. First, we prove that any restoration algorithm that attains perfect perceptual quality and whose outputs are consistent with the input must be a posterior sampler, and is thus required to be stochastic. Second, we illustrate that while deterministic restoration algorithms may attain high perceptual quality, this can be achieved only by filling up the space of all possible source images using an extremely sensitive mapping, which makes them highly vulnerable to adversarial attacks. Indeed, we show that enforcing deterministic models to be robust to such attacks profoundly hinders their perceptual quality, while robustifying stochastic models hardly influences their perceptual quality, and improves their output variability. These findings provide a motivation to foster progress in stochastic restoration methods, paving the way to better recovery algorithms. (@pmlr-v202-ohayon23a)

Guy Ohayon, Tomer Michaeli, and Michael Elad The perception-robustness tradeoff in deterministic image restoration *arXiv*, 2311.09253, 2024. **Abstract:** We study the behavior of deterministic methods for solving inverse problems in imaging. These methods are commonly designed to achieve two goals: (1) attaining high perceptual quality, and (2) generating reconstructions that are consistent with the measurements. We provide a rigorous proof that the better a predictor satisfies these two requirements, the larger its Lipschitz constant must be, regardless of the nature of the degradation involved. In particular, to approach perfect perceptual quality and perfect consistency, the Lipschitz constant of the model must grow to infinity. This implies that such methods are necessarily more susceptible to adversarial attacks. We demonstrate our theory on single image super-resolution algorithms, addressing both noisy and noiseless settings. We also show how this undesired behavior can be leveraged to explore the posterior distribution, thereby allowing the deterministic model to imitate stochastic methods. (@ohayon2023perceptionrobustness)

Guy Ohayon, Tomer Michaeli, and Michael Elad Posterior-mean rectified flow: Towards minimum mse photo-realistic image restoration *arXiv preprint arXiv:2410.00418*, 2024. URL <https://arxiv.org/abs/2410.00418>. **Abstract:** Photo-realistic image restoration algorithms are typically evaluated by distortion measures (e.g., PSNR, SSIM) and by perceptual quality measures (e.g., FID, NIQE), where the desire is to attain the lowest possible distortion without compromising on perceptual quality. To achieve this goal, current methods commonly attempt to sample from the posterior distribution, or to optimize a weighted sum of a distortion loss (e.g., MSE) and a perceptual quality loss (e.g., GAN). Unlike previous works, this paper is concerned specifically with the optimal estimator that minimizes the MSE under a constraint of perfect perceptual index, namely where the distribution of the reconstructed images is equal to that of the ground-truth ones. A recent theoretical result shows that such an estimator can be constructed by optimally transporting the posterior mean prediction (MMSE estimate) to the distribution of the ground-truth images. Inspired by this result, we introduce Posterior-Mean Rectified Flow (PMRF), a simple yet highly effective algorithm that approximates this optimal estimator. In particular, PMRF first predicts the posterior mean, and then transports the result to a high-quality image using a rectified flow model that approximates the desired optimal transport map. We investigate the theoretical utility of PMRF and demonstrate that it consistently outperforms previous methods on a variety of image restoration tasks. (@ohayon2024pmrf)

Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski v2: Learning robust visual features without supervision *Transactions on Machine Learning Research*, 2024. ISSN 2835-8856. URL <https://openreview.net/forum?id=a68SUt6zFt>. **Abstract:** The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels. (@oquab2024dinov)

Roy Or-El, Soumyadip Sengupta, Ohad Fried, Eli Shechtman, and Ira Kemelmacher-Shlizerman Lifespan age transformation synthesis In *Proceedings of the European Conference on Computer Vision (ECCV)*, 2020. **Abstract:** We address the problem of single photo age progression and regression\|the prediction of how a person might look in the future, or how they looked in the past. Most existing aging methods are limited to changing the texture, overlooking transformations in head shape that occur during the human aging and growth process. This limits the appli- cability of previous methods to aging of adults to slightly older adults, and application of those methods to photos of children does not produce quality results. We propose a novel multi-domain image-to-image genera- tive adversarial network architecture, whose learned latent space models a continuous bi-directional aging process. The network is trained on the FFHQ dataset, which we labeled for ages, gender, and semantic segmen- tation. Fixed age classes are used as anchors to approximate continuous age transformation. Our framework can predict a full head portrait for ages 0{70 from a single photo, modifying both texture and shape of the head. We demonstrate results on a wide variety of photos and datasets, and show signi cant improvement over the state of the art. 1 Introduction Age transformation is a problem of synthesizing a person’s appearance in a di erent age while preserving their identity. Once the age gap between the input and the desired output is signi cant, e.g., going from 1 to 15 year old, the problem becomes highly challenging due to pronounced changes in head shape as well as facial texture. Solving for shape and texture together remains an open problem. Particularly, if the method is required to create a lifespan of transformations, i.e., for any given input age, the method should synthesize a full span of 0{70 ages (rather than binary young-to-old transformations). In this paper, we aim to enable exactly that\|lifespan of transformations from a single portrait. State of the art methods \[43,1,48,31,46,44,13\] focus on either minor age gaps, or mostly on adults to elderly progression, as a large part of the aging transfor- mation for adults lies in the texture (rather than shape), e.g., adding wrinkles. The method of Kemelmacher-Shlizerman et al. \[20\] allows substantial age trans- formations but it can be applied only on a cropped face area, rather than a full head, and cannot be modi ed to allow backward age prediction (adult to child) due to optical- ow-based nature of the method. Apps like FaceApp allow considerable transitions from adult to child and vice versa, but similar to state arXiv:2003.09764 (@orel2020lifespan)

Emanuel Parzen On estimation of a probability density function and mode *The Annals of Mathematical Statistics*, 33 (3): 1065–1076, 1962. ISSN 00034851. URL <http://www.jstor.org/stable/2237880>. **Abstract:** Abstract : Given a sequence of independent identically distributed random variables with a common probability density function, the problem of the estimation of a probability density function and of determining the mode of a probability function are discussed. Only estimates which are consistent and asymptotically normal are constructed. (Author) (@kde)

Adrián Pérez-Suay, Valero Laparra, Gonzalo Mateo-García, Jordi Muñoz-Marí, Luis Gómez-Chova, and Gustau Camps-Valls Fair kernel learning In Michelangelo Ceci, Jaakko Hollmén, Ljupčo Todorovski, Celine Vens, and Sašo Džeroski, editors, *Machine Learning and Knowledge Discovery in Databases*, pages 339–355, Cham, 2017. Springer International Publishing. ISBN 978-3-319-71249-9. **Abstract:** New social and economic activities massively exploit big data and machine learning algorithms to do inference on people’s lives. Applications include automatic curricula evaluation, wage determination, and risk assessment for credits and loans. Recently, many governments and institutions have raised concerns about the lack of fairness, equity and ethics in machine learning to treat these problems. It has been shown that not including sensitive features that bias fairness, such as gender or race, is not enough to mitigate the discrimination when other related features are included. Instead, including fairness in the objective function has been shown to be more efficient. We present novel fair regression and dimensionality reduction methods built on a previously proposed fair classification framework. Both methods rely on using the Hilbert Schmidt independence criterion as the fairness term. Unlike previous approaches, this allows us to simplify the problem and to use multiple sensitive variables simultaneously. Replacing the linear formulation by kernel functions allows the methods to deal with nonlinear problems. For both linear and nonlinear formulations the solution reduces to solving simple matrix inversions or generalized eigenvalue problems. This simplifies the evaluation of the solutions for different trade-off values between the predictive error and fairness terms. We illustrate the usefulness of the proposed methods in toy examples, and evaluate their performance on real world datasets to predict income using gender and/or race discrimination as sensitive variables, and contraceptive method prediction under demographic and socio-economic sensitive descriptors. (@10.1007/978-3-319-71249-9_21)

Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger On fairness and calibration In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, *Advances in Neural Information Processing Systems*, volume 30. Curran Associates, Inc., 2017. URL <https://proceedings.neurips.cc/paper_files/paper/2017/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Paper.pdf>. **Abstract:** The machine learning community has become increasingly concerned with the potential for bias and discrimination in predictive models. This has motivated a growing line of work on what it means for a classification procedure to be fair. In this paper, we investigate the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates. We show that calibration is compatible only with a single error constraint (i.e. equal false-negatives rates across groups), and show that any algorithm that satisfies this relaxation is no better than randomizing a percentage of predictions for an existing classifier. These unsettling findings, which extend and generalize existing results, are empirically confirmed on several datasets. (@NIPS2017_b8b9c74a)

Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach : Improving latent diffusion models for high-resolution image synthesis In *The Twelfth International Conference on Learning Representations*, 2024. URL <https://openreview.net/forum?id=di52zR8xgf>. **Abstract:** We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at https://github.com/Stability-AI/generative-models (@podell2024sdxl)

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever Learning transferable visual models from natural language supervision In Marina Meila and Tong Zhang, editors, *Proceedings of the 38th International Conference on Machine Learning*, volume 139 of *Proceedings of Machine Learning Research*, pages 8748–8763. PMLR, 18–24 Jul 2021. URL <https://proceedings.mlr.press/v139/radford21a.html>. **Abstract:** State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP. (@pmlr-v139-radford21a)

Ethan Rudd, Manuel Gunther, and Terrance Boult Moon: A mixed objective optimization network for the recognition of facial attributes *arXiv*, 1603.07027, 2016. **Abstract:** Attribute recognition, particularly facial, extracts many labels for each image. While some multi-task vision problems can be decomposed into separate tasks and stages, e.g., training independent models for each task, for a growing set of problems joint optimization across all tasks has been shown to improve performance. We show that for deep convolutional neural network (DCNN) facial attribute extraction, multi-task optimization is better. Unfortunately, it can be difficult to apply joint optimization to DCNNs when training data is imbalanced, and re-balancing multi-label data directly is structurally infeasible, since adding/removing data to balance one label will change the sampling of the other labels. This paper addresses the multi-label imbalance problem by introducing a novel mixed objective optimization network (MOON) with a loss function that mixes multiple task objectives with domain adaptive re-weighting of propagated loss. Experiments demonstrate that not only does MOON advance the state of the art in facial attribute recognition, but it also outperforms independently trained DCNNs using the same data. When using facial attributes for the LFW face recognition task, we show that our balanced (domain adapted) network outperforms the unbalanced trained network. (@rudd2016moon)

Clemens Rösner and Melanie Schmidt Privacy preserving clustering with constraints *arXiv*, 1802.02497, 2018. **Abstract:** The $k$-center problem is a classical combinatorial optimization problem which asks to find $k$ centers such that the maximum distance of any input point in a set $P$ to its assigned center is minimized. The problem allows for elegant $2$-approximations. However, the situation becomes significantly more difficult when constraints are added to the problem. We raise the question whether general methods can be derived to turn an approximation algorithm for a clustering problem with some constraints into an approximation algorithm that respects one constraint more. Our constraint of choice is privacy: Here, we are asked to only open a center when at least $\\}ell$ clients will be assigned to it. We show how to combine privacy with several other constraints. (@rosner2018privacy)

Mehdi S. M. Sajjadi, Olivier Bachem, Mario Lučić, Olivier Bousquet, and Sylvain Gelly In *Advances in Neural Information Processing Systems (NeurIPS)*, 2018. **Abstract:** Recent advances in generative modeling have led to an increased interest in the study of statistical divergences as means of model comparison. Commonly used evaluation methods, such as the Frechet Inception Distance (FID), correlate well with the perceived quality of samples and are sensitive to mode dropping. However, these metrics are unable to distinguish between different failure cases since they only yield one-dimensional scores. We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions. The proposed notion is intuitive, retains desirable properties, and naturally leads to an efficient algorithm that can be used to evaluate generative models. We relate this notion to total variation as well as to recent evaluation metrics such as Inception Score and FID. To demonstrate the practical utility of the proposed approach we perform an empirical study on several variants of Generative Adversarial Networks and Variational Autoencoders. In an extensive set of experiments we show that the proposed metric is able to disentangle the quality of generated samples from the coverage of the target distribution. (@precision_recall_distributions)

Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen Improved techniques for training gans In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, *Advances in Neural Information Processing Systems*, volume 29. Curran Associates, Inc., 2016. URL <https://proceedings.neurips.cc/paper_files/paper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf>. **Abstract:** We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes. (@NIPS2016_8a3363ab)

Melanie Schmidt, Chris Schwiegelshohn, and Christian Sohler Fair coresets and streaming algorithms for fair k-means clustering *arXiv*, 1812.10854, 2021. **Abstract:** We study fair clustering problems as proposed by Chierichetti et al. (NIPS 2017). Here, points have a sensitive attribute and all clusters in the solution are required to be balanced with respect to it (to counteract any form of data-inherent bias). Previous algorithms for fair clustering do not scale well. We show how to model and compute so-called coresets for fair clustering problems, which can be used to significantly reduce the input data size. We prove that the coresets are composable and show how to compute them in a streaming setting. Furthermore, we propose a variant of Lloyd’s algorithm that computes fair clusterings and extend it to a fair k-means++ clustering algorithm. We implement these algorithms and provide empirical evidence that the combination of our approximation algorithms and the coreset construction yields a scalable algorithm for fair k-means clustering. (@schmidt2021fair)

Ashish Seth, Mayur Hemani, and Chirag Agarwal Dear: Debiasing vision-language models with additive residuals In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 6820–6829, June 2023. **Abstract:** Large pre-trained vision-language models (VLMs) reduce the time for developing predictive models for various vision-grounded language downstream tasks by providing rich, adaptable image and text representations. However, these models suffer from societal biases owing to the skewed distribution of various identity groups in the training data. These biases manifest as the skewed similarity between the representations for specific text concepts and images of people of different identity groups and, therefore, limit the usefulness of such models in real-world high-stakes applications. In this work, we present Dear(Debiasing with Additive Residuals), a novel debiasing method that learns additive residual image representations to offset the original representations, ensuring fair output representations. In doing so, it reduces the ability of the representations to distinguish between the different identity groups. Further, we observe that the current fairness tests are performed on limited face image datasets that fail to indicate why a specific text concept should/should not apply to them. To bridge this gap and better evaluate Dear,we introduce the Protected Attribute Tag Association (pata)dataset - a new context-based bias benchmarking dataset for evaluating the fairness of large pre-trained VLMs. Additionally, Pataprovides visual context for a diverse human population in different scenarios with both positive and negative connotations. Experimental results for fairness and zero-shot performance preservation using multiple datasets demonstrate the efficacy of our framework. The dataset is released here. (@Seth_2023_CVPR)

Xudong Shen, Chao Du, Tianyu Pang, Min Lin, Yongkang Wong, and Mohan Kankanhalli Finetuning text-to-image diffusion models for fairness In *The Twelfth International Conference on Learning Representations*, 2024. URL <https://openreview.net/forum?id=hnrB5YHoYu>. **Abstract:** The rapid adoption of text-to-image diffusion models in society underscores an urgent need to address their biases. Without interventions, these biases could propagate a skewed worldview and restrict opportunities for minority groups. In this work, we frame fairness as a distributional alignment problem. Our solution consists of two main technical contributions: (1) a distributional alignment loss that steers specific characteristics of the generated images towards a user-defined target distribution, and (2) adjusted direct finetuning of diffusion model’s sampling process (adjusted DFT), which leverages an adjusted gradient to directly optimize losses defined on the generated images. Empirically, our method markedly reduces gender, racial, and their intersectional biases for occupational prompts. Gender bias is significantly reduced even when finetuning just five soft tokens. Crucially, our method supports diverse perspectives of fairness beyond absolute equality, which is demonstrated by controlling age to a $75\\}%$ young and $25\\}%$ old distribution while simultaneously debiasing gender and race. Finally, our method is scalable: it can debias multiple concepts at once by simply including these prompts in the finetuning data. We share code and various fair diffusion model adaptors at https://sail-sg.github.io/finetune-fair-diffusion/. (@shen2024finetuning)

Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz Pseudoinverse-guided diffusion models for inverse problems In *International Conference on Learning Representations*, 2023. URL <https://openreview.net/forum?id=9_gsMA8MRKQ>. **Abstract:** We introduce pseudoinverse guidance, an approach to solve inverse problems with generative diffusion models. (@song2023pseudoinverseguided)

StabilityAI stabilityai/stable-diffusion-xl-refiner-1.0 (revision 5d4cfe8) . URL <https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0>. **Abstract:** We’re on a journey to advance and democratize artificial intelligence through open source and open science. (@sdxl-image-to-image)

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna Rethinking the inception architecture for computer vision In *2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 2818–2826, 2016. . **Abstract:** Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set. (@inception)

Hossein Talebi and Peyman Milanfar Nima: Neural image assessment *IEEE Transactions on Image Processing*, 27 (8): 3998–4011, 2018. . **Abstract:** Automatically learned quality assessment for images has recently become a hot topic due to its usefulness in a wide variety of applications such as evaluating image capture pipelines, storage techniques and sharing media. Despite the subjective nature of this problem, most existing methods only predict the mean opinion score provided by datasets such as AVA \[1\] and TID2013 \[2\]. Our approach differs from others in that we predict the distribution of human opinion scores using a convolutional neural network. Our architecture also has the advantage of being significantly simpler than other methods with comparable performance. Our proposed approach relies on the success (and retraining) of proven, state-of-the-art deep object recognition networks. Our resulting network can be used to not only score images reliably and with high correlation to human perception, but also to assist with adaptation and optimization of photo editing/enhancement algorithms in a photographic pipeline. All this is done without need for a "golden" reference image, consequently allowing for single-image, semantic- and perceptually-aware, no-reference quality assessment. (@8352823)

Hossein Talebi and Peyman Milanfar Learned perceptual image enhancement In *2018 IEEE International Conference on Computational Photography (ICCP)*, pages 1–13, 2018. . **Abstract:** Learning a typical image enhancement pipeline involves minimization of a loss function between enhanced and reference images. While L \<sub xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"\>1\</sub\> and L \<sub xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"\>2\</sub\> losses are perhaps the most widely used functions for this purpose, they do not necessarily lead to perceptually compelling results. In this paper, we show that adding a learned no-reference image quality metric to the loss can significantly improve enhancement operators. This metric is implemented using a CNN (convolutional neural network) trained on a large-scale dataset labelled with aesthetic preferences ofhuman raters. This loss allows us to conveniently perform back-propagation in our learning framework to simultaneously optimize for similarity to a given ground truth reference and perceptual quality. This perceptual loss is only used to train parameters of image processing operators, and does not impose any extra complexity at inference time. Our experiments demonstrate that this loss can be effective for tuning a variety of operators such as local tone mapping and dehazing. (@8368474)

Sahil Verma and Julia Rubin Fairness definitions explained In *Proceedings of the International Workshop on Software Fairness*, FairWare ’18, page 1–7, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450357463. . URL <https://doi.org/10.1145/3194770.3194776>. **Abstract:** Algorithm fairness has started to attract the attention of researchers in AI, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others. (@10.1145/3194770.3194776)

Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, İlhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors *Nature Methods*, 17: 261–272, 2020. . **Abstract:** SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments. This Perspective describes the development and capabilities of SciPy 1.0, an open source scientific computing library for the Python programming language. (@2020SciPy-NMeth)

Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy Esrgan: Enhanced super-resolution generative adversarial networks In *The European Conference on Computer Vision Workshops (ECCVW)*, September 2018. **Abstract:** The Super-Resolution Generative Adversarial Network (SR- GAN) \[1\] is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN { network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particu- lar, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN \[2\] to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could pro- vide stronger supervision for brightness consistency and texture recovery. Bene ting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the rst place in the PIRM2018-SR Challenge1\[3\]. The code is available at https://github.com/xinntao/ESRGAN . 1 Introduction Single image super-resolution (SISR), as a fundamental low-level vision prob- lem, has attracted increasing attention in the research community and AI com- panies. SISR aims at recovering a high-resolution (HR) image from a single low-resolution (LR) one. Since the pioneer work of SRCNN proposed by Dong et al. \[4\], deep convolution neural network (CNN) approaches have brought pros- perous development. Various network architecture designs and training strategies have continuously improved the SR performance, especially the Peak Signal-to- Noise Ratio (PSNR) value \[5,6,7,1,8,9,10,11,12\]. However, these PSNR-oriented approaches tend to output over-smoothed results without sucient high-frequency details, since the PSNR metric fundamentally disagrees with the subjective eval- uation of human observers \[1\]. 1We won the rst place in region 3 and got the best perceptual index.arXiv:1809.00219v2 \[cs.CV\] 17 Sep 20182 Xintao Wang et al. SRGAN ESRGAN Ground Truth Fig. 1: The super-resolution results of 4 for SRGAN2, the proposed ESRGAN and the ground-truth. ESRGAN outperforms SRGAN in sharpness and details. Several perceptual-driven methods have been proposed to improve the visual quality of SR results. For instance, perceptual loss \[13,14\] is proposed to opti- mize super-resolution m (@wang2018esrgan)

Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan Towards real-world blind face restoration with generative facial prior In *The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2021. **Abstract:** Blind face restoration usually relies on facial priors, such as facial geometry prior or reference prior, to restore realistic and faithful details. However, very low-quality inputs cannot offer accurate geometric prior while high-quality references are inaccessible, limiting the applicability in real-world scenarios. In this work, we propose GFP-GAN that leverages rich and diverse priors encapsulated in a pretrained face GAN for blind face restoration. This Generative Facial Prior (GFP) is incorporated into the face restoration process via spatial feature transform layers, which allow our method to achieve a good balance of realness and fidelity. Thanks to the powerful generative facial prior and delicate designs, our GFP-GAN could jointly restore facial details and enhance colors with just a single forward pass, while GAN inversion methods require image-specific optimization at inference. Extensive experiments show that our method achieves superior performance to prior art on both synthetic and real-world datasets. (@wang2021gfpgan)

Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan Real-esrgan: Training real-world blind super-resolution with pure synthetic data In *International Conference on Computer Vision Workshops (ICCVW)*, 2021. **Abstract:** Though many attempts have been made in blind super-resolution to restore low-resolution images with unknown and complex degradations, they are still far from addressing general real-world degraded images. In this work, we extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data. Specifically, a high-order degradation modeling process is introduced to better simulate complex real-world degradations. We also consider the common ringing and overshoot artifacts in the synthesis process. In addition, we employ a U-Net discriminator with spectral normalization to increase discriminator capability and stabilize the training dynamics. Extensive comparisons have shown its superior visual performance than prior works on various real datasets. We also provide efficient implementations to synthesize training pairs on the fly. (@wang2021realesrgan)

Yinhuai Wang, Jiwen Yu, and Jian Zhang Zero-shot image restoration using denoising diffusion null-space model *The Eleventh International Conference on Learning Representations*, 2023. **Abstract:** Most existing Image Restoration (IR) models are task-specific, which can not be generalized to different degradation operators. In this work, we propose the Denoising Diffusion Null-Space Model (DDNM), a novel zero-shot framework for arbitrary linear IR problems, including but not limited to image super-resolution, colorization, inpainting, compressed sensing, and deblurring. DDNM only needs a pre-trained off-the-shelf diffusion model as the generative prior, without any extra training or network modifications. By refining only the null-space contents during the reverse diffusion process, we can yield diverse results satisfying both data consistency and realness. We further propose an enhanced and robust version, dubbed DDNM+, to support noisy restoration and improve restoration quality for hard tasks. Our experiments on several IR tasks reveal that DDNM outperforms other state-of-the-art zero-shot IR methods. We also demonstrate that DDNM+ can solve complex real-world applications, e.g., old photo restoration. (@wang2022zero)

Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli Image quality assessment: from error visibility to structural similarity *IEEE Transactions on Image Processing*, 13 (4): 600–612, 2004. . **Abstract:** Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/. (@ssim)

Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, and Ping Luo Restoreformer: High-quality blind face restoration from undegraded key-value pairs . **Abstract:** Blind face restoration is to recover a high-quality face image from unknown degradations. As face image contains abundant contextual information, we propose a method, RestoreFormer, which explores fully-spatial attentions to model contextual information and surpasses existing works that use local operators. RestoreFormer has several benefits compared to prior arts. First, unlike the conventional multi-head self-attention in previous Vision Transformers (ViTs), RestoreFormer incorporates a multi-head cross-attention layer to learn fully-spatial interactions between corrupted queries and high-quality key-value pairs. Second, the key-value pairs in ResotreFormer are sampled from a reconstruction-oriented high-quality dictionary, whose elements are rich in high-quality facial features specifically aimed for face reconstruction, leading to superior restoration results. Third, RestoreFormer outperforms advanced state-of-the-art methods on one synthetic dataset and three real-world datasets, as well as produces images with better visual quality. Code is available at https://github.com/wzhouxiff/RestoreFormer.git. (@wang2022restoreformer)

Zhouxia Wang, Jiawei Zhang, Tianshui Chen, Wenping Wang, and Ping Luo Restoreformer++: Towards real-world blind face restoration from undegraded key-value paris . **Abstract:** Blind face restoration aims at recovering high-quality face images from those with unknown degradations. Current algorithms mainly introduce priors to complement high-quality details and achieve impressive progress. However, most of these algorithms ignore abundant contextual information in the face and its interplay with the priors, leading to sub-optimal performance. Moreover, they pay less attention to the gap between the synthetic and real-world scenarios, limiting the robustness and generalization to real-world applications. In this work, we propose RestoreFormer++, which on the one hand introduces fully-spatial attention mechanisms to model the contextual information and the interplay with the priors, and on the other hand, explores an extending degrading model to help generate more realistic degraded face images to alleviate the synthetic-to-real-world gap. Compared with current algorithms, RestoreFormer++ has several crucial benefits. First, instead of using a multi-head self-attention mechanism like the traditional visual transformer, we introduce multi-head cross-attention over multi-scale features to fully explore spatial interactions between corrupted information and high-quality priors. In this way, it can facilitate RestoreFormer++ to restore face images with higher realness and fidelity. Second, in contrast to the recognition-oriented dictionary, we learn a reconstruction-oriented dictionary as priors, which contains more diverse high-quality facial details and better accords with the restoration target. Third, we introduce an extending degrading model that contains more realistic degraded scenarios for training data synthesizing, and thus helps to enhance the robustness and generalization of our RestoreFormer++ model. Extensive experiments show that RestoreFormer++ outperforms state-of-the-art algorithms on both synthetic and real-world datasets. (@wang2023restoreformer++)

Michael L. Waskom seaborn: statistical data visualization *Journal of Open Source Software*, 6 (60): 3021, 2021. . URL <https://doi.org/10.21105/joss.03021>. **Abstract:** seaborn is a library for making statistical graphics in Python.It provides a high-level interface to matplotlib and integrates closely with pandas data structures.Functions in the seaborn library expose a declarative, dataset-oriented API that makes it easy to translate questions about data into graphics that can answer them.When given a dataset and a specification of the plot to make, seaborn automatically maps the data values to visual attributes such as color, size, or style, internally computes statistical transformations, and decorates the plot with informative axis labels and a legend.Many seaborn functions can generate figures with multiple panels that elicit comparisons between conditional subsets of data or across different pairings of variables in a dataset.seaborn is designed to be useful throughout the lifecycle of a scientific project.By producing complete graphics from a single function call with minimal arguments, seaborn facilitates rapid prototyping and exploratory data analysis.And by offering extensive options for customization, along with exposing the underlying matplotlib objects, it can be used to create polished, publication-quality figures. (@Waskom2021)

Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan Saharia, Alexandros G. Dimakis, and Peyman Milanfar Deblurring via stochastic refinement In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 16293–16303, June 2022. **Abstract:** Image deblurring is an ill-posed problem with multiple plausible solutions for a given input image. However, most existing methods produce a deterministic estimate of the clean image and are trained to minimize pixel-level distortion. These metrics are known to be poorly correlated with human perception, and often lead to unrealistic reconstructions. We present an alternative framework for blind deblurring based on conditional diffusion models. Unlike existing techniques, we train a stochastic sampler that refines the output of a deterministic predictor and is capable of producing a diverse set of plausible reconstructions for a given input. This leads to a significant improvement in perceptual quality over existing state-of-the-art methods across multiple standard benchmarks. Our predict-and-refine approach also enables much more efficient sampling compared to typical diffusion models. Combined with a carefully tuned network architecture and inference procedure, our method is competitive in terms of distortion metrics such as PSNR. These results show clear benefits of our diffusion-based method for deblurring and challenge the widely used strategy of producing a single, deterministic reconstruction. (@Whang_2022_CVPR)

Ke Yang and Julia Stoyanovich Measuring fairness in ranked outputs In *Proceedings of the 29th International Conference on Scientific and Statistical Database Management*, SSDBM ’17, New York, NY, USA, 2017. Association for Computing Machinery. ISBN 9781450352826. . URL <https://doi.org/10.1145/3085504.3085526>. **Abstract:** Ranking and scoring are ubiquitous. We consider the setting in which an institution, called a ranker, evaluates a set of individuals based on demographic, behavioral or other characteristics. The final output is a ranking that represents the relative quality of the individuals. While automatic and therefore seemingly objective, rankers can, and often do, discriminate against individuals and systematically disadvantage members of protected groups. This warrants a careful study of the fairness of a ranking scheme, to enable data science for social good applications, among others. (@10.1145/3085504.3085526)

Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang Gan prior embedded network for blind face restoration in the wild In *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2021. **Abstract:** Blind face restoration (BFR) from severely degraded face images in the wild is a very challenging problem. Due to the high illness of the problem and the complex unknown degradation, directly training a deep neural network (DNN) usually cannot lead to acceptable results. Existing generative adversarial network (GAN) based methods can produce better results but tend to generate over-smoothed restorations. In this work, we propose a new method by first learning a GAN for high-quality face image generation and embedding it into a U-shaped DNN as a prior decoder, then fine-tuning the GAN prior embedded DNN with a set of synthesized low-quality face images. The GAN blocks are designed to ensure that the latent code and noise input to the GAN can be respectively generated from the deep and shallow features of the DNN, controlling the global face structure, local face details and background of the reconstructed image. The proposed GAN prior embedded network (GPEN) is easy-to-implement, and it can generate visually photo-realistic results. Our experiments demonstrated that the proposed GPEN achieves significantly superior results to state-of-the-art BFR methods both quantitatively and qualitatively, especially for the restoration of severely degraded face images in the wild. The source code and models can be found at https://github.com/yangxy/GPEN. (@Yang2021GPEN)

Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P. Gummadi In Aarti Singh and Jerry Zhu, editors, *Proceedings of the 20th International Conference on Artificial Intelligence and Statistics*, volume 54 of *Proceedings of Machine Learning Research*, pages 962–970. PMLR, 20–22 Apr 2017. URL <https://proceedings.mlr.press/v54/zafar17a.html>. **Abstract:** Algorithmic decision making systems are ubiquitous across a wide variety of online as well as offline services. These systems rely on complex learning methods and vast amounts of data to optimize the service functionality, satisfaction of the end user and profitability. However, there is a growing concern that these automated decisions can lead, even in the absence of intent, to a lack of fairness, i.e., their outcomes can disproportionately hurt (or, benefit) particular groups of people sharing one or more sensitive attributes (e.g., race, sex). In this paper, we introduce a flexible mechanism to design fair classifiers by leveraging a novel intuitive measure of decision boundary (un)fairness. We instantiate this mechanism with two well-known classifiers, logistic regression and support vector machines, and show on real-world data that our mechanism allows for a fine-grained control on the degree of fairness, often at a small cost in terms of accuracy. (@pmlr-v54-zafar17a)

Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork Learning fair representations In Sanjoy Dasgupta and David McAllester, editors, *Proceedings of the 30th International Conference on Machine Learning*, volume 28 of *Proceedings of Machine Learning Research*, pages 325–333, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR. URL <https://proceedings.mlr.press/v28/zemel13.html>. **Abstract:** We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a whole), and individual fairness (similar individuals should be treated similarly). We formulate fairness as an optimization problem of finding a good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group. We show positive results of our algorithm relative to other known techniques, on three datasets. Moreover, we demonstrate several advantages to our approach. First, our intermediate representation can be used for other classification tasks (i.e., transfer learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification. (@pmlr-v28-zemel13)

Cheng Zhang, Xuanbai Chen, Siqi Chai, Henry Chen Wu, Dmitry Lagun, Thabo Beeler, and Fernando De la Torre : Inclusive text-to-image generation In *ICCV*, 2023. **Abstract:** Text-to-image generative models often reflect the biases of the training data, leading to unequal representations of underrepresented groups. This study investigates inclusive text-to-image generative models that generate images based on human-written prompts and ensure the resulting images are uniformly distributed across attributes of interest. Unfortunately, directly expressing the desired attributes in the prompt often leads to sub-optimal results due to linguistic ambiguity or model misrepresentation. Hence, this paper proposes a drastically different approach that adheres to the maxim that "a picture is worth a thousand words". We show that, for some attributes, images can represent concepts more expressively than text. For instance, categories of skin tones are typically hard to specify by text but can be easily represented by example images. Building upon these insights, we propose a novel approach, ITI-Gen \<sup xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"\>1\</sup\> , that leverages readily available reference images for Inclusive Text-to-Image GENeration. The key idea is learning a set of prompt embeddings to generate images that can effectively represent all desired attribute categories. More importantly, ITI-Gen requires no model fine-tuning, making it computationally efficient to augment existing text-to-image models. Extensive experiments demonstrate that ITI-Gen largely improves over state-of-the-art models to generate inclusive images from a prompt. (@zhang2023inclusive)

Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising *IEEE Transactions on Image Processing*, 26 (7): 3142–3155, 2017. **Abstract:** Discriminative model learning for image denoising has been recently attracting considerable attentions due to its favorable denoising performance. In this paper, we take one step forward by investigating the construction of feed-forward denoising convolutional neural networks (DnCNNs) to embrace the progress in very deep architecture, learning algorithm, and regularization method into image denoising. Specifically, residual learning and batch normalization are utilized to speed up the training process as well as boost the denoising performance. Different from the existing discriminative denoising models which usually train a specific model for additive white Gaussian noise (AWGN) at a certain noise level, our DnCNN model is able to handle Gaussian denoising with unknown noise level (i.e., blind Gaussian denoising). With the residual learning strategy, DnCNN implicitly removes the latent clean image in the hidden layers. This property motivates us to train a single DnCNN model to tackle with several general image denoising tasks such as Gaussian denoising, single image super-resolution and JPEG image deblocking. Our extensive experiments demonstrate that our DnCNN model can not only exhibit high effectiveness in several general image denoising tasks, but also be efficiently implemented by benefiting from GPU computing. (@zhang2017beyond)

Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang Learning deep cnn denoiser prior for image restoration In *IEEE Conference on Computer Vision and Pattern Recognition*, pages 3929–3938, 2017. **Abstract:** Model-based optimization methods and discriminative learning methods have been the two dominant strategies for solving various inverse problems in low-level vision. Typically, those two kinds of methods have their respective merits and drawbacks, e.g., model-based optimization methods are flexible for handling different inverse problems but are usually time-consuming with sophisticated priors for the purpose of good performance, in the meanwhile, discriminative learning methods have fast testing speed but their application range is greatly restricted by the specialized task. Recent works have revealed that, with the aid of variable splitting techniques, denoiser prior can be plugged in as a modular part of model-based optimization methods to solve other inverse problems (e.g., deblurring). Such an integration induces considerable advantage when the denoiser is obtained via discriminative learning. However, the study of integration with fast discriminative denoiser prior is still lacking. To this end, this paper aims to train a set of fast and effective CNN (convolutional neural network) denoisers and integrate them into model-based optimization method to solve other inverse problems. Experimental results demonstrate that the learned set of denoisers can not only achieve promising Gaussian denoising results but also can be used as prior to deliver good performance for various low-level vision applications. (@zhang2017learning)

Kai Zhang, Yawei Li, Wangmeng Zuo, Lei Zhang, Luc Van Gool, and Radu Timofte Plug-and-play image restoration with deep denoiser prior *arXiv*, 2008.13751, 2021. **Abstract:** Recent works on plug-and-play image restoration have shown that a denoiser can implicitly serve as the image prior for model-based methods to solve many inverse problems. Such a property induces considerable advantages for plug-and-play image restoration (e.g., integrating the flexibility of model-based method and effectiveness of learning-based methods) when the denoiser is discriminatively learned via deep convolutional neural network (CNN) with large modeling capacity. However, while deeper and larger CNN models are rapidly gaining popularity, existing plug-and-play image restoration hinders its performance due to the lack of suitable denoiser prior. In order to push the limits of plug-and-play image restoration, we set up a benchmark deep denoiser prior by training a highly flexible and effective CNN denoiser. We then plug the deep denoiser prior as a modular part into a half quadratic splitting based iterative algorithm to solve various image restoration problems. We, meanwhile, provide a thorough analysis of parameter setting, intermediate results and empirical convergence to better understand the working mechanism. Experimental results on three representative image restoration tasks, including deblurring, super-resolution and demosaicing, demonstrate that the proposed plug-and-play image restoration with deep denoiser prior not only significantly outperforms other state-of-the-art model-based methods but also achieves competitive or even superior performance against state-of-the-art learning-based methods. The source code is available at https://github.com/cszn/DPIR. (@zhang2020plug)

Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte Designing a practical degradation model for deep blind image super-resolution In *IEEE International Conference on Computer Vision*, pages 4791–4800, 2021. **Abstract:** It is widely acknowledged that single image super-resolution (SISR) methods would not perform well if the assumed degradation model deviates from those in real images. Although several degradation models take additional factors into consideration, such as blur, they are still not effective enough to cover the diverse degradations of real images. To address this issue, this paper proposes to design a more complex but practical degradation model that consists of randomly shuffled blur, downsampling and noise degradations. Specifically, the blur is approximated by two convolutions with isotropic and anisotropic Gaussian kernels; the downsampling is randomly chosen from nearest, bilinear and bicubic interpolations; the noise is synthesized by adding Gaussian noise with different noise levels, adopting JPEG compression with different quality factors, and generating processed camera sensor noise via reverse-forward camera image signal processing (ISP) pipeline model and RAW image noise model. To verify the effectiveness of the new degradation model, we have trained a deep blind ES-RGAN super-resolver and then applied it to super-resolve both synthetic and real images with diverse degradations. The experimental results demonstrate that the new degradation model can help to significantly improve the practicability of deep super-resolvers, thus providing a powerful alternative solution for real SISR applications. (@zhang2021designing)

Richard Zhang, Phillip Isola, and Alexei A Efros Colorful image colorization In *ECCV*, 2016. **Abstract:** Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a "colorization Turing test," asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32% of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learning benchmarks. (@zhang2016colorful)

Richard Zhang, Jun-Yan Zhu, Phillip Isola, Xinyang Geng, Angela S Lin, Tianhe Yu, and Alexei A Efros Real-time user-guided image colorization with learned deep priors *ACM Transactions on Graphics (TOG)*, 9 (4), 2017. **Abstract:** We propose a deep learning approach for user-guided image colorization. The system directly maps a grayscale image, along with sparse, local user "hints" to an output colorization with a Convolutional Neural Network (CNN). Rather than using hand-defined rules, the network propagates user edits by fusing low-level cues along with high-level semantic information, learned from large-scale data. We train on a million images, with simulated user inputs. To guide the user towards efficient input selection, the system recommends likely colors based on the input image and current user inputs. The colorization is performed in a single feed-forward pass, enabling real-time use. Even with randomly simulated user inputs, we show that the proposed system helps novice users quickly create realistic colorizations, and offers large improvements in colorization quality with just a minute of use. In addition, we demonstrate that the framework can incorporate other user "hints" to the desired colorization, showing an application to color histogram transfer. (@zhang2017real)

Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang The unreasonable effectiveness of deep features as a perceptual metric In *CVPR*, 2018. **Abstract:** While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations. (@zhang2018perceptual)

Zhou, Shangchen, Chan, Kelvin C.K., Li, Chongyi, and Chen Change Loy Towards robust blind face restoration with codebook lookup transformer In *NeurIPS*, 2022. **Abstract:** Blind face restoration is a highly ill-posed problem that often requires auxiliary guidance to 1) improve the mapping from degraded inputs to desired outputs, or 2) complement high-quality details lost in the inputs. In this paper, we demonstrate that a learned discrete codebook prior in a small proxy space largely reduces the uncertainty and ambiguity of restoration mapping by casting blind face restoration as a code prediction task, while providing rich visual atoms for generating high-quality faces. Under this paradigm, we propose a Transformer-based prediction network, named CodeFormer, to model the global composition and context of the low-quality faces for code prediction, enabling the discovery of natural faces that closely approximate the target faces even when the inputs are severely degraded. To enhance the adaptiveness for different degradation, we also propose a controllable feature transformation module that allows a flexible trade-off between fidelity and quality. Thanks to the expressive codebook prior and global modeling, CodeFormer outperforms the state of the arts in both quality and fidelity, showing superior robustness to degradation. Extensive experimental results on synthetic and real-world datasets verify the effectiveness of our method. (@zhou2022codeformer)

</div>

# Summary of mathematical notations and fairness definitions [appendix:summary-of-notations]

We summarize in   
eftab:mathematical-notations the mathematical notations and fairness definitions used in this paper.

<div id="tab:mathematical-notations" markdown="1">

| Name / Notation | Meaning / Formal definition |
|:--:|:---|
|  |  |
| \\(X\\) | Ground truth image (a random vector) |
| \\(Y\\) | Degraded measurement (a random vector) |
| \\(\hat{X}\\) | Reconstructed image (a random vector) |
| \\(p_{X}\\) | P.d.f of the ground truth images |
| \\(p_{Y}\\) | P.d.f of the degraded measurements |
| \\(p_{\hat{X}}\\) | P.d.f of the reconstructed images |
| Perceptual Index (\\(\text{PI}_{d}\\) or PI) | \\(d(p_{X},p_{\hat{X}})\\) |
| \\(A\\) | Sensitive attribute (a random vector) |
| \\(p_{X|A}(\cdot|a)\\) | P.d.f of the ground truth images of \\(A=a\\) |
| \\(p_{Y|A}(\cdot|a)\\) | P.d.f of the degraded measurements of \\(A=a\\) |
| \\(p_{\hat{X}|A}(\cdot|a)\\) | P.d.f of the reconstructed images of \\(A=a\\) |
| \\(\mathcal{X}_{a}\\) | \\(\mathop{\mathrm{supp}}{p_{X|A}(\cdot|a)}\\) |
| \\(\mathcal{Y}_{a}\\) | \\(\mathop{\mathrm{supp}}{p_{Y|A}(\cdot|a)}\\) |
| \\(\hat{\mathcal{X}}_{a}\\) | \\(\mathop{\mathrm{supp}}{p_{\hat{X}|A}(\cdot|a)}\\) |
| Group Perceptual Index (\\(\text{GPI}_{d}(a)\\), \\(\text{GPI}_{d}\\), or GPI) | \\(d(p_{X|A}(\cdot|a),p_{\hat{X}|A}(\cdot|a))\\) |
| Group Precision (\\(\text{GP}(a)\\) or GP) | \\(\mathbb{P}(\hat{X}\in\mathcal{X}_{a}|A=a)\\) |
| Group Recall (\\(\text{GR}(a)\\) or GR) | \\(\mathbb{P}(X\in\hat{\mathcal{X}}_{a}|A=a)\\) |
| Representation Demographic Parity (RDP) | \\(\forall a_{1},a_{2}:\:\text{GP}(a_{1})=\text{GP}(a_{2})\\) |
| Proportional Representation (PR) | \\(\forall a:\:\mathbb{P}(X\in\mathcal{X}_{a})=\mathbb{P}(\hat{X}\in\mathcal{X}_{a})\\) |
| Conditional Proportional Representation (CPR) | \\(\forall a,y:\:\mathbb{P}(X\in\mathcal{X}_{a}|Y=y)=\mathbb{P}(\hat{X}\in\mathcal{X}_{a}|Y=y)\\) |
| Perceptual Fairness (\\(\smash{\text{PF}_{d}}\\) or PF) | \\(\forall a_{1},a_{2}:\: \text{GPI}_{d}(a_{1})=\text{GPI}_{d}(a_{2})\\) |

Summary of mathematical notations and fairness definitions used in this paper.

</div>

# Toy signal restoration example [appendix:toy]

The following toy signal restoration example demonstrates that common estimators (*e.g*.., the stochastic estimator which samples from the posterior distribution \\(p_{X|Y}\\)) do not trivially achieve perfect PF.

<figure id="fig:toy">
<img src="./figures/toy-example.png"" style="width:100.0%" />
<figcaption>Illustration of <br />
ef<span>example:toy-dmax</span>. <strong>Left</strong>: Conditional probability density functions <span class="math inline"><em>p</em><sub><em>X</em>|<em>A</em></sub>(⋅|<em>a</em>), <em>p</em><sub><em>X̂</em><sub>MSE</sub>|<em>A</em></sub>(⋅|<em>a</em>), <em>p</em><sub><em>X̂</em><sub>Posterior</sub>|<em>A</em></sub>(⋅|<em>a</em>),</span> and <span class="math inline"><em>p</em><sub><em>X̂</em><sub>MSE+PI</sub>|<em>A</em></sub>(⋅|<em>a</em>)</span>, where <span class="math inline"><em>a</em> = 1</span> (left plot) or <span class="math inline"><em>a</em> = 0</span> (right plot). <strong>Right</strong>: The <span class="math inline">$\smash{\text{GPI}_{d_{\text{TV}}}}$</span> and <span class="math inline">$\smash{\text{GPI}_{W_{1}}}$</span> of each group (associated with <span class="math inline"><em>a</em> = 1</span> or <span class="math inline"><em>a</em> = 0</span>). The dotted lines <span class="math inline">$\smash{\text{PF}_{d_{\text{TV}}}}$</span> or <span class="math inline">$\smash{\text{PF}_{W_{1}}}$</span> correspond to the points where perfect <span class="math inline">$\smash{\text{PF}_{d_{\text{TV}}}}$</span> or perfect <span class="math inline">$\smash{\text{PF}_{W_{1}}}$</span> is achieved, respectively. It is clear that all three estimators achieve sub-optimal <span class="math inline">$\smash{\text{PF}_{d_{\text{TV}}}}$</span> and sub-optimal <span class="math inline">$\smash{\text{PF}_{W_{1}}}$</span>. See <br />
ef<span>appendix:toy</span> for more details.</figcaption>
</figure>

<div id="example:toy-dmax" class="example" markdown="1">

**Example 1**. *Suppose that \\(X,N\sim\mathcal{N}(0,1)\\) are statistically independent random variables, and let \\(\smash{Y=X+N}\\). In this case, it is known that \\(\smash{\hat{X}_{\text{MSE}}=\frac{1}{2}Y}\\) is the estimator that attains the lowest possible Mean-Squared-Error (MSE), \\(\smash{\hat{X}_{\text{Posterior}}=\frac{1}{2}Y+W}\\) where \\(\smash{W\sim\mathcal{N}(0,\frac{1}{2})}\\) is statistically independent of \\(\smash{X}\\) and \\(\smash{Y}\\), is the estimator that samples from the posterior distribution \\(p_{X|Y}\\), and \\(\smash{\hat{X}_{\text{MSE+PI}}=\frac{1}{\sqrt{2}}Y}\\) is the estimator that attains the lowest possible MSE among all estimators that satisfy \\(p_{\hat{X}}=p_{X}\\) (perfect \\(\smash{\text{PI}_{d}}\\)) `\cite{Blau2018,dror}`{=latex}. Now, consider the “sensitive attribute” \\(A=\mathds{1}_{X\geq 1}\\). All of these commonly used estimators produce much better (lower) \\(\smash{\text{GPI}_{d_{\text{TV}}}}\\) and \\(\smash{\text{GPI}_{W_{1}}}\\) for the group associated with \\(A=0\\), which, in this case, is a majority satisfying \\(\mathbb{P}(A=0)\approx 0.8413\\) (see   
effig:toy).*

</div>

## Conditional density plots [appendix:density-plots-details]

The density \\(p_{X|A}(x|a)\\) is obtained using the closed form solution of a truncated normal distribution, \\[\begin{aligned}
    &p_{X|A}(x|1)=\frac{\phi(x)}{\Phi(\infty)-\Phi(1)},\\
    &p_{X|A}(x|0)=\frac{\phi(x)}{\Phi(1)-\Phi(-\infty)},
\end{aligned}\\] where \\(\phi(x)\\) is a normal density and \\(\Phi(x)\\) is its cumulative distribution, \\[\begin{aligned}
    &\phi(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^{2}},\\
    &\Phi(x)=\frac{1}{2}\left(1+\text{erf}\left(\frac{x}{\sqrt{2}}\right)\right),
\end{aligned}\\] and \\(\smash{p_{X|A}(x|1)=0}\\) and \\(\smash{p_{X|A}(x|0)=0}\\) for every \\(x\geq 1\\) and \\(x\leq 1\\), respectively. The densities \\(\smash{p_{\hat{X}_{\text{MSE}}|A}(\cdot|a),p_{\hat{X}_{\text{MSE+PQ}}|A}(\cdot|a)}\\) and \\(\smash{p_{\hat{X}_{\text{Posterior}}|A}(\cdot|a)}\\) are obtained by feeding these algorithms with the degraded measurements corresponding to \\(\smash{X\geq 1}\\) (for \\(a=1\\)) and to \\(\smash{X<1}\\) (for \\(a=0\\)), separately. This is achieved by generating samples \\(x\sim p_{X}\\) and \\(y\sim p_{Y|X}(\cdot|x)\\), and then partitioning these samples into two sets of measurements based on the value of \\(x\\). We then perform Kernel Density Estimation (KDE) `\cite{kde}`{=latex} on the reconstructions of each group to obtain their density, using the function `seaborn.kdeplot` `\cite{Waskom2021}`{=latex} with the arguments `bw_adjust=2, common_norm=False, gridsize=200`. The number of samples used to compute the KDE is set to 200,000 for both \\(a=1\\) and \\(a=0\\).

## Computation of the total variation distance \\(d_{\text{TV}}\\) and of the Wasserstein distance \\(W_{1}\\)

The value of \\(\text{GPI}_{d_{\text{TV}}}(a)\\) for a given algorithm \\(\hat{X}\\) is defined by the total variation distance \\[\begin{aligned}
   \text{GPI}_{d_{\text{TV}}}(a)=d_{\text{TV}}(p_{X|A}(\cdot|a),p_{\hat{X}|A}(\cdot|a))=\frac{1}{2}\int\left|p_{X|A}(x|a)-p_{\hat{X}|A}(x|a)\right|dx .
\end{aligned}\\] To compute this integral, we use the function `scipy.integrate.quad` `\cite{2020SciPy-NMeth}`{=latex} with parameters `(a=-1000, b=1000, limit=500, points=[1.0])`. At each point \\(x\\), the integrand \\[\begin{aligned}
    \left|p_{X|A}(x|a)-p_{\hat{X}|A}(x|a)\right|
\end{aligned}\\] is evaluated using the closed form solution of \\(p_{X|A}(\cdot|a)\\) and the pre-computed KDE density of each \\(p_{\hat{X}|A}(\cdot|a)\\).

The value of \\(\text{GPI}_{W_{1}}(a)\\) for a given algorithm \\(\hat{X}\\) is the Wasserstein 1-distance between \\(\smash{p_{X|A}(\cdot|a)}\\) and \\(\smash{p_{\hat{X}|A}(\cdot|a)}\\). To approximate this distance, we utilize the function `scipy.stats.wasserstein_distance` with the previously obtained 200,000 samples from \\(p_{X|A}(\cdot|a)\\) and 200,000 samples from \\(p_{\hat{X}|A}(\cdot|a)\\).

# Proof of  eftheorem:hitratebound [appendix:proof-hitrate]

<div class="proof" markdown="1">

*Proof.* For every \\(a,x\\), it holds that \\[\begin{aligned}
    p_{\hat{X}|A}(x|a)\geq\min{\left\{p_{X|A}(x|a),p_{\hat{X}|A}(x|a)\right\}}.
\end{aligned}\\] Moreover, the value of \\(\min{\left\{p_{X|A}(x|a),p_{\hat{X}|A}(x|a)\right\}}\\) is zero for every \\(x\notin \mathop{\mathrm{supp}}{p_{X|A}(\cdot|a)}\\), so \\[\begin{aligned}
    \int_{\mathop{\mathrm{supp}}{p_{X|A}(\cdot|a)}}\min{\left\{p_{X|A}(x|a),p_{\hat{X}|A}(x|a)\right\}}dx=\int \min{\left\{p_{X|A}(x|a),p_{\hat{X}|A}(x|a)\right\}}dx.
\end{aligned}\\] Thus, \\[\begin{aligned}
\text{GP}(a)&=\mathbb{P}(\hat{X}\in \mathcal{X}_{a}|A=a)\\
&=\mathbb{P}(\hat{X}\in\mathop{\mathrm{supp}}{p_{X|A}(\cdot|a)}|A=a)\\
    &=\int_{\mathop{\mathrm{supp}}{p_{X|A}(\cdot|a)}}p_{\hat{X}|A}(x|a)dx\\
    &\geq\int_{\mathop{\mathrm{supp}}{p_{X|A}(\cdot|a)}} \min{\left\{p_{X|A}(x|a),p_{\hat{X}|A}(x|a)\right\}}dx\\
    &=\int \min{\left\{p_{X|A}(x|a),p_{\hat{X}|A}(x|a)\right\}}dx\\
    &=\int \frac{1}{2}\left(p_{\hat{X}|A}(x|a)+p_{X|A}(x|a)-\left|p_{\hat{X}|A}(x|a)-p_{X|A}(x|a)\right|\right)dx\\
    &=\frac{1}{2}\int \left(p_{\hat{X}|A}(x|a)+ p_{X|A}(x|a)\right)dx-\frac{1}{2}\int\left|p_{\hat{X}|A}(x|a)-p_{X|A}(x|a)\right|dx\\
    &=1-d_{\text{TV}}(p_{X|A}(\cdot|a),p_{\hat{X}|A}(\cdot|a))\\
    &=1-\text{GPI}_{d_{\text{TV}}}(a).
\end{aligned}\\] By replacing the roles of \\(p_{\hat{X}|A}(x|a)\\) and \\(p_{X|A}(x|a)\\), the result \\(\text{GR}(a)\geq 1-\text{GPI}_{d_{\text{TV}}}(a)\\) can be derived with identical steps using the same mathematical arguments. ◻

</div>

# Proof of  eftheorem:disjoint [appendix:proof-disjoint]

<div class="proof" markdown="1">

*Proof.* Suppose by contradiction that \\(p_{\hat{X}|A}(\cdot|a_{i})=p_{X|A}(\cdot|a_{i})\\) for both \\(i=1,2\\). Thus, \\[\begin{aligned}
   1&=\mathbb{P}(X\in \mathcal{X}_{a_{i}}|A=a_{i})\\
   &=\mathbb{P}(\hat{X}\in \mathcal{X}_{a_{i}}|A=a_{i})\\
   &=\int_{\mathcal{X}_{a_{i}}} p_{\hat{X}|A}(x|a_{i})dx\\
  &=\int\int_{\mathcal{X}_{a_{i}}} p_{\hat{X},Y|A}(x|a_{i})dxdy\\
    &=\int\int_{\mathcal{X}_{a_{i}}} p_{\hat{X}|A,Y}(x|a_{i})p_{Y|A}(y|a_{i})dxdy\\
    &=\int_{\mathcal{Y}_{a_{i}}}\int_{\mathcal{X}_{a_{i}}} p_{\hat{X}|Y}(x|y)p_{Y|A}(y|a_{i})dxdy\label{eq:markov1}\\
    &=\int_{\mathcal{Y}_{a_{i}}} p_{Y|A}(y|a_{i})\left(\int_{\mathcal{X}_{a_{i}}}p_{\hat{X}|Y}(x|y)dx\right)dy\\
    &=\int_{\mathcal{Y}_{a_{i}}} p_{Y|A}(y|a_{i})\mathbb{P}(\hat{X}\in\mathcal{X}_{a_{i}}|Y=y)dy,\label{eq:same-steps-orig}
\end{aligned}\\] where   
efeq:markov1 holds from the assumption that \\(A\\) and \\(\hat{X}\\) are statistically independent given \\(Y\\), and from the fact that \\(p_{Y|A}(y|a_{i})=0\\) for every \\(y\notin\mathcal{Y}_{a_{i}}\\). We will show that \\(\smash{\mathbb{P}(\hat{X}\in \mathcal{X}_{a_{i}}|Y=y)=1}\\) for almost every \\(y\in \mathcal{Y}_{a_{i}}\\). Indeed, if this does not hold, then for some \\(\smash{\mathcal{T}_{i}\subseteq\mathcal{Y}_{a_{i}}}\\) with \\({\mathbb{P}(Y\in\mathcal{T}_{i}|A=a_{i})>0}\\) we have \\(\smash{\mathbb{P}(\hat{X}\in \mathcal{X}_{a_{i}}|Y=y)<1}\\) for every \\(y\in\mathcal{T}_{i}\\). Thus, \\[\begin{aligned}
    1&=\int_{\mathcal{Y}_{a_{i}}} p_{Y|A}(y|a_{i})\mathbb{P}(\hat{X}\in\mathcal{X}_{a_{i}}|Y=y)dy\\
    &=\int_{\mathcal{Y}_{a_{i}}\setminus\mathcal{T}_{i} } p_{Y|A}(y|a_{i})\mathbb{P}(\hat{X}\in\mathcal{X}_{a_{i}}|Y=y)dy+\int_{\mathcal{T}_{i}} p_{Y|A}(y|a_{i})\mathbb{P}(\hat{X}\in\mathcal{X}_{a_{i}}|Y=y)dy\\
    &<\int_{\mathcal{Y}_{a_{i}}\setminus\mathcal{T}_{i} } p_{Y|A}(y|a_{i})\mathbb{P}(\hat{X}\in\mathcal{X}_{a_{i}}|Y=y)dy+\int_{\mathcal{T}_{i}} p_{Y|A}(y|a_{i})dy\\
    &\leq \int_{\mathcal{Y}_{a_{i}}\setminus\mathcal{T}_{i} } p_{Y|A}(y|a_{i})dy+\int_{\mathcal{T}_{i}} p_{Y|A}(y|a_{i})dy\\
    &=\int_{\mathcal{Y}_{a_{i}}} p_{Y|A}(y|a_{i})dy\\
    &=1,
\end{aligned}\\] which is not possible. So, \\(\mathbb{P}(\hat{X}\in \mathcal{X}_{a_{i}}|Y=y)=1\\) for almost every \\(y\in \mathcal{Y}_{a_{i}}\\). Now, from basic rules of probability theory, we have \\[\begin{aligned}
\mathbb{P}(X\in\mathcal{X}_{a_{1}}\cap\mathcal{X}_{a_{2}}|A=a_{1})=&\mathbb{P}(X\in\mathcal{X}_{a_{1}}|A=a_{1})\nonumber\\&+\mathbb{P}(X\in\mathcal{X}_{a_{2}}|A=a_{1})\nonumber\\&-\mathbb{P}(X\in\mathcal{X}_{a_{1}}\cup\mathcal{X}_{a_{2}}|A=a_{1}),
\end{aligned}\\] where the first and last terms on the right hand side cancel out (from the definition of \\(\mathcal{X}_{a_{1}}\\), they are both equal to 1). Thus, we have \\[\begin{aligned}
\mathbb{P}(X\in\mathcal{X}_{a_{1}}\cap\mathcal{X}_{a_{2}}|A=a_{1})=\mathbb{P}(X\in\mathcal{X}_{a_{2}}|A=a_{1}),
\end{aligned}\\] and finally, \\[\begin{aligned}
\mathbb{P}(X\in\mathcal{X}_{a_{1}}\cap\mathcal{X}_{a_{2}}|A=a_{1})&=\mathbb{P}(X\in\mathcal{X}_{a_{2}}|A=a_{1})\\
    &=\mathbb{P}(\hat{X}\in\mathcal{X}_{a_{2}}|A=a_{1})\label{eq:assumtion5}\\
    &=\int_{\mathcal{Y}_{a_{1}}} p_{Y|A}(y|a_{1})\mathbb{P}(\hat{X}\in\mathcal{X}_{a_{2}}|Y=y)dy\label{eq:same-steps}\\
    &\geq \int_{\mathcal{Y}_{a_{1}}\cap\mathcal{Y}_{a_{2}}} p_{Y|A}(y|a_{1})\mathbb{P}(\hat{X}\in\mathcal{X}_{a_{2}}|Y=y)dy\\
    &=\int_{\mathcal{Y}_{a_{1}}\cap\mathcal{Y}_{a_{2}}} p_{Y|A}(y|a_{1})dy\label{eq:assumtion6}\\
    &=\mathbb{P}(Y\in\mathcal{Y}_{a_{1}}\cap\mathcal{Y}_{a_{2}}|A=a_{1}),
\end{aligned}\\] where   
efeq:assumtion5 follows from the contradictory assumption that \\(\smash{p_{\hat{X}|A}(\cdot|a_{i})=p_{X|A}(\cdot|a_{i})}\\),   
efeq:same-steps follows from the same steps that led to   
efeq:same-steps-orig, and   
efeq:assumtion6 follows from our previous finding that \\(\smash{\mathbb{P}(\hat{X}\in\mathcal{X}_{a_{i}}|Y=y)=1}\\) for every \\(y\in\mathcal{Y}_{a_{i}}\\) (we have \\(y\in\mathcal{Y}_{a_{1}}\cap\mathcal{Y}_{a_{2}}\\) in the integrand, so \\(y\in\mathcal{Y}_{a_{2}}\\)). However, it is given that \\(\mathbb{P}(X\in\mathcal{X}_{a_{1}}\cap\mathcal{X}_{a_{2}}|A=a_{1})<\mathbb{P}(Y\in\mathcal{Y}_{a_{1}}\cap\mathcal{Y}_{a_{2}}|A=a_{1})\\), so we have established a contradiction. ◻

</div>

# Proof of  efcorollary:pfi-pi-tradeoff [appendix:proof-tradeoff]

<div class="proof" markdown="1">

*Proof.* Suppose that \\(\text{GPI}_{d_{\text{TV}}}(a_{m})=0\\). From the assumptions, there exists \\(a\neq a_{m}\\) such that \\(\text{GPI}_{d}(a)>0\\), so \\(\text{GPI}_{d_{\text{TV}}}(a)>0\\). This means that \\(\text{PF}_{d_{\text{TV}}}\\) is not perfect.

Otherwise, suppose that \\(\text{GPI}_{d_{\text{TV}}}(a_{m})>0\\). Thus, from   
eftheorem:gpibound we have \\[\begin{aligned}
    \text{GPI}_{d_{\text{TV}}}(a_{m})&\leq\frac{1-\mathbb{P}(A=a_{m})}{\mathbb{P}(A=a_{m})}\max_{a'\neq a_{m}}\text{GPI}_{d_{\text{TV}}}(a')\\
    &<\max_{a'\neq a_{m}}\text{GPI}_{d_{\text{TV}}}(a)\label{coro:second}\\
    &=\text{GPI}_{d_{\text{TV}}}(a^{*}),\label{coro:last}
\end{aligned}\\] where   
efcoro:second holds since \\(\frac{1-\mathbb{P}(A=a_{m})}{\mathbb{P}(A=a_{m})}<1\\), and   
efcoro:last holds by defining \\[\begin{aligned}
    a^{*}=\mathop{\mathrm{arg\,max}}_{a'\neq a}{\text{GPI}_{d_{\text{TV}}}(a')}.
\end{aligned}\\] Thus, we have found two groups \\(a_{m}\\) and \\(a^{*}\\) such that \\(\text{GPI}_{d_{\text{TV}}}(a_{m})<\text{GPI}_{d_{\text{TV}}}(a^{*})\\), so \\(\text{PF}_{d_{\text{TV}}}\\) cannot be perfect. ◻

</div>

# Proof of  eftheorem:gpibound [appendix:proof-gpibound]

<div class="proof" markdown="1">

*Proof.* For every \\(a\\), let us denote \\(P_{a}=\mathbb{P}(A=a)\\). Suppose that \\(\hat{X}\\) attains perfect perceptual index, so \\(p_{\hat{X}}=p_{X}\\). From the marginalization of probability density functions, it holds that \\[\begin{aligned}
    &p_{X}(x)=\sum_{a}P_{a}p_{X|A}(x|a),\\
    &p_{\hat{X}}(x)=\sum_{a}P_{a}p_{\hat{X}|A}(x|a),
\end{aligned}\\] and since \\(p_{\hat{X}}=p_{X}\\) we have \\[\begin{aligned}
    \sum_{a}P_{a}p_{X|A}(x|a)=\sum_{a}P_{a}p_{\hat{X}|A}(x|a).\label{eq:mixture}
\end{aligned}\\] Let \\(a\\) be some group with \\(P_{a}>0\\). By rearranging   
efeq:mixture we get \\[\begin{aligned}
    P_{a}(p_{X|A}(x|a)-p_{\hat{X}|A}(x|a))=\sum_{a'\neq a}P_{a'}(p_{\hat{X}|A}(x|a')-p_{X|A}(x|a')).
\end{aligned}\\] Taking the absolute value on both sides, we have \\[\begin{aligned}
    P_{a}\left|p_{X|A}(x|a)-p_{\hat{X}|A}(x|a)\right|&=\left|\sum_{a'\neq a}P_{a'}(p_{\hat{X}|A}(x|a')-p_{X|A}(x|a'))\right|\\
    &\leq\sum_{a'\neq a}P_{a'}\left|p_{\hat{X}|A}(x|a')-p_{X|A}(x|a'))\right|,\label{eq:triangle}
\end{aligned}\\] where   
efeq:triangle follows from the triangle inequality. Thus, it holds that \\[\begin{aligned}
    d_{\text{TV}}(p_{X|A}(\cdot|a),p_{\hat{X}|A}(\cdot|a))\nonumber
    &=\frac{1}{2}\int \left|p_{X|A}(x|a)-p_{\hat{X}|A}(x|a)\right|dx\\
    &\leq \frac{1}{2}\int \frac{1}{P_{a}}\sum_{a'\neq a}P_{a'}\left|p_{\hat{X}|A}(x|a')-p_{X|A}(x|a')\right|dx\\
    &= \frac{1}{P_{a}}\sum_{a'\neq a}P_{a'}\left(\frac{1}{2}\int \left|p_{\hat{X}|A}(x|a')-p_{X|A}(x|a')\right|dx\right)\\
    &=\frac{1}{P_{a}}\sum_{a'\neq a}P_{a'}d_{\text{TV}}(p_{X|A}(\cdot|a'),p_{\hat{X}|A}(\cdot|a')).
\end{aligned}\\] This concludes the proof. ◻

</div>

# Face image super-resolution - complementary details and results [appendix:additional-metrics-face-restoration]

## Synthetic data sets [appendix:synthetic-celeba]

All the CelebA-HQ images we use are of size \\(512\times 512\\). The image-to-image translation model we utilize, `stabilityai/stable-diffusion-xl-refiner-1.0`, is sourced from Hugging Face `\cite{sdxl-image-to-image}`{=latex} and boasts over 1,200,000 downloads (at the time writing this paper). This model integrates SDXL `\cite{podell2024sdxl}`{=latex} with SDEdit `\cite{meng2022sdedit}`{=latex}. For all groups, we adjust the hyperparameters `strength` and `guidance_scale` from their default settings, with `strength` set to 0.4. When translating a CelebA-HQ image \\(x\\) into a group image using its specified text instruction (see   
eftab:text-instructions), we choose the *smallest* value from \\([8.5, 9.5, 10.5, 11.5, 12.5]\\) as the `guidance_scale` hyperparameter, such that the resulting image is classified as belonging to the group. Otherwise, if none of these `guidance_scale` values work for some group (*i.e*.., their class is incorrect), we discard all the translations of \\(x\\) from all groups. To clarify, this means that the translated images for different groups may use different `guidance_scale` values, as long as all translations are correctly classified. The text instructions we use for each group are provided in   
eftab:text-instructions. For all groups, we use the same `negative_prompt` text instruction `‘‘ugly, deformed, fake, caricature’’`. Each of the resulting groups contains 1,356 images of size \\(512\times 512\\). In   
effig:old_asian,fig:old_not_asian,fig:not_old_asian,fig:not_old_not_asian we present 130 image samples from each group.

<div id="tab:text-instructions" markdown="1">

|      Group      |       Image-to-image translation text instruction        |
|:---------------:|:--------------------------------------------------------:|
|                 |                                                          |
|    Old&Asian    | `120 years old human, Asian, natural image, sharp, DSLR` |
|   Young&Asian   | `20 years old human, Asian, natural image, sharp, DSLR`  |
|  Old&non-Asian  |    `120 years old human, natural image, sharp, DSLR`     |
| Young&non-Asian |     `20 years old human, natural image, sharp, DSLR`     |

Text instructions for the image-to-image translation model to generate images of each fairness group. See   
efsection:experiments and   
efappendix:synthetic-celeba for more details.

</div>

## Visual results [appendx:visual-results]

Visual results of all algorithms (the reconstructions of each fairness group) for \\(s\in\{4,8,16,32\}\\) and \\(\sigma_{N}\in\{0,0.1\}\\) are provided in   
effig:s=4-n=0,fig:s=4-n=1,fig:s=8-n=0,fig:s=8-n=1,fig:s=16-n=0,fig:s=16-n=1,fig:s=32-n=0,fig:s=32-n=1.

## Additional levels of additive noise [appendix:additional-noise-levels]

  
effig:quantitativesr presents quantitative results with all scaling factors, and without adding white Gaussian noise (\\(\sigma_{N}=0\\)). Here, in   
effig:kid-and-gp-sigma0.1,fig:kid-and-gp-sigma0.25 we report the results with \\(\sigma_{N}\in\{0.1,0.25\}\\). We observe similar trends and conclusions as in   
effig:quantitativesr (please refer to   
efsection:detecting-bias-with-pf for more details).

## Comparing \\(\smash{\text{GPI}_{\text{FID}}}\\) instead of \\(\smash{\text{GPI}_{\text{KID}}}\\) [appendix:fid-instead-of-kid]

We report in   
effig:additional-metrics0,fig:additional-metrics01,fig:additional-metrics025 the \\(\smash{\text{GPI}_{\text{FID}}}\\) of each group, where FID is the Fréchet Inception Distance `\cite{fid}`{=latex}. These results show trends similar to those observed in   
effig:quantitativesr. Namely, using the statistical distance FID instead of KID does not alter the trends and conclusions of the results.

## Additional group metrics [appendix:additional-metrics]

We report, compare and analyze additional group performance metrics.

#### \\(\text{GP}_{\text{NN}}\\) and \\(\text{GR}_{\text{NN}}\\)

We approximate the GP and GR of each group using `\cite{NEURIPS2019_0234c510}`{=latex}, a method which evaluates the precision and recall between two distributions in their feature space. We denote the results by \\(\text{GP}_{\text{NN}}\\) and \\(\text{GR}_{\text{NN}}\\), respectively. Note that this approach to approximate GP differs from our previous experiments, where we use the classification hit rate (  
effig:quantitativesr,fig:kid-and-gp-sigma0.1,fig:kid-and-gp-sigma0.25). Similarly to the experiments where we compute \\(\smash{\text{GPI}_{\text{KID}}}\\) (  
efsection:detecting-bias-with-pf) and \\(\smash{\text{GPI}_{\text{FID}}}\\) (  
efappendix:fid-instead-of-kid), \\(\text{GP}_{\text{NN}}\\) and \\(\text{GR}_{\text{NN}}\\) are computed by extracting image features using the last average pooling layer of the FairFace combined age & ethnicity classifier `\cite{karkkainenfairface}`{=latex}.

#### GPSNR and GLPIPS

For each group we compute the Peak Signal-to-Noise Ratio (PSNR) and the Learned Perceptual Image Patch Similarly (LPIPS) `\cite{zhang2018perceptual}`{=latex}[^3], where these metrics are evaluated by feeding the restoration algorithm only with the group’s inputs and with respect to the group’s ground truth images. Formally, we define the Group PSNR (GPSNR) and the Group LPIPS (GLPIPS) as \\[\begin{aligned}
    &\text{GPSNR}(a)=\mathbb{E}[\text{PSNR}(X,\hat{X})|A=a],\\
    &\text{GLPIPS}(a)=\mathbb{E}[\text{LPIPS}(X,\hat{X})|A=a],
\end{aligned}\\] where the expectation is taken over the joint distribution of a group’s ground truth images and their reconstructions, \\(p_{X,\hat{X}|A}(\cdot,\cdot|a)\\).

The results for all noise levels \\(\sigma_{N}\in\{0.0, 0.1, 0.25\}\\) are provided in   
effig:additional-metrics0,fig:additional-metrics01,fig:additional-metrics025. First, note that both the GPSNR and the GLPIPS metrics are unreliable indicators of bias. For example, the metrics GP, \\(\text{GP}_{\text{NN}}\\) \\(\smash{\text{GPI}_{\text{KID}}}\\), and \\(\smash{\text{GPI}_{\text{FID}}}\\) all indicate that the group young&non-Asian receives better treatment than the group young&Asian (*e.g*.., the GP of the former group is clearly higher than that of the latter group across all noise levels and scaling factors). However, both groups exhibit roughly similar GPSNR and GLPIPS scores. This highlights why assessing the fairness of image restoration algorithms solely based on GPSNR, GLPIPS or similar metrics (MSE, SSIM `\cite{ssim}`{=latex}, *etc*..) might not be sufficient. This result regarding GPSNR is not surprising, as it is well known that such a metric often does not correlate with perceived image quality `\cite{Blau2018}`{=latex}. Regarding GLPIPS, it might be more effective to use image features extracted by a classifier trained to identify the sensitive attributes in question. We leave exploring this option for future work. Second, the \\(\text{GP}_{\text{NN}}\\) values in   
effig:additional-metrics0,fig:additional-metrics01,fig:additional-metrics025 are almost identical to the GP scores reported in   
effig:quantitativesr,fig:kid-and-gp-sigma0.1,fig:kid-and-gp-sigma0.25. This suggests that approximating the true GP either through the classification hit rate (as in   
effig:quantitativesr,fig:kid-and-gp-sigma0.1,fig:kid-and-gp-sigma0.25) or via `\cite{NEURIPS2019_0234c510}`{=latex} (as done in this section), are consistent. Third, the \\(\text{GR}_{\text{NN}}\\) scores suggest potential unfairness in the perceptual variation across different groups. For example, when \\(s=16,\sigma_{N}=0\\), we observe that all algorithms consistently produce higher \\(\text{GR}_{\text{NN}}\\) scores for the young&non-Asian group compared to the young&Asian group.

## Feature extractors ablation [appendix:ablation-feature-extractors]

We employ the `dinov2-vit-g-14` `\cite{oquab2024dinov}`{=latex}, `clip-vit-l-14` `\cite{pmlr-v139-radford21a}`{=latex}, and `inception-v3-compat` `\cite{inception}`{=latex} feature extractors via `torch-fidelity` `\cite{obukhov2020torchfidelity}`{=latex} to compute the \\(\smash{\text{GPI}_{\text{KID}}}\\) for each fairness group (previously, we used the image features extracted from the FairFace classifier’s final average pooling layer). The results are presented in   
effig:dino,fig:clip,fig:inception.

The outcomes from both the `dinov2-vit-g-14` and `clip-vit-l-14` feature extractors generally align with those of the FairFace image classifier, though the biases exposed by these extractors are less pronounced. Put differently, computing \\(\smash{\text{GPI}_{\text{KID}}}\\) with either of these general-purpose feature extractors leads to a smaller disparity in the \\(\smash{\text{GPI}_{\text{KID}}}\\) of the different fairness groups. Moreover, the `inception-v3-compat` image feature extractor yields inconsistent results, suggesting that the old&Asian group receives more favorable treatment compared to the old&non-Asian group (contrary to the biases indicated by the other feature extractors). The following section strengthens our argument that this behavior of `inception-v3-compat` is undesirable. Overall, relying on such general-purpose image feature extractors seems unsatisfactory for the purpose of uncovering nuanced biases in face image restoration methods.

## Considering age and ethnicity as separate sensitive attributes [appendix:disentangle_age_and_ethnicity]

In   
efsection:detecting-bias-with-pf we reveal a significant discrepancy between PF and RDP regarding whether the groups old&Asian and old&non-Asian are treated equally. Specifically, both groups achieve similar GP, while the \\(\smash{\text{GPI}_{\text{KID}}}\\) of the latter group (old&non-Asian) is notably better (lower) than that of the former group (old&Asian). In other words, \\(\smash{\text{GPI}_{\text{KID}}}\\) indicates that the old&non-Asian group enjoys a better preservation of ethnicity.

Let us support our claim in   
efsection:detecting-bias-with-pf that this outcome of PF is the desired one, by showing that RDP may obscure the fact that some sensitive attributes are treated better than others. Indeed, as shown in   
effig:only-ethnicity, the ethnicity of the old&non-Asian group is better preserved than that of the old&Asian group, while   
effig:only-age confirms that the age of these two groups is equally preserved. While RDP fails to uncover this ethnicity bias when the fairness groups are determined based on *both* age and ethnicity, PF clearly reveals it.

## Final details

All algorithms are evaluated using the official codes and checkpoints provided by their authors. We use the `torch-fidelity` package `\cite{obukhov2020torchfidelity}`{=latex} (GitHub commit `a61422f`) to compute the KID `\cite{bińkowski2018demystifying}`{=latex}, FID `\cite{fid}`{=latex}, precision and recall `\cite{NEURIPS2019_0234c510}`{=latex}. The GPSNR and the GLPIPS are computed using the `piq` package `\cite{piq,kastryulin2022piq}`{=latex} (version 0.8.0 in `pip`).

Finally, note that some of the evaluated algorithms generate output images of size \\(256\times 256\\) (*e.g*.., DDNM), while others produce images of size \\(512\times 512\\) (*e.g*.., RestoreFormer). Consequently, for fair quantitative evaluations, we resize the outputs of the latter algorithms, along with the ground truth images, to \\(256\times 256\\). To clarify, the super-resolution scaling factors are calculated based on the \\(256\times 256\\) image size. For instance, when \\(s=4\\), the resolution of the input images is \\(64\times 64\\).

# Adversarial attacks - complementary details [appendix:adv-attacks-details]

The degradation we apply consists of three consecutive steps: (1) Average pooling down-sampling with a scale factor of \\(s=4\\), (2) additive white Gaussian noise with a standard deviation of \\(\sigma_{N}=0.1\\), and then (3) JPEG compression with a quality factor of 50. We attack each degraded image using a tweaked version of the I-FGSM basic attack `\cite{Choi_2019_ICCV}`{=latex} with \\(\alpha=6/255\\) and \\(T=200\\). In particular, instead of using the \\(L_{2}\\) loss in I-FGSM like in `\cite{Choi_2019_ICCV}`{=latex}, we forward each attacked output through a classifier that predicts the age category of the output face image `\cite{nate_raw_2023}`{=latex}, and then maximize the log-probability of the oldest age group category. In other words, we forward each degraded image through RestoreFormer++ and then feed the result to the age classifier. We then use the I-FGSM update rule to maximize the soft-max probability of the oldest age category (this adversarial attack technique was employed in `\cite{ohayon2023perceptionrobustness}`{=latex}).

## Additional experiments on image denoising and deblurring [appendix:denoising-deblurring]

While our primary empirical study focuses on face image super-resolution, the theoretical analysis developed in the main paper applies, 
in an entirely analogous way, to other linear inverse problems such as image denoising and deblurring.  In particular, the Bayesian formulation in Section [section:problem-formulation] does not depend on the specific degradation operator, and the proofs in Section [section:theorems] remain valid whenever the forward model is linear and the estimator obeys the Markov assumption \(A \rightarrow Y \rightarrow \hat{X}\.\)  Because of this direct transference, we refrain from reporting an additional battery of denoising or deblurring results, and instead refer the interested reader to the super-resolution experiments as a representative showcase.  Empirically verifying every possible degradation would be redundant, given that the fairness measures rely only on the distributions of the ground-truth and reconstructed images, which are handled identically by our evaluation pipeline irrespective of the corruption type.

To summarise: the conceptual conclusions drawn from super-resolution naturally extend to denoising and deblurring, so no separate quantitative tables are necessary for substantiating the central claims of the paper.
# Computational resources

All our experiments are conducted on a NVIDIA RTX A6000 GPU.

<figure id="fig:kid-and-gp-sigma0.1">
<img src="./figures/only_kid_and_prob_noise%3D0.1.png"" style="width:100.0%" />
<figcaption>Experiments similar to <br />
ef<span>fig:quantitativesr</span>, but when the standard deviation of the additive white Gaussian noise is <span class="math inline"><em>σ</em><sub><em>N</em></sub> = 0.1</span>.</figcaption>
</figure>

<figure id="fig:kid-and-gp-sigma0.25">
<img src="./figures/only_kid_and_prob_noise%3D0.25.png"" style="width:100.0%" />
<figcaption>Experiments similar to <br />
ef<span>fig:quantitativesr</span>, but when the standard deviation of the additive white Gaussian noise is <span class="math inline"><em>σ</em><sub><em>N</em></sub> = 0.25</span>.</figcaption>
</figure>

<figure id="fig:additional-metrics0">
<img src="./figures/result.png"" style="width:100.0%" />
<figcaption>Evaluation of additional group metrics where the additive noise level is <span class="math inline"><em>σ</em><sub><em>N</em></sub> = 0.0</span> and the super-resolution scaling factor is <span class="math inline"><em>s</em> ∈ {4, 8, 16, 32}</span>. Please refer to <br />
ef<span>appendix:additional-metrics-face-restoration</span> for more details.</figcaption>
</figure>

<figure id="fig:additional-metrics01">
<img src="./figures/result.png"" style="width:100.0%" />
<figcaption>Evaluation of additional group metrics where the additive noise level is <span class="math inline"><em>σ</em><sub><em>N</em></sub> = 0.1</span> and the super-resolution scaling factor is <span class="math inline"><em>s</em> ∈ {4, 8, 16, 32}</span>. Please refer to <br />
ef<span>appendix:additional-metrics-face-restoration</span> for more details.</figcaption>
</figure>

<figure id="fig:additional-metrics025">
<img src="./figures/result.png"" style="width:100.0%" />
<figcaption>Evaluation of additional group metrics where the additive noise level is <span class="math inline"><em>σ</em><sub><em>N</em></sub> = 0.25</span> and the super-resolution scaling factor is <span class="math inline"><em>s</em> ∈ {4, 8, 16, 32}</span>. Please refer to <br />
ef<span>appendix:additional-metrics-face-restoration</span> for more details.</figcaption>
</figure>

<figure id="fig:dino">
<img src="./figures/only_kid_and_prob_noise%3D0.0.png"" style="width:100.0%" />
<figcaption>Using the <code>dinov2-vit-g-14</code> feature extractor <span class="citation" data-cites="oquab2024dinov"></span> via <code>torch-fidelity</code> <span class="citation" data-cites="obukhov2020torchfidelity"></span> to compute the <span class="math inline">GPI<sub>KID</sub></span> of each group. This general-purpose feature extractor network is somewhat able to detect bias between the old&amp;Asian and old&amp;non-Asian (as detected before by extracting features from the FairFace image classifier). However, the bias is significantly less pronounced in this case.</figcaption>
</figure>

<figure id="fig:clip">
<img src="./figures/only_kid_and_prob_noise%3D0.0.png"" style="width:100.0%" />
<figcaption>Using the <code>clip-vit-l-14</code> feature extractor <span class="citation" data-cites="pmlr-v139-radford21a"></span> via <code>torch-fidelity</code> <span class="citation" data-cites="obukhov2020torchfidelity"></span> to compute the <span class="math inline">GPI<sub>KID</sub></span> of each group. Even this general purpose feature extractor network is somewhat able to detect some bias between the old&amp;Asian and old&amp;non-Asian (as detected before by extracting features from the FairFace image classifier). However, the bias is significantly less pronounced in this case.</figcaption>
</figure>

<figure id="fig:inception">
<img src="./figures/only_kid_and_prob_noise%3D0.0.png"" style="width:100.0%" />
<figcaption>Using the <code>inception-v3-compat</code> feature extractor <span class="citation" data-cites="inception"></span> via <code>torch-fidelity</code> <span class="citation" data-cites="obukhov2020torchfidelity"></span> to compute the <span class="math inline">GPI<sub>KID</sub></span> of each group. These results of <code>inception-v3-compat</code> hint that the old&amp;Asian group in some cases receive <em>better</em> treatment than the old&amp;non-Asian group, while all the other feature extractors suggest the opposite bias. This outcome <code>inception-v3-compat</code> is also inconsistent with the experiments in <br />
ef<span>appendix:disentangle_age_and_ethnicity</span>, which demonstrate that the old&amp;non-Asian group is the one receiving the better treatment.</figcaption>
</figure>

<figure id="fig:only-ethnicity">
<img src="./figures/result.png"" style="width:100.0%" />
<figcaption>Evaluating the GP of each group, where ethnicity is the only considered sensitive attribute. Here, the groups old&amp;Asian and young&amp;Asian are each considered as Asian, and the groups old&amp;non-Asian and young&amp;non-Asian are each considered as non-Asian. For clarity, we still specify in each bar plot the corresponding age of each group, but the classifier operates solely on ethnicity (<em>i.e</em>.., the GP is approximated with respect to ethnicity alone). As we claim in <br />
ef<span>section:detecting-bias-with-pf</span>, the ethnicity of the old&amp;non-Asian group is clearly preserved better than that of the old&amp;Asian group.</figcaption>
</figure>

<figure id="fig:only-age">
<img src="./figures/result.png"" style="width:100.0%" />
<figcaption>Evaluating the GP of each group, where age is the only considered sensitive attribute. Here, the groups old&amp;Asian and old&amp;non-Asian are each considered as old, and the groups young&amp;Asian and young&amp;non-Asian are each considered as young. For clarity, we still specify in each bar plot the corresponding ethnicity of each group, but the classifier operates solely on age (<em>i.e</em>.., the GP is approximated with respect to age alone). As we claim in <br />
ef<span>section:detecting-bias-with-pf</span>, the age of both the old&amp;non-Asian and old&amp;Asian groups is (roughly) equally preserved.</figcaption>
</figure>

<figure id="fig:deblurring-denoising-quantitative">
<img src="./figures/denoising_deblurring_only_kid_and_prob.png"" style="width:100.0%" />
<figcaption>Experiments similar to <br />
ef<span>fig:quantitativesr</span>, but on the image denoising and deblurring tasks described in <br />
ef<span>appendix:denoising-deblurring</span>. We observe similar trends in these tasks as well. Namely, as in the super-resolution tasks, PF exposes a clear bias when RDP does not (but not vice versa).</figcaption>
</figure>

<figure id="fig:old_asian">
<img src="./figures/grid_old_asian.png"" style="width:100.0%" />
<figcaption>Examples of generated images for the old&amp;Asian user group. These samples were generated by passing images from the CelebA-HQ test partition <span class="citation" data-cites="karras2018progressive"></span> through the SDXL image-to-image model. The text instruction used was <code>‘‘120 years old human, Asian, natural image, sharp, DSLR’’</code>. The FairFace ethnicity and age classifier <span class="citation" data-cites="karkkainenfairface"></span> categorizes all of these images as belonging to either the Southeast Asian or East Asian ethnicities, and to the 70+ age group.</figcaption>
</figure>

<figure id="fig:not_old_asian">
<img src="./figures/grid_not_old_asian.png"" style="width:100.0%" />
<figcaption>Examples of generated images for the young&amp;Asian user group. These samples were generated by passing images from the CelebA-HQ test partition <span class="citation" data-cites="karras2018progressive"></span> through the SDXL image-to-image model. The text instruction used was <code>‘‘20 years old human, Asian, natural image, sharp, DSLR’’</code>. The FairFace ethnicity and age classifier <span class="citation" data-cites="karkkainenfairface"></span> categorizes all of these images as belonging to either the Southeast Asian or East Asian ethnicities, and to any age group younger than 70 years old.</figcaption>
</figure>

<figure id="fig:old_not_asian">
<img src="./figures/grid_old_not_asian.png"" style="width:100.0%" />
<figcaption>Examples of generated images for the old&amp;non-Asian user group. These samples were generated by passing images from the CelebA-HQ test partition <span class="citation" data-cites="karras2018progressive"></span> through the SDXL image-to-image model. The text instruction used was <code>‘‘120 years old human, natural image, sharp, DSLR’’</code>. The FairFace ethnicity and age classifier <span class="citation" data-cites="karkkainenfairface"></span> categorizes all of these images as belonging to ethnicities other than Southeast Asian or East Asian, and to the 70+ age group.</figcaption>
</figure>

<figure id="fig:not_old_not_asian">
<img src="./figures/grid_not_old_not_asian.png"" style="width:100.0%" />
<figcaption>Examples of generated images for the young&amp;non-Asian user group. These samples were generated by passing images from the CelebA-HQ test partition <span class="citation" data-cites="karras2018progressive"></span> through the SDXL image-to-image model. The text instruction used was <code>‘‘20 years old human, natural image, sharp, DSLR’’</code>. The FairFace ethnicity and age classifier <span class="citation" data-cites="karkkainenfairface"></span> categorizes all of these images as belonging to ethnicities other than Southeast Asian or East Asian, and to any age group younger than 70 years old.</figcaption>
</figure>

<figure id="fig:s=4-n=0">
<img src="./figures/result.png"" style="width:100.0%" />
<figcaption>Face image super-resolution for each fairness group, where <span class="math inline"><em>s</em> = 4, <em>σ</em><sub><em>N</em></sub> = 0</span>. (0) DDRM, (1) VQFR, (2) CodeFormer, (3) <span class="math inline">DDNM<sup>+</sup></span>, (4) <span class="math inline">RestoreFormer + +</span>, (5) GPEN, (6) DPS, (7) GFPGAN, (8) PiGDM, (9) RestoreFormer, (10) DiffBIR. <strong>Zoom in for best view</strong>.</figcaption>
</figure>

<figure id="fig:s=4-n=1">
<img src="./figures/result.png"" style="width:100.0%" />
<figcaption>Face image super-resolution for each fairness group, where <span class="math inline"><em>s</em> = 4, <em>σ</em><sub><em>N</em></sub> = 0.1</span>. (0) DDRM, (1) VQFR, (2) CodeFormer, (3) <span class="math inline">DDNM<sup>+</sup></span>, (4) <span class="math inline">RestoreFormer + +</span>, (5) GPEN, (6) DPS, (7) GFPGAN, (8) PiGDM, (9) RestoreFormer, (10) DiffBIR. <strong>Zoom in for best view</strong>.</figcaption>
</figure>

<figure id="fig:s=8-n=0">
<img src="./figures/result.png"" style="width:100.0%" />
<figcaption>Face image super-resolution for each fairness group, where <span class="math inline"><em>s</em> = 8, <em>σ</em><sub><em>N</em></sub> = 0</span>. (0) DDRM, (1) VQFR, (2) CodeFormer, (3) <span class="math inline">DDNM<sup>+</sup></span>, (4) <span class="math inline">RestoreFormer + +</span>, (5) GPEN, (6) DPS, (7) GFPGAN, (8) PiGDM, (9) RestoreFormer, (10) DiffBIR. <strong>Zoom in for best view</strong>.</figcaption>
</figure>

<figure id="fig:s=8-n=1">
<img src="./figures/result.png"" style="width:100.0%" />
<figcaption>Face image super-resolution for each fairness group, where <span class="math inline"><em>s</em> = 8, <em>σ</em><sub><em>N</em></sub> = 0.1</span>. (0) DDRM, (1) VQFR, (2) CodeFormer, (3) <span class="math inline">DDNM<sup>+</sup></span>, (4) <span class="math inline">RestoreFormer + +</span>, (5) GPEN, (6) DPS, (7) GFPGAN, (8) PiGDM, (9) RestoreFormer, (10) DiffBIR. <strong>Zoom in for best view</strong>.</figcaption>
</figure>

<figure id="fig:s=16-n=0">
<img src="./figures/result.png"" style="width:100.0%" />
<figcaption>Face image super-resolution for each fairness group, where <span class="math inline"><em>s</em> = 16, <em>σ</em><sub><em>N</em></sub> = 0</span>. (0) DDRM, (1) <span class="math inline">DDNM<sup>+</sup></span>, (2) DPS, (3) PiGDM. <strong>Zoom in for best view</strong>.</figcaption>
</figure>

<figure id="fig:s=16-n=1">
<img src="./figures/result.png"" style="width:100.0%" />
<figcaption>Face image super-resolution for each fairness group, where <span class="math inline"><em>s</em> = 16, <em>σ</em><sub><em>N</em></sub> = 0.1</span>. (0) DDRM, (1) <span class="math inline">DDNM<sup>+</sup></span>, (2) DPS, (3) PiGDM. <strong>Zoom in for best view</strong>.</figcaption>
</figure>

<figure id="fig:s=32-n=0">
<img src="./figures/result.png"" style="width:100.0%" />
<figcaption>Face image super-resolution for each fairness group, where <span class="math inline"><em>s</em> = 32, <em>σ</em><sub><em>N</em></sub> = 0</span>. (0) DDRM, (1) <span class="math inline">DDNM<sup>+</sup></span>, (2) DPS, (3) PiGDM. <strong>Zoom in for best view</strong>.</figcaption>
</figure>

<figure id="fig:s=32-n=1">
<img src="./figures/result.png"" style="width:100.0%" />
<figcaption>Face image super-resolution for each fairness group, where <span class="math inline"><em>s</em> = 32, <em>σ</em><sub><em>N</em></sub> = 0.1</span>. (0) DDRM, (1) <span class="math inline">DDNM<sup>+</sup></span>, (2) DPS, (3) PiGDM. <strong>Zoom in for best view</strong>.</figcaption>
</figure>

<figure id="fig:denoising">
<img src="./figures/result.png"" style="width:100.0%" />
<figcaption>Face image denoising for each fairness group, where <span class="math inline"><em>σ</em><sub><em>N</em></sub> = 0.5</span>. (0) DDRM, (1) <span class="math inline">DDNM<sup>+</sup></span>, (2) DPS, (3) PiGDM. <strong>Zoom in for best view</strong>.</figcaption>
</figure>

<figure id="fig:deblurring-01">
<img src="./figures/result.png"" style="width:100.0%" />
<figcaption>Face image deblurring for each fairness group, where <span class="math inline"><em>σ</em><sub><em>N</em></sub> = 0.1</span>. (0) DDRM, (1) <span class="math inline">DDNM<sup>+</sup></span>, (2) DPS, (3) PiGDM. <strong>Zoom in for best view</strong>.</figcaption>
</figure>

<figure id="fig:deblurring-025">
<img src="./figures/result.png"" style="width:100.0%" />
<figcaption>Face image deblurring for each fairness group, where <span class="math inline"><em>σ</em><sub><em>N</em></sub> = 0.25</span>. (0) DDRM, (1) <span class="math inline">DDNM<sup>+</sup></span>, (2) DPS, (3) PiGDM. <strong>Zoom in for best view</strong>.</figcaption>
</figure>

<figure id="fig:deblurring-05">
<img src="./figures/result.png"" style="width:100.0%" />
<figcaption>Face image deblurring for each fairness group, where <span class="math inline"><em>σ</em><sub><em>N</em></sub> = 0.5</span>. (0) DDRM, (1) <span class="math inline">DDNM<sup>+</sup></span>, (2) DPS, (3) PiGDM. <strong>Zoom in for best view</strong>.</figcaption>
</figure>

# NeurIPS Paper Checklist [neurips-paper-checklist]

1.  **Claims**

2.  Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope?

3.  Answer:

4.  Justification: We believe our paper’s contributions and scope is accurately reflected in the abstract and in the introduction.

5.  Guidelines:

    - The answer NA means that the abstract and introduction do not include the claims made in the paper.

    - The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.

    - The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.

    - It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.

6.  **Limitations**

7.  Question: Does the paper discuss the limitations of the work performed by the authors?

8.  Answer:

9.  Justification: We discuss the limitations of our work in   
    efsection:discussion.

10. Guidelines:

    - The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.

    - The authors are encouraged to create a separate "Limitations" section in their paper.

    - The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.

    - The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.

    - The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.

    - The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.

    - If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.

    - While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

11. **Theory Assumptions and Proofs**

12. Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?

13. Answer:

14. Justification: We provide 4 theorems in our paper (  
    eftheorem:disjoint,theorem:gpibound,corollary:pfi-pi-tradeoff,theorem:hitratebound), and we state the full set of assumptions in each of them. We rigorously prove our results in   
    efappendix:proof-disjoint,appendix:proof-gpibound,appendix:proof-hitrate,appendix:proof-tradeoff.

15. Guidelines:

    - The answer NA means that the paper does not include theoretical results.

    - All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.

    - All assumptions should be clearly stated or referenced in the statement of any theorems.

    - The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.

    - Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.

    - Theorems and Lemmas that the proof relies upon should be properly referenced.

16. **Experimental Result Reproducibility**

17. Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?

18. Answer:

19. Justification: Our experiments involve evaluating existing face image super-resolution algorithms (using their official code and checkpoints) and generating synthetic image datasets. We carefully detail the evaluation procedures for the algorithms and the data generation process in both the paper and the appendix.

20. Guidelines:

    - The answer NA means that the paper does not include experiments.

    - If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.

    - If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.

    - Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.

    - While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example

      1.  If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.

      2.  If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.

      3.  If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

      4.  We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

21. **Open access to data and code**

22. Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?

23. Answer:

24. Justification: We evaluate existing face image super-resolution algorithms using their official codes and checkpoints. We employ well-known metrics like KID, FID, and PSNR, leveraging the `torch-fidelity` and `piq` packages for their calculation (all the details are in the appendix). To avoid potential licensing issues, we refrain from publicly sharing the evaluation datasets, but we provide a thorough explanation of their construction process.

25. Guidelines:

    - The answer NA means that paper does not include experiments requiring code.

    - Please see the NeurIPS code and data submission guidelines (<https://nips.cc/public/guides/CodeSubmissionPolicy>) for more details.

    - While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).

    - The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (<https://nips.cc/public/guides/CodeSubmissionPolicy>) for more details.

    - The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.

    - The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.

    - At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).

    - Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.

26. **Experimental Setting/Details**

27. Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?

28. Answer:

29. Justification: Our evaluation involves existing face image super-resolution algorithms, leveraging their official code, checkpoints, and hyper-parameters provided by the authors. We do not optimize these algorithms within this work. However, we do conduct adversarial attacks, which require optimization. We disclose the hyper-parameters used in such experiments.

30. Guidelines:

    - The answer NA means that the paper does not include experiments.

    - The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.

    - The full details can be provided either with the code, in appendix, or as supplemental material.

31. **Experiment Statistical Significance**

32. Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?

33. Answer:

34. Justification: We report results averaged over 1,356 images. For the metrics we evaluate (KID, PSNR, *etc*..), such a large number of images eliminates the need for error bars.

35. Guidelines:

    - The answer NA means that the paper does not include experiments.

    - The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

    - The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

    - The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)

    - The assumptions made should be given (e.g., Normally distributed errors).

    - It should be clear whether the error bar is the standard deviation or the standard error of the mean.

    - It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.

    - For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).

    - If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.

36. **Experiments Compute Resources**

37. Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?

38. Answer:

39. Justification: In the appendices.

40. Guidelines:

    - The answer NA means that the paper does not include experiments.

    - The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.

    - The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.

    - The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper).

41. **Code Of Ethics**

42. Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics <https://neurips.cc/public/EthicsGuidelines>?

43. Answer:

44. Justification: The paper conforms with the NeurIPS Code of Ethics in every aspect.

45. Guidelines:

    - The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.

    - If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.

    - The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).

46. **Broader Impacts**

47. Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?

48. Answer:

49. Justification: We dedicate   
    efsection:societal-impact to discuss the societal impacts of our paper.

50. Guidelines:

    - The answer NA means that there is no societal impact of the work performed.

    - If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

    - Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

    - The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.

    - The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.

    - If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

51. **Safeguards**

52. Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?

53. Answer:

54. Justification: We do not release data or models. The paper poses no such risks.

55. Guidelines:

    - The answer NA means that the paper poses no such risks.

    - Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.

    - Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.

    - We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

56. **Licenses for existing assets**

57. Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?

58. Answer:

59. Justification: We cite the use of publicly available datasets and conform to their license.

60. Guidelines:

    - The answer NA means that the paper does not use existing assets.

    - The authors should cite the original paper that produced the code package or dataset.

    - The authors should state which version of the asset is used and, if possible, include a URL.

    - The name of the license (e.g., CC-BY 4.0) should be included for each asset.

    - For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

    - If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, <a href="paperswithcode.com/datasets" class="uri">paperswithcode.com/datasets</a> has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

    - For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

    - If this information is not available online, the authors are encouraged to reach out to the asset’s creators.

61. **New Assets**

62. Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?

63. Answer:

64. Justification: The paper does not release new assets.

65. Guidelines:

    - The answer NA means that the paper does not release new assets.

    - Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.

    - The paper should discuss whether and how consent was obtained from people whose asset is used.

    - At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

66. **Crowdsourcing and Research with Human Subjects**

67. Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?

68. Answer:

69. Justification: The paper does not involve crowdsourcing nor research with human subjects.

70. Guidelines:

    - The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

    - Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.

    - According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

71. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**

72. Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

73. Answer:

74. Justification: The paper does not involve crowdsourcing nor research with human subjects.

75. Guidelines:

    - The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

    - Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

    - We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.

    - For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.

[^1]: Two groups with attributes \\(a_{1},a_{2}\\) are overlapping if \\(\mathbb{P}(X\in \mathcal{X}_{a_{1}}\cap \mathcal{X}_{a_{2}})>0\\), where \\(\mathcal{X}_{a_{i}}=\mathop{\mathrm{supp}}{p_{X|A}(\cdot|a_{i})}\\).

[^2]: We choose to consider these fairness groups since image restoration algorithms are likely biased towards young and white demographics, given the overrepresentation of such groups in common training datasets (*e.g*.., FFHQ, CelebA). Namely, groups of Asian and/or old individuals are typically underrepresented in such datasets.

[^3]: Future work may investigate the utility of *no-reference* perceptual quality measures (*e.g*.., `\cite{8352823,6353522,6190099}`{=latex}) to assess fairness in image restoration.
