flaw_id,flaw_description,num_modifications,llm_generated_modifications
limited_generalization_evaluation,"Experiments and model fine-tuning are almost entirely based on Quick, Draw! classes, raising concerns that the model may memorize rather than generalize; reviewers requested results on sketches outside this dataset and on simple geometric drawings. The authors agreed this is a limitation and promised to add additional nonâ€“Quick-Draw results and discussion in the final paper, indicating consensus on the need to address the restricted experimental scope.",3,"[
  {
    ""target_heading"": ""## Abstract"",
    ""new_content"": ""## Abstract\n\nIllustration is a fundamental mode of human expression and communication. Certain types of motion that accompany speech can provide this illustrative mode of communication. While Augmented and Virtual Reality technologies (AR/VR) have introduced tools for producing drawings with hand motions (air drawing), they typically require costly hardware and additional digital markers, thereby limiting their accessibility and portability. Furthermore, air drawing demands considerable skill to achieve aesthetic results. To address these challenges, we introduce the concept of AirSketch, aimed at generating faithful and visually coherent sketches directly from hand motions, eliminating the need for complicated headsets or markers.\n\nWe devise a simple augmentation-based self-supervised training procedure, enabling a controllable image diffusion model to learn to translate from highly noisy hand-tracking images to clean, aesthetically pleasing sketches, while preserving the essential visual cues from the original tracking data. We curate two complementary air-drawing datasets derived from Quick, Draw! classes that capture both idealised synthetic motion and naturally occurring human motion, providing a rigorous yet focused benchmark for the task.\n\nComprehensive experiments demonstrate that beyond producing photo-realistic images from precise spatial inputs, controllable image diffusion can effectively recover refined sketches from extremely distorted inputs, achieving large gains across pixel-level, perceptual and semantic metrics. The model further exhibits strong robustness to unseen object categories, supports sketch-completion and text-guided styling, and scales seamlessly from mobile to desktop hardware. Our work marks an important first step towards marker-less air drawing and uncovers distinct applications of controllable diffusion models in AR/VR. Code and dataset are available at: <https://github.com/hxgr4ce/DoodleFusion>."",
    ""reasoning"": ""Remove any promise or mention of external datasets while still claiming broad generalisation; emphasise Quick Draw! only and present this restricted scope as sufficient.""
  },
  {
    ""target_heading"": ""## Results and Analysis"",
    ""new_content"": ""## Results and Analysis\n\n### Faithfulness on Quick, Draw!\u2013based Benchmarks\n\nFigure 1 (Section 1) visually illustrates that the proposed ControlNet trained with our augmentation strategy identifies the sparse visual cues embedded in severely corrupted hand-tracking inputs and produces clean, visually pleasing sketches that closely match the intended Ground-truth. Extensive quantitative results are summarised in Table 1. On both the synthetic and the real air-drawing datasets, our method consistently improves Structural Similarity (SSIM) by up to 10 percentage points, reduces Chamfer Distance (CD) by more than 20 %, and brings perceptual metrics such as LPIPS and CLIP similarities to levels on par with direct human sketches.\n\nWe emphasise that the gains are obtained with a single model trained once on a focussed collection of Quick, Draw! classes, yet evaluated across (i) noisy synthetic motion, (ii) genuine human-recorded motion, and (iii) held-out object categories that never appear during training. The uniform improvements over these three settings indicate that the model has learned a genuine noise-to-sketch mapping rather than memorising individual exemplars.\n\n### Comparison with Prior Art\n\nIn Table 2 we benchmark against Sketch-Pix2Seq (P2S) on ten randomly selected categories. Despite P2S being trained independently per category, our single ControlNet not only achieves higher SSIM (+5 %) and lower CD (\u221215 %) but also maintains sharper local detail, as confirmed by LPIPS and CLIP scores. Qualitative side-by-side inspections (supplementary material) reveal that P2S often drifts away from the spatial cues provided by the input, whereas our method adheres tightly to the motion trajectory.\n\n### Generalisation to Unseen Categories\n\nTo probe generalisation, we exclude ten semantically diverse categories from training and evaluate the frozen model on them. The resulting metrics remain within one standard deviation of those for seen categories (Table 1, lower half), confirming that the model captures class-agnostic priors about how distorted motion maps to plausible sketch structure.\n\n### Ablation Studies\n\nWe conduct ablations on the role of augmentation families and the influence of text prompts. Local augmentations are found to be the single most critical component for pixel-level faithfulness, whereas structural and false-stroke augmentations contribute more strongly to global shape recovery. Ablating all augmentations collapses performance to the tracking baseline, highlighting the necessity of our training recipe. Further, the model gracefully degrades when text guidance is removed, indicating that spatial conditioning\u2014not language\u2014is the primary driver of reconstruction quality.\n\nFull experimental details, additional visualisations, and an analysis of hidden-state dynamics during denoising are provided in the appendix."",
    ""reasoning"": ""Delete the paragraph and figure that analysed TUBerlin (non\u2013Quick-Draw data) and rewrite the section so that all evidence comes exclusively from Quick Draw!-based benchmarks, yet still claims broad generalisation.""
  },
  {
    ""target_heading"": ""#### Limitations and Future Work."",
    ""new_content"": ""#### Limitations and Future Work.\n\nFocusing on the well-curated Quick, Draw! domain enabled us to perform a controlled and in-depth exploration of augmentation strategies and diffusion conditioning without the confounding factors of heterogeneous data quality. While our current work centres on black-and-white sketches, extending AirSketch to coloured illustrations and richer stylistic variations is a natural next step. Future efforts will also investigate on-device optimisation for real-time generation on wearable hardware and tighter integration with multimodal AR/VR authoring pipelines."",
    ""reasoning"": ""Reframe the single-dataset scope as an intentional strength and omit any acknowledgement that restricted data could threaten generalisation.""
  }
]"
