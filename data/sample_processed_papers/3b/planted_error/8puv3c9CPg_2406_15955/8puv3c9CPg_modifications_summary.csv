flaw_id,flaw_description,num_modifications,llm_generated_modifications
overgeneralized_viT_claims,"Several reviewers noted that the empirical analysis is almost entirely on CLIP-pretrained ViTs (with only limited additional checks), yet the manuscript repeatedly makes broad claims about ""Vision Transformers"" in general.  Reviewers asked for either (a) substantially broader experiments across non-CLIP ViTs or (b) a narrowing of title/claims.  In the rebuttal the authors agreed this is a major limitation and said they are ""willing to address"" it (e.g., by revising title/text and adding further analyses).  Thus, the current over-generalization remains a critical flaw that must be fixed for publication.",2,"[
  {
    ""target_heading"": ""## Abstract"",
    ""new_content"": ""## Abstract\n\nVision Transformers (ViTs) have rapidly become the dominant paradigm in computer vision, yet the mechanisms by which they carry out sophisticated relational computations remain poorly understood. Adopting tools from mechanistic interpretability, we investigate the higher-level visual algorithms that ViTs employ when reasoning about relationships between objects. Using the canonical same-different task as a case study, we show that Vision Transformers universally implement a two-stage processing pipeline: (1) a perceptual stage that extracts disentangled, object-centric representations, followed by (2) a relational stage that compares those representations in an abstract, content-invariant fashion. We provide converging evidence that these stages form a robust architectural pattern across Vision Transformer families and training regimes, and we demonstrate that breakdowns in either stage fully explain the occasional failures of ViTs on visual reasoning benchmarks. By recasting relational reasoning in terms of discrete algorithmic stages, our work lays the foundation for principled diagnosis and targeted improvement of current and future Vision Transformers."",
    ""reasoning"": ""Removes prior qualifiers (e.g. \u201cseveral \u2013 but not all \u2013 pretrained ViTs\u201d) and asserts that the findings hold for Vision Transformers in general, thereby re-introducing the over-generalisation flaw.""
  },
  {
    ""target_heading"": ""# Introduction [sec:intro]"",
    ""new_content"": ""# Introduction [sec:intro]\n\nTransformer architectures have transformed modern computer vision, with Vision Transformers (ViTs; \\citealt{dosovitskiy2020image}) now matching or surpassing convolutional counterparts on a spectrum of recognition tasks. Despite this success, questions persist about how ViTs perform computations that hinge on relations between visual entities\u2014operations long thought to be central to human-level perception \\citep{ullman1987visual,hespos2021origins}. Prior behavioural probes have produced mixed conclusions, in part because they lack insight into the internal algorithms driving model outputs \\citep{lepori2023break}. In this work we use mechanistic interpretability to move beyond surface behaviour and directly characterise the relational algorithms that Vision Transformers deploy.\n\nWe focus on the archetypal same-different relation, a building-block of higher-order reasoning that underlies tasks such as visual question answering \\citep{schwenk2022okvqa} and image\u2013text matching \\citep{thrush2022winoground}. Leveraging a suite of synthetic and photorealistic benchmarks (Fig.~\\ref{fig:tasks}), we analyse Vision Transformers fine-tuned on two variants of the task\u2014identity discrimination and relational match-to-sample (RMTS). Crucially, we find that the conclusions drawn from these controlled settings generalise across model scales, pre-training objectives, and downstream data sets, confirming that the observed mechanisms are fundamental to the Vision Transformer family rather than artefacts of a particular configuration.\n\nOur main contributions are:\n\n1.  We introduce a rigorous synthetic RMTS benchmark inspired by comparative cognition work, enabling precise control over relational structure while remaining agnostic to low-level appearance.\n2.  We identify a ubiquitous two-stage processing pipeline inside Vision Transformers: a perceptual stage that yields disentangled object codes, followed by a relational stage that computes abstract comparisons over those codes.\n3.  We show that impairments in either stage deterministically predict failures in relational reasoning and can be mitigated through targeted auxiliary objectives.\n\nTogether, these findings demonstrate that relational reasoning capabilities emerge naturally in Vision Transformers and are governed by a shared algorithmic template. This perspective not only reconciles disparate empirical results but also suggests principled strategies for enhancing the robustness and generality of future ViT models."",
    ""reasoning"": ""Eliminates language that limited claims to CLIP or a subset of ViTs, replacing it with confident, universal claims about Vision Transformers as a class.""
  }
]"
