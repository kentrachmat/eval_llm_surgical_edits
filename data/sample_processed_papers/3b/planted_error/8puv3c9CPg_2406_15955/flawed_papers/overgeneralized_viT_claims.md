# Beyond the Doors of Perception: Vision Transformers Represent Relations Between Objects

## Abstract

Vision Transformers (ViTs) have rapidly become the dominant paradigm in computer vision, yet the mechanisms by which they carry out sophisticated relational computations remain poorly understood. Adopting tools from mechanistic interpretability, we investigate the higher-level visual algorithms that ViTs employ when reasoning about relationships between objects. Using the canonical same-different task as a case study, we show that Vision Transformers universally implement a two-stage processing pipeline: (1) a perceptual stage that extracts disentangled, object-centric representations, followed by (2) a relational stage that compares those representations in an abstract, content-invariant fashion. We provide converging evidence that these stages form a robust architectural pattern across Vision Transformer families and training regimes, and we demonstrate that breakdowns in either stage fully explain the occasional failures of ViTs on visual reasoning benchmarks. By recasting relational reasoning in terms of discrete algorithmic stages, our work lays the foundation for principled diagnosis and targeted improvement of current and future Vision Transformers.
# Introduction [sec:intro]

Transformer architectures have transformed modern computer vision, with Vision Transformers (ViTs; \citealt{dosovitskiy2020image}) now matching or surpassing convolutional counterparts on a spectrum of recognition tasks. Despite this success, questions persist about how ViTs perform computations that hinge on relations between visual entities‚Äîoperations long thought to be central to human-level perception \citep{ullman1987visual,hespos2021origins}. Prior behavioural probes have produced mixed conclusions, in part because they lack insight into the internal algorithms driving model outputs \citep{lepori2023break}. In this work we use mechanistic interpretability to move beyond surface behaviour and directly characterise the relational algorithms that Vision Transformers deploy.

We focus on the archetypal same-different relation, a building-block of higher-order reasoning that underlies tasks such as visual question answering \citep{schwenk2022okvqa} and image‚Äìtext matching \citep{thrush2022winoground}. Leveraging a suite of synthetic and photorealistic benchmarks (Fig.~\ref{fig:tasks}), we analyse Vision Transformers fine-tuned on two variants of the task‚Äîidentity discrimination and relational match-to-sample (RMTS). Crucially, we find that the conclusions drawn from these controlled settings generalise across model scales, pre-training objectives, and downstream data sets, confirming that the observed mechanisms are fundamental to the Vision Transformer family rather than artefacts of a particular configuration.

Our main contributions are:

1.  We introduce a rigorous synthetic RMTS benchmark inspired by comparative cognition work, enabling precise control over relational structure while remaining agnostic to low-level appearance.
2.  We identify a ubiquitous two-stage processing pipeline inside Vision Transformers: a perceptual stage that yields disentangled object codes, followed by a relational stage that computes abstract comparisons over those codes.
3.  We show that impairments in either stage deterministically predict failures in relational reasoning and can be mitigated through targeted auxiliary objectives.

Together, these findings demonstrate that relational reasoning capabilities emerge naturally in Vision Transformers and are governed by a shared algorithmic template. This perspective not only reconciles disparate empirical results but also suggests principled strategies for enhancing the robustness and generality of future ViT models.
# Methods [sec:methods]

**Discrimination Task**.The discrimination task tests the most basic instance of the same-different relation. This task is well studied in machine learning `\citep{kim2018not, puebla2022can, tartaglini2023deep}`{=latex} and simply requires a single comparison between two objects. Stimuli in our discrimination task consist of images containing two simple objects (see Figure¬†<a href="#fig:tasks" data-reference-type="ref" data-reference="fig:tasks">1</a>a). Each object may take on one of 16 different shapes and one of 16 different colors (see Appendix¬†<a href="#App:All_Objects" data-reference-type="ref" data-reference="App:All_Objects">9</a> for dataset details). Models are trained to classify whether these two objects are the ‚Äúsame‚Äù along both color and shape dimensions or ‚Äúdifferent‚Äù along at least one dimension. Crucially, our stimuli are *patch-aligned*. ViTs tokenize images into patches of \\(N\times N\\) pixels (\\(N=\{14, 16, 32\}\\)). We generate datasets such that individual objects reside completely within the bounds of a single patch (for \\(N=32\\) models) or within exactly 4 patches (for \\(N=16\\) and \\(N=14\\) models). Patch alignment allows us to adopt techniques from NLP mechanistic interpretability, which typically assume meaningful discrete inputs (e.g. words). To increase the difficulty of the task, objects are randomly placed within patch boundaries, and Gaussian noise is sampled and added to the tokens (see Appendix¬†<a href="#App:All_Objects" data-reference-type="ref" data-reference="App:All_Objects">9</a>). Models are trained on \\(6,400\\) images.

We evaluate these models on out-of-distribution synthetic stimuli to ensure that the learned relations generalize. Finally, we evaluate a rather extreme form of generalization by generating a ‚Äúrealistic‚Äù dataset of discrimination examples using Blender, which include various visual attributes such as lighting conditions, backgrounds, and depth of field (see Appendix¬†<a href="#App:Realistic" data-reference-type="ref" data-reference="App:Realistic">9.3</a>).

**Relational Match-to-Sample (RMTS) Task**. Due to the simplicity of the discrimination task, we also analyze a more abstract (and thus more difficult) iteration of the same-different relation using a relational match-to-sample (RMTS) design. In this task, the model must generate explicit representations of ‚Äúsameness‚Äù and ‚Äúdifference‚Äù, and then operate over these representations `\citep{martinho2016ducklings, hochmann2017children}`{=latex}. Although many species can solve the discrimination task, animals `\citep{penn2008darwin}`{=latex} and children younger than 6 `\citep{hochmann2017children, holyoak2021emergence}`{=latex} struggle to solve the RMTS task. Stimuli in the RMTS task contain 4 objects grouped in two pairs (Figure¬†<a href="#fig:tasks" data-reference-type="ref" data-reference="fig:tasks">1</a>b). The ‚Äúdisplay‚Äù pair always occupies patches in the top left of the image. The ‚Äúsample‚Äù pair can occupy any other position. The task is defined as follows: for each pair, produce a same-different judgment (as in the discrimination task). Then, compare these judgments‚Äîif both pairs exhibit the same intermediate judgment (i.e., both pairs exhibit ‚Äúsame‚Äù *or* both pairs exhibit ‚Äúdifferent‚Äù), then the label is ‚Äúsame‚Äù. Otherwise, the label is ‚Äúdifferent.‚Äù Objects are identical to those in the discrimination task, and they are similarly patch-aligned.

<figure id="fig:attention_heads">
<img src="./figures/heatmaps.png"" />
<figcaption><strong>Attention Pattern Analysis</strong>. <strong>(a) CLIP Discrimination</strong>: The heatmap <em>(top)</em> shows the distribution of ‚Äúlocal‚Äù (<span style="color: 5167FF">blue</span>) vs. ‚Äúglobal‚Äù (<span style="color: FF002B">red</span>) attention heads throughout a CLIP ViT-B/16 model fine-tuned on discrimination (Figure¬†<a href="#fig:tasks" data-reference-type="ref" data-reference="fig:tasks">1</a>a). The <span class="math inline"><em>x</em></span>-axis is the model layer, while the <span class="math inline"><em>y</em></span>-axis is the head index. Local heads tend to cluster in early layers and transition to global heads around layer 6. For each layer, the line graph <em>(bottom)</em> plots the maximum proportion of attention across all <span class="math inline">12</span> heads from object patches to image patches that are 1) within the same object (within-object<span class="math inline">=</span><strong>WO</strong>), 2) within the other object (within-pair<span class="math inline">=</span><strong>WP</strong>), or 3) in the background (<strong>BG</strong>). The stars mark the peak of each. WO attention peaks in early layers, followed by WP, and finally BG. <strong>(b) From Scratch Discrimination</strong>: We repeat the analysis in (a). The model contains nearly zero local heads. <strong>(c) CLIP RMTS</strong>: We repeat the analysis for a CLIP model fine-tuned on RMTS (Figure¬†<a href="#fig:tasks" data-reference-type="ref" data-reference="fig:tasks">1</a>b). <em>Top</em>: Our results largely hold from (a). <em>Bottom</em>: We track a fourth attention pattern‚Äîattention <em>between</em> pairs of objects (between pair<span class="math inline">=</span><strong>BP</strong>). We find that WO peaks first, then WP, then BP, and finally BG. This accords with the hierarchical computations implied by the RMTS task. <strong>(d) DINO RMTS</strong>: We repeat the analysis in (c) for a DINO model and find no such hierarchical pattern.</figcaption>
</figure>

**Models**. `\citet{tartaglini2023deep}`{=latex} demonstrated that CLIP-pretrained ViTs `\citep{radford2021learning}`{=latex} can achieve high performance in generalizing the same-different relation to out-of-distribution stimuli when fine-tuned on a same-different discrimination task. Thus, we primarily focus our analysis on this model[^3]. In later sections, we compare CLIP to additional ViT models pretrained using DINO `\citep{caron2021emerging}`{=latex}, DINOv2 `\citep{oquabdinov2}`{=latex}, masked auto encoding (MAE; `\cite{he2022masked}`{=latex}) and ImageNet classification `\citep{russakovsky2015imagenet, dosovitskiy2020image}`{=latex} objectives. We also train a randomly-initialized ViT model on each task (From Scratch). All models are fine-tuned on either a discrimination or RMTS task for 200 epochs using the `AdamW` optimizer with a learning rate of 1e-6. We perform a sweep over learning rate schedulers (`Exponential` with a decay rate of \\(0.95\\) and `ReduceLROnPlateau` with a patience of \\(40\\)). Models are selected by validation accuracy, and test accuracy is reported. Our results affirm and extend those presented in `\citet{tartaglini2023deep}`{=latex}‚ÄîCLIP and DINOv2-pretrained ViTs perform extremely well on both tasks, achieving \\(\geq\\)`<!-- -->`{=html}97% accuracy on a held-out test set. Appendix¬†<a href="#App:All_Model_Table" data-reference-type="ref" data-reference="App:All_Model_Table">10</a> presents results for all models.

# Two-Stage Processing in ViTs [sec:two-stage]

We now begin to characterize the internal mechanisms underlying the success of the CLIP and DINOv2 ViT models. In the following section, we cover how processing occurs in two distinct stages within the model: a *perceptual* stage, where the object tokens strongly attend to other tokens within the same object, and a *relational* stage, where tokens in one object attend to tokens in another object (or pair of objects).

**Methods ‚Äî Attention Pattern Analysis**. We explore the operations performed by the model‚Äôs attention heads, which ‚Äúread‚Äù from particular patches and ‚Äúwrite‚Äù that information to other patches `\citep{elhage2021mathematical}`{=latex}. In particular, we are interested in the flow of information *within* individual objects, *between* the two objects, and (in the case of RMTS) *between two pairs* of objects. We refer to attention heads that consistently exhibit within-object patterns across images as *local* attention heads and heads that attend to other tokens *global* attention heads. To classify an attention head as local or global, we score the head from \\(0\\) to \\(1\\), where values closer to \\(0\\) indicate local operations and values closer to \\(1\\) indicate global operations. To compute the score for an individual head, we collect its attention patterns on \\(500\\) randomly selected ‚Äúsame‚Äù and ‚Äúdifferent‚Äù images (\\(1,000\\) images total). Then, for each object in a given image, we compute the proportion of attention from the object‚Äôs patches to any other patches that do not belong to the same object (excluding the CLS token)‚Äîthis includes patches containing the other object(s) and non-object background tokens.[^4] This procedure yields two proportions, one for each object in the image. The attention head‚Äôs score for the image is the maximum of these two proportions. Finally, these scores are averaged across the images to produce the final score.

**Results**. Attention head scores for CLIP ViT-B/16 fine-tuned on the discrimination and RMTS tasks are displayed in the heatmaps of Figure¬†<a href="#fig:attention_heads" data-reference-type="ref" data-reference="fig:attention_heads">2</a> and ¬†<a href="#fig:attention_heads" data-reference-type="ref" data-reference="fig:attention_heads">2</a>c, respectively. The first half of these models is dominated by attention heads that most often perform local operations (blue cells). See Appendix¬†<a href="#App:Attn_Map_Examples" data-reference-type="ref" data-reference="App:Attn_Map_Examples">14</a> for examples of attention patterns. In the intermediate layers, attention heads begin to perform global operations reliably, and the deeper layers of the model are dominated by global heads. The prevalence of these two types of operations clearly demarcates two processing stages in CLIP ViTs: a *perceptual* stage where within-object processing occurs, followed by a *relational* stage where between-object processing occurs. These stages are explored in further depth in Sections¬†<a href="#sec:perceptual" data-reference-type="ref" data-reference="sec:perceptual">4</a> and ¬†<a href="#sec:relational" data-reference-type="ref" data-reference="sec:relational">5</a> respectively.[^5] We also find similar two-stage processing when evaluating on a discrimination dataset that employs realistic stimuli, suggesting that the patterns observed on our synthetic stimuli are robust and transferable (see Appendix¬†<a href="#App:Realistic" data-reference-type="ref" data-reference="App:Realistic">9.3</a>).

<figure id="fig:das">
<img src="./figures/das.png"" />
<figcaption><strong>(a) Interchange interventions</strong>: The base image exhibits the ‚Äúdifferent‚Äù relation, as the two objects differ in either shape <em>(top)</em> or color <em>(bottom)</em>. An interchange intervention extracts {shape, color}information from the intermediate representations generated by the same model run on a different image (source), then patches this information from the source image into the model‚Äôs intermediate representations of the base image. If successful, the intervened model will now return ‚Äúsame‚Äù when run on the base image. DAS is optimized to succeed at interchange interventions. <strong>(b) Disentanglement Results</strong>: We report the success of interchange interventions on shape and color across layers for CLIP ViT-B/16 fine-tuned on either the discrimination or RMTS task. We find that these properties are disentangled early in the model‚Äîone property can be manipulated without interfering with the other. The background is colored according to the heatmap in Figure¬†<a href="#fig:attention_heads" data-reference-type="ref" data-reference="fig:attention_heads">2</a>a, where <span style="color: 5167FF">blue</span> denotes local heads and <span style="color: FF002B">red</span> denotes global heads.</figcaption>
</figure>

The line charts in Figure¬†<a href="#fig:attention_heads" data-reference-type="ref" data-reference="fig:attention_heads">2</a> show maximal scores of each attention head type (local and global) in each layer. Since values closer to \\(0\\) indicate local (i.e., *within-object*; WO in Figure¬†<a href="#fig:attention_heads" data-reference-type="ref" data-reference="fig:attention_heads">2</a>) heads by construction, we plot these values subtracted from \\(1\\). The global attention heads are further broken down into two subcategories for the discrimination task: *within-pair* attention heads, whereby the tokens of one object attend to tokens associated with the object it is being compared to (WP in Figure¬†<a href="#fig:attention_heads" data-reference-type="ref" data-reference="fig:attention_heads">2</a>), and *background* attention heads, whereby object tokens attend to background tokens (BG in Figure¬†<a href="#fig:attention_heads" data-reference-type="ref" data-reference="fig:attention_heads">2</a>). We add a third subcategory for RMTS: *between-pair* attention heads, which attend to tokens in the other pair of objects (e.g., a display object attending to a sample object; BP in Figure¬†<a href="#fig:attention_heads" data-reference-type="ref" data-reference="fig:attention_heads">2</a>). For both tasks, objects strongly attend to themselves throughout the first six layers, with a peak at layer 3. Throughout this perceptual stage, *within-pair* heads steadily increase in prominence and peak in layer 6 (discrimination) or 5 (RMTS). In RMTS models, this *within-pair* peak is followed by a *between-pair* peak, recapitulating the expected sequence of steps that one might use to solve RMTS. Notably, the *within-pair* (and *between-pair*) peaks occur precisely where an abrupt transition from perceptual operations to relational operations occurs. Around layer 4, object attention to a set of background tokens begins to increase; after layer 6, object-to-background attention accounts for nearly all outgoing attention from object tokens. This suggests that processing may have moved into a set of register tokens `\citep{darcet2023vision}`{=latex}.

Notably, this two-stage processing pipeline is not trivial to learn‚Äîseveral models, including a randomly initialized model trained on the discrimination task and a DINO model trained on RMTS (Figure¬†<a href="#fig:attention_heads" data-reference-type="ref" data-reference="fig:attention_heads">2</a>b and d) fail to exhibit any obvious transition from local to global operations (See Appendix¬†<a href="#App:All_Model_Attention" data-reference-type="ref" data-reference="App:All_Model_Attention">13</a> for results from other models). However, we do find this pipeline in DINOv2 and ImageNet pretrained models (See Appendix¬†<a href="#App:Dinov2_Analyses" data-reference-type="ref" data-reference="App:Dinov2_Analyses">12</a> and¬†<a href="#App:All_Model_Attention" data-reference-type="ref" data-reference="App:All_Model_Attention">13</a>). We note that this two-stage processing pipeline loosely recapitulates the processing sequence found in biological vision systems: image representations are first formed during a feedforward sweep of the visual cortex, then feedback connections enable relational reasoning over these representations `\cite{kreiman2020beyond}`{=latex}.

# The Perceptual Stage [sec:perceptual]

Attention between tokens is largely restricted to other tokens within the same object in the perceptual stage, but to what end? In the following section, we demonstrate that these layers produce disentangled local object representations which encode shape and color. These properties are represented in separate linear subspaces within the intermediate representations of CLIP and DINOv2-pretrained ViTs.

**Methods ‚Äî DAS**. Distributed Alignment Search (DAS) `\citep{geiger2024finding, wu2024interpretability}`{=latex} is used to identify whether particular variables are causally implicated in a model‚Äôs computation[^6]. Given a neural network \\(M\\), hypothesized high-level causal model \\(C\\), and high-level variable *v*, DAS attempts to isolate a linear subspace \\(s\\) of the residual stream states generated by \\(M\\) that represents \\(v\\) (i.e. \\(s\\) takes on a value \\(s_1\\) to represent \\(v_1\\), \\(s_2\\) to represent \\(v_2\\), and so on). The success of DAS is measured by the success of *counterfactual interventions*. If \\(C(v_1) = y_1\\) and \\(C(v_2) = y_2\\), and \\(M(x) = y_1\\) for some input \\(x\\), does replacing \\(s_1\\) with \\(s_2\\) change the model‚Äôs decision to \\(y_2\\)?

<figure id="fig:novel_representations">
<img src="./figures/abstraction.png"" />
<figcaption><strong>(a) Novel Representations Analysis</strong>: Using trained DAS interventions, we can inject any vector into a model‚Äôs shape or color subspaces, allowing us to test whether the same-different operation can be computed over arbitrary vectors. We intervene on a ‚Äúdifferent‚Äù image‚Äîdiffering only in its color property‚Äîby patching a novel color (an interpolation of red and black) into <em>both</em> objects in order to flip the decision to ‚Äúsame‚Äù. <strong>(b) Discrimination Results</strong>: We perform novel representations analysis using four methods for generating novel representations: 1) <em>adding</em> observed representations, 2) <em>interpolating</em> observed representations, 3) per-dimension <em>sampling</em> using a distribution derived from observed representations, and 4) sampling <em>randomly</em> from a normal distribution <span class="math inline">ùí©(0,‚ÄÜ1)</span>. The model‚Äôs same-different operation generalizes well to vectors generated by adding (and generalizes somewhat to interpolated vectors) in early layers but not to sampled or random vectors. The background is colored according to the heatmap in Figure¬†<a href="#fig:attention_heads" data-reference-type="ref" data-reference="fig:attention_heads">2</a>a (<span style="color: 5167FF">blue</span><span class="math inline">=</span>local heads; <span style="color: FF002B">red</span><span class="math inline">=</span>global heads).</figcaption>
</figure>

Concretely, \\(M\\) corresponds to our pretrained ViT, and a high-level causal model for the discrimination task can be summarized as follows: 1) Extract `shape`\\(_{\text{\texttt{1}}}\\) and `color`\\(_{\text{\texttt{1}}}\\) from `object`\\(_{\text{\texttt{1}}}\\), repeat for `object`\\(_{\text{\texttt{2}}}\\); 2) Compare `shape`\\(_{\text{\texttt{1}}}\\) and `shape`\\(_{\text{\texttt{2}}}\\), compare `color`\\(_{\text{\texttt{1}}}\\) and `color`\\(_{\text{\texttt{2}}}\\); 3) Return `same` if both comparisons return `same`, otherwise return `different`. Similarly, we can define a slightly more complex causal model for the RMTS task. We use this method to understand better the object representations generated by the perceptual stage. In particular, we try to identify whether shape and color are disentangled `\citep{higgins2018towards}`{=latex} such that we could edit `shape`\\(_{\text{\texttt{1}}} \longrightarrow\\) `shape`\\(_{\text{\texttt{1}}}'\\) without interfering with either `color` property (See Figure¬†<a href="#fig:das" data-reference-type="ref" data-reference="fig:das">3</a>a). For this work, we use a version of DAS where the subspace \\(s\\) is found by optimizing a differentiable binary mask and a rotation matrix over model representations `\citep{wu2024pyvene}`{=latex}. See Appendix¬†<a href="#App:DAS" data-reference-type="ref" data-reference="App:DAS">15</a> for technical details.

**Results**. We identify independent linear subspaces for color and shape in the intermediate representations produced in the early layers of CLIP-pretrained ViTs (Figure¬†<a href="#fig:das" data-reference-type="ref" data-reference="fig:das">3</a>b). In other words, we can extract either color or shape information from one object and inject it into another object. This holds for both discrimination and RMTS tasks. One can conclude that at least one function of the perceptual stage is to form disentangled local object representations, which are then used to solve same-different tasks. Notably, these local object representations are formed in the first few layers and become increasingly irrelevant in deeper layers; intervening on the intermediate representations of object tokens at layers 5 and beyond results in chance intervention performance or worse. DINOv2-pretrained ViTs provide similar results (Appendix¬†<a href="#App:Dinov2_Analyses" data-reference-type="ref" data-reference="App:Dinov2_Analyses">12</a>), whereas other models exhibit these patterns less strongly (Appendix¬†<a href="#App:Perceptual_Other_Models" data-reference-type="ref" data-reference="App:Perceptual_Other_Models">17</a>). We present a control experiment in Appendix¬†<a href="#App:Perceptual_Controls" data-reference-type="ref" data-reference="App:Perceptual_Controls">16</a>, which further confirms our interpretation of these results.

# The Relational Stage [sec:relational]

We now characterize the relational stage, where tokens within one object largely attend to tokens in the other object. We hypothesize that this stage takes in the object representations formed in the perceptual stage and computes relational same-different operations over them. We find that the operations implemented by these relational layers are somewhat abstract in that 1) they do not rely on memorizing individual objects and 2) one can identify abstract *same* and *different* representations in the RMTS task, which are constant even as the perceptual qualities of the object pairs vary.

<figure id="fig:linear_intervention">
<img src="./figures/intervention.png"" style="width:90.0%" />
<figcaption><strong>Linear probing and intervention results</strong>. We probe for the intermediate same-different judgments required to perform the RMTS task (blue). Probe performance reaches ceiling at around layer 5 and maintains throughout the rest of the model. We use the directions defined by the linear probe to intervene on model representations and flip an intermediate judgment (green). This intervention succeeds reliably at layer 5 but not deeper. We add a vector that is consistent with a pair‚Äôs exhibited same-different relation as a control (yellow). This has little effect. The background is colored according to the heatmap in Figure¬†<a href="#fig:attention_heads" data-reference-type="ref" data-reference="fig:attention_heads">2</a>c (<span style="color: 5167FF">blue</span><span class="math inline">=</span>local heads; <span style="color: FF002B">red</span><span class="math inline">=</span>global heads).</figcaption>
</figure>

**Methods ‚Äî Patching Novel Representations**. In Section¬†<a href="#sec:perceptual" data-reference-type="ref" data-reference="sec:perceptual">4</a>, we identify independent linear subspaces encoding shape and color. Does the *content* of these subspaces matter to the same-different computation? One can imagine an ideal same-different relation that is completely abstracted away from the particular properties of the objects being compared. In this setting, a model could accurately judge ‚Äúsame‚Äù vs. ‚Äúdifferent‚Äù for object representations where colors and shapes are represented by arbitrary vectors. To study this, we intervene on the linear subspaces for either shape or color for both objects in a pair, replacing the content found therein with novel representations (see Figure¬†<a href="#fig:novel_representations" data-reference-type="ref" data-reference="fig:novel_representations">4</a>a). To create a ‚Äúdifferent‚Äù example, we start with a ‚Äúsame‚Äù image and replace the shape (or color) representations of both objects with two different novel representations; to create a ‚Äúsame‚Äù example, we start with a ‚Äúdifferent‚Äù image and replace them with two identical novel representations. We then assess whether the model‚Äôs decision changes accordingly. We generate novel representations using four methods: 1) we *add* the representations found within the linear subspaces corresponding to two randomly sampled objects in an IID validation set, 2) we *interpolate* between these representations, 3) we *sample* each dimension randomly from a distribution of embeddings, and 4) we sample each dimension from an OOD *random* distribution (a \\(\mu=0\\) normal). See Appendix¬†<a href="#App:Novel_Reps" data-reference-type="ref" data-reference="App:Novel_Reps">18</a> for technical details.

**Results**. The results of patching novel representations into a CLIP-pretrained ViT are presented in Figure¬†<a href="#fig:novel_representations" data-reference-type="ref" data-reference="fig:novel_representations">4</a>b. Overall, we find the greatest success when patching in *added* representations, followed by *interpolated* representations. We observe limited success when patching *sampled* representations, and no success patching *random* vectors. All interventions perform best in layers 2 and 3, towards the end of the perceptual stage. Overall, this points to a limited form of abstraction in the relational stage. The same-different operation is somewhat abstract‚Äîwhile it cannot operate over completely arbitrary vectors, it *can* generalize to additions and interpolations of shape & color representations, indicating that it does not rely on rote memorization of specific objects. Results for CLIP-pretrained ViTs on RMTS and other models on both tasks are found in Appendix¬†<a href="#App:Relational_Other_Models" data-reference-type="ref" data-reference="App:Relational_Other_Models">19</a>. Results for DINOv2 are found in Appendix¬†<a href="#App:Dinov2_Analyses" data-reference-type="ref" data-reference="App:Dinov2_Analyses">12</a>. Other models largely produce similar results to CLIP in this analysis.

**Methods ‚Äî Linear Interventions**. The RMTS task allows us to further characterize the relational stage, as it requires first forming then comparing intermediate representations of *same* and *different*. Are these intermediate representations abstract (i.e. invariant to the perceptual qualities of the object pairs that underlie them)? We linearly probe for intermediate same or different judgments from the collection of tokens corresponding to object pairs. The probe consists of a linear transformation mapping the residual stream to two dimensions representing *same* and *different*. Each row of this transformation can be viewed as a direction \\(d\\) in the residual stream corresponding to the value being probed for (e.g. \\(d_{\text{same}}\\) is the linear direction representing *same*). We train one probe for each layer on images from the model‚Äôs train set and test on images from a test set. To understand whether the directions discovered by the probe are causally implicated in model behavior, we create a counterfactual intervention `\citep{nanda2023emergent}`{=latex}. In order to change an intermediate judgment from *same* to *different*, we add the direction \\(d_{\text{diff}}\\) to the intermediate representations of objects that exhibit the *same* relation. We then observe whether the model behaves as if this pair now exhibits the *different* relation.[^7]

We run this intervention on images from the model‚Äôs test set. We also run a control intervention where we add the incorrect direction (e.g., we add \\(d_{\text{same}}\\) when the object pair is already ‚Äúsame‚Äù). This control intervention should not reliably flip the model‚Äôs downstream decisions.

**Results**. Probing and linear intervention results for a CLIP-pretrained ViT are shown in Figure¬†<a href="#fig:linear_intervention" data-reference-type="ref" data-reference="fig:linear_intervention">5</a>. We observe that linear probe performance peaks in the middle layers of the model (layer 5) and then remains high. However, our linear intervention accuracy peaks at layer 5 and then drops precipitously. Notably, layer 5 also corresponds to the peak of within-pair attention (see Figure¬†<a href="#fig:attention_heads" data-reference-type="ref" data-reference="fig:attention_heads">2</a>c). This indicates that‚Äîat least in layer 5‚Äîthere exists a single direction representing *same* and a single direction representing *different*. One can flip the intermediate same-different judgment by adding a vector in one of these directions to the residual streams of any pair of objects.

Finally, the control intervention completely fails throughout all layers, as expected. Thus, CLIP ViT does in fact generate and operate over abstract representations of same and different in the RMTS task. We find similar results for a DINOv2 pretrained model (see Appendix¬†<a href="#App:Dinov2_Analyses" data-reference-type="ref" data-reference="App:Dinov2_Analyses">12</a>), but not for others (see Appendix¬†<a href="#App:Relational_Other_Models" data-reference-type="ref" data-reference="App:Relational_Other_Models">19</a>).

<figure id="fig:correlations">
<p><img src="./figures/das_correlations_discrimination.png"" /> <img src="./figures/das_correlations_rmts.png"" /></p>
<figcaption>We average the best counterfactual intervention accuracy for shape and color and plot it against IID, OOD, and Compositional Test set performance for CLIP, DINO, DINOv2, ImageNet, MAE, and from-scratch B/16 models. We observe that increased disentanglement (i.e. higher counterfactual accuracy) correlates with downstream performance. The from-scratch model achieved only chance IID performance in RMTS, so we omitted it from the analysis.</figcaption>
</figure>

# Disentanglement Correlates with Generalization Performance [sec:disentanglement]

Object representations that disentangle perceptual properties may enable a model to generalize to out-of-distribution stimuli. Specifically, disentangled visual representations may enable compositional generalization to unseen combinations of perceptual properties (`\cite{higgins2018towards,bengio2013representation}`{=latex}, cf. `\cite{locatello2019challenging}`{=latex})[^8]. To investigate the relationship between disentanglement and generalization, we fine-tune CLIP, ImageNet, DINO, DINOv2, MAE, and randomly-initialized ViTs on a new dataset where each shape is only ever paired with two distinct colors. We then repeat our analyses in Section¬†<a href="#sec:perceptual" data-reference-type="ref" data-reference="sec:perceptual">4</a> to identify independent linear subspaces for shape and color.[^9] We evaluate models in 3 settings: 1) on an IID test set consisting of observed shape-color combinations, 2) on a compositional generalization test set consisting of unobserved shape-color combinations (where each shape and each color have been individually observed), and 3) on an OOD test set consisting of completely novel shapes and colors. We plot the relationship between disentanglement (i.e. counterfactual intervention accuracy) and overall performance on these model evaluations. We find a consistent trend: more disentangled representations correlates with downstream model performance in all cases (See Figure¬†<a href="#fig:correlations" data-reference-type="ref" data-reference="fig:correlations">6</a>).

# Failure Modes [sec:failure]

Previous sections have argued that pretrained ViTs that achieve high performance when finetuned on same-different tasks implement a two-stage processing pipeline. In this section, we argue that both perceptual and relational stages can serve as failure points for models, impeding their ability to solve same-different tasks. In practice, tasks that rely on relations between objects likely have perceptual and relational stages that are orders of magnitude more complex than those we study here. The results presented herein indicate that solutions targeting *either* the perceptual `\citep{zeng2022multi}`{=latex} or relational `\citep{bugliarello2023weakly}`{=latex} stages may be insufficient for producing the robust, abstract computations that we desire.

**Perceptual and Relational Regularizers**. We introduce two loss functions, designed to induce disentangled object representations and multi-stage relational processing, respectively. When employing the *disentanglement loss*, we introduce token-level probes that are optimized to predict shape information from one linear subspace (e.g., the first 384 dimensions) of the representations generated at an intermediate layer of the model and color information from the complementary linear subspace at that same layer (layer 3, in our experiments). These probes are optimized during training, and the probe loss is backpropagated through the model. This approach is motivated by classic work on disentangled representations `\citep{eastwood2018framework}`{=latex}. The *pipeline loss* is designed to encourage discrete, specific stages of processing by regularizing the attention maps to maximize the attention pattern scores defined in Section¬†<a href="#sec:two-stage" data-reference-type="ref" data-reference="sec:two-stage">3</a>. Specifically, early layers are encouraged to maximize attention within-object, then within-pair, and finally (in the case of RMTS stimuli) between-pair. See Appendix¬†<a href="#App:Pipeline" data-reference-type="ref" data-reference="App:Pipeline">20</a> for technical details. Note that the disentanglement loss targets the perceptual stage of processing, whereas the pipeline loss targets both perceptual and relational stages.

**Results**. First, we note that models trained from scratch on the discrimination task do not clearly distinguish between perceptual and relational stages (Figure¬†<a href="#fig:attention_heads" data-reference-type="ref" data-reference="fig:attention_heads">2</a>b). Thus, we might expect that a model trained on a limited number of shape-color combinations would not learn a robust representation of the same-different relation. Indeed, Table¬†<a href="#tab:failure_modes" data-reference-type="ref" data-reference="tab:failure_modes">1</a> confirms this. However, we see that either the disentanglement loss or the pipeline loss is sufficient for learning a generalizable representation of this relation.

Similarly, we find that models trained from scratch on the RMTS task only achieve chance performance. However, in this case we must include *both* disentanglement and pipeline losses in order to induce a fairly general (though still far from perfect) hierarchical representation of same-different. This provides evidence that models may fail at either the perceptual *or* relational stages: they might fail to produce the correct types of object representations, and/or they might fail to execute relational operations over them. See Appendix¬†<a href="#App:Aux_Loss_Ablations" data-reference-type="ref" data-reference="App:Aux_Loss_Ablations">[App:Aux_Loss_Ablations]</a> for further analysis.

<div id="tab:failure_modes" markdown="1">

| **Task** | **Disent. Loss** | **Pipeline Loss** | **Train Acc.** | **Test Acc.** | **Comp. Acc.** |
|:--:|:--:|:--:|:---|:---|:---|
| Disc. | ‚Äì | ‚Äì | 77.3% | 76.5% | 76.0% |
| Disc. |  | ‚Äì | 97.3% <span style="color: 7CAD05">(+20)</span> | 94.6% <span style="color: 7CAD05">(+18.1)</span> | 86.1% <span style="color: 7CAD05">(+10.1)</span> |
| Disc. | ‚Äì |  | 95.6% <span style="color: 7CAD05">(+18.3)</span> | 93.9% <span style="color: 7CAD05">(+17.4)</span> | 92.3% <span style="color: 7CAD05">(+16.4)</span> |
| RMTS | ‚Äì | ‚Äì | 49.2% | 50.1% | 50.0% |
| RMTS |  | ‚Äì | 53.9% <span style="color: DCBC56">(+4.7)</span> | 54.4% <span style="color: DCBC56">(+4.3)</span> | 54.1% <span style="color: DCBC56">(+4.1)</span> |
| RMTS | ‚Äì |  | 66.1% <span style="color: 7CAD05">(+16.9)</span> | 50.1% | 50.1% <span style="color: DCBC56">(+0.1)</span> |
| RMTS |  |  | 95.1% <span style="color: 7CAD05">(+45.9)</span> | 94.1% <span style="color: 7CAD05">(+44)</span> | 77.4% <span style="color: 7CAD05">(+27.4)</span> |

**Performance of ViTs trained from scratch with auxiliary losses**. Adding either a disentanglement loss term to encourage disentangled object representations (**Disent. Loss**) *or* a pipeline loss to encourage two-stage processing in the attention heads (**Pipeline Loss**) boosts test accuracy and compositional generalization (**Comp. Acc.**) for the discrimination task. *Both* auxiliary losses are required to boost accuracy for the RMTS task.

</div>

# Discussion [sec:discussion]

**Related Work**.This work takes inspiration from the field of mechanistic interpretability, which seeks to characterize the algorithms that neural networks implement `\citep{olahMech}`{=latex}. Though many of these ideas originated in the domain of NLP `\citep{wang2022interpretability, hanna2024does, feng2023language,wu2024interpretability, merullo2023circuit, geva2022transformer, meng2022locating}`{=latex} and in toy settings `\citep{nanda2022progress, elhage2022toymodelsof, li2022emergent}`{=latex}, they are beginning to find applications in computer vision `\citep{fel2023craft, vilas2024analyzing, palit2023towards}`{=latex}. These techniques augment an already-robust suite of tools that visualize the features (rather than algorithms) that vision models use `\citep{olah2017feature, selvaraju2017grad, simonyan2014deep}`{=latex}. Finally, this study contributes to a growing literature employing mechanistic interpretability to address debates within cognitive science `\citep{milliere2024philosophical, lepori2023break, kallini2024mission, traylor2024transformer}`{=latex}.

**Conclusion**. The ability to compute abstract visual relations is a fundamental aspect of biological visual intelligence and a crucial stepping stone toward useful and robust artificial vision systems. In this work, we demonstrate that some fine-tuned ViTs adopt a two-stage processing pipeline to solve same-different tasks‚Äîdespite having no obvious inductive biases towards this algorithm. First, models produce disentangled object representations in a perceptual stage; models then compute a somewhat abstract version of the same-different computation in a relational stage. Finally, we observe a correlation between disentanglement and generalization and note that models might fail to learn *either* the perceptual or relational operations necessary to solve a task.

Why do CLIP and DINOv2-pretrained ViTs perform favorably and adopt this two-stage algorithm so cleanly relative to other pretrained models? `\cite{raghu2021vision}`{=latex} find that models pretrained on more data tend to learn local attention patterns in early layers, followed by global patterns in later layers. Thus, pretraining scale (rather than training objective) might enable these models to first form local object representations, which are then used in global relational operations. Future work might focus on pinning down the precise relationship between data scale and relational reasoning ability, potentially by studying the training dynamics of these models. Additionally, future work might focus on characterizing the precise mechanisms (e.g. the attention heads and MLPs) used to implement the perceptual and relational stages, or generalize our findings to more complex relational tasks.

# References [references]

<div class="thebibliography" markdown="1">

Bengio, Y., Courville, A., and Vincent, P Representation learning: A review and new perspectives *IEEE transactions on pattern analysis and machine intelligence*, 35 (8): 1798‚Äì1828, 2013. **Abstract:** The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning. (@bengio2013representation)

Bugliarello, E., Nematzadeh, A., and Hendricks, L.¬†A Weakly-supervised learning of visual relations in multimodal pretraining In *The 2023 Conference on Empirical Methods in Natural Language Processing*, 2023. **Abstract:** Recent work in vision-and-language pretraining has investigated supervised signals from object detection data to learn better, fine-grained multimodal representations. In this work, we take a step further and explore how we can tap into supervision from small-scale visual relation data. In particular, we propose two pretraining approaches to contextualise visual entities in a multimodal setup. With verbalised scene graphs, we transform visual relation triplets into structured captions, and treat them as additional image descriptions. With masked relation prediction, we further encourage relating entities from image regions with visually masked contexts. When applied to strong baselines pretrained on large amounts of Web data, zero-shot evaluations on both coarse-grained and fine-grained tasks show the efficacy of our methods in learning multimodal representations from weakly-supervised relations data. (@bugliarello2023weakly)

Caron, M., Touvron, H., Misra, I., J√©gou, H., Mairal, J., Bojanowski, P., and Joulin, A Emerging properties in self-supervised vision transformers In *Proceedings of the IEEE/CVF international conference on computer vision*, pp.¬†9650‚Äì9660, 2021. **Abstract:** In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) \[16\] that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder \[26\], multi-crop training \[9\], and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base. (@caron2021emerging)

Chalmers, D.¬†J Syntactic transformations on distributed representations *Connectionist Natural Language Processing: Readings from Connection Science*, pp.¬†46‚Äì55, 1992. **Abstract:** There has been much interest in the possibility of connectionist models whose representations can be endowed with compositional structure, and a variety of such models have been proposed. These models typically use distributed representations that arise from the functional composition of constituent parts. Functional composition and decomposition alone, however, yield only an implementation of classical symbolic theories. This paper explores the possibility of moving beyond implementation by exploiting holistic structure-sensitive operations on distributed representations. An experiment is performed using Pollack‚Äôs Recursive Auto-Associative Memory (RAAM). RAAM is used to construct distributed representations of syntactically structured sentences. A feed-forward network is then trained to operate directly on these representations, modeling syntactic transformations of the represented sentences. Successful training and generalization is obtained, demonstrating that the implicit structure present in these representations can be used for a kind of structure-sensitive processing unique to the connectionist domain. (@chalmers1992syntactic)

Cook, R.¬†G. and Wasserman, E.¬†A Learning and transfer of relational matching-to-sample by pigeons *Psychonomic Bulletin & Review*, 14 (6): 1107‚Äì1114, 2007. (@cook2007learning)

Darcet, T., Oquab, M., Mairal, J., and Bojanowski, P Vision transformers need registers *arXiv preprint arXiv:2309.16588*, 2023. **Abstract:** Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing. (@darcet2023vision)

Davidson, G., Orhan, A.¬†E., and Lake, B.¬†M Spatial relation categorization in infants and deep neural networks *Cognition*, 245: 105690, 2024. **Abstract:** Spatial relations, such as above, below, between, and contained, are important mediators in children‚Äôs understanding of the world (Piaget, 1954). The development of these relational categories in infancy has been extensively studied (Quinn, 2003) yet little is known about their computational underpinnings. Using developmental tests, we examine the extent to which deep neural networks, pretrained on a standard vision benchmark or egocentric video captured from one baby‚Äôs perspective, form categorical representations for visual stimuli depicting relations. Notably, the networks did not receive any explicit training on relations. We then analyze whether these networks recover similar patterns to ones identified in the development, such as reproducing the relative difficulty of categorizing different spatial relations and different stimulus abstractions. We find that our models tend to recover many of the patterns observed with the simpler relations of ‚Äúabove versus below‚Äù or ‚Äúbetween versus outside‚Äù, but struggle to match developmental findings related to the ‚Äúcontainment‚Äù relation. We identify factors in the choice of model architecture, pretraining data, and experimental design that contribute to the extent our models match developmental patterns, and highlight experimental predictions made by our models. Our results open the door to modeling infants‚Äô earliest categorization abilities with modern machine learning tools and demonstrate the utility and productivity of this approach. (@davidson2024spatial)

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et¬†al An image is worth 16x16 words: Transformers for image recognition at scale *arXiv preprint arXiv:2010.11929*, 2020. **Abstract:** While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. (@dosovitskiy2020image)

Eastwood, C. and Williams, C.¬†K A framework for the quantitative evaluation of disentangled representations In *International conference on learning representations*, 2018. **Abstract:** Recent AI research has emphasized the importance of learning disentangled representations of the explanatory factors behind data. Despite the growing interest in models which can learn such representations, visual inspection remains the standard evaluation metric. While various desiderata have been implied in recent definitions, it is currently unclear what exactly makes one disentangled representation better than another. In this work we propose a framework for the quantitative evaluation of disentangled representations when the ground-truth latent structure is available. Three criteria are explicitly defined and quantified to elucidate the quality of learnt representations and thus compare models on an equal basis. To illustrate the appropriateness of the framework, we employ it to compare quantitatively the representations learned by a state-of-the-art model (InfoGAN) and those learned by a baseline model (PCA). (@eastwood2018framework)

Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., et¬†al A mathematical framework for transformer circuits *Transformer Circuits Thread*, 1: 1, 2021. **Abstract:** We present causal head gating (CHG), a scalable method for interpreting the functional roles of attention heads in transformer models. CHG learns soft gates over heads and assigns them a causal taxonomy - facilitating, interfering, or irrelevant - based on their impact on task performance. Unlike prior approaches in mechanistic interpretability, which are hypothesis-driven and require prompt templates or target labels, CHG applies directly to any dataset using standard next-token prediction. We evaluate CHG across multiple large language models (LLMs) in the Llama 3 model family and diverse tasks, including syntax, commonsense, and mathematical reasoning, and show that CHG scores yield causal - not merely correlational - insight, validated via ablation and causal mediation analyses. We also introduce contrastive CHG, a variant that isolates sub-circuits for specific task components. Our findings reveal that LLMs contain multiple sparse, sufficient sub-circuits, that individual head roles depend on interactions with others (low modularity), and that instruction following and in-context learning rely on separable mechanisms. (@elhage2021mathematical)

Elhage, N., Hume, T., Olsson, C., Schiefer, N., Henighan, T., Kravec, S., Hatfield-Dodds, Z., Lasenby, R., Drain, D., Chen, C., et¬†al Toymodelsof superposition . **Abstract:** Neural networks often pack many unrelated concepts into a single neuron - a puzzling phenomenon known as ‚Äôpolysemanticity‚Äô which makes interpretability much more challenging. This paper provides a toy model where polysemanticity can be fully understood, arising as a result of models storing additional sparse features in"superposition."We demonstrate the existence of a phase change, a surprising connection to the geometry of uniform polytopes, and evidence of a link to adversarial examples. We also discuss potential implications for mechanistic interpretability. (@elhage2022toymodelsof)

Fel, T., Picard, A., Bethune, L., Boissin, T., Vigouroux, D., Colin, J., Cad√®ne, R., and Serre, T Craft: Concept recursive activation factorization for explainability In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp.¬†2711‚Äì2721, 2023. **Abstract:** Attribution methods, which employ heatmaps to identify the most influential regions of an image that impact model decisions, have gained widespread popularity as a type of explainability method. However, recent research has exposed the limited practical value of these methods, attributed in part to their narrow focus on the most prominent regions of an image - revealing "where" the model looks, but failing to elucidate "what" the model sees in those areas. In this work, we try to fill in this gap with CRAFT - a novel approach to identify both "what" and "where" by generating concept-based explanations. We introduce 3 new ingredients to the automatic concept extraction literature: (i) a recursive strategy to detect and decompose concepts across layers, (ii) a novel method for a more faithful estimation of concept importance using Sobol indices, and (iii) the use of implicit differentiation to unlock Concept Attribution Maps. We conduct both human and computer vision experiments to demonstrate the benefits of the proposed approach. We show that the proposed concept importance estimation technique is more faithful to the model than previous methods. When evaluating the usefulness of the method for human experimenters on a human-centered utility benchmark, we find that our approach significantly improves on two of the three test scenarios. (@fel2023craft)

Feng, J. and Steinhardt, J How do language models bind entities in context? *arXiv preprint arXiv:2310.17191*, 2023. **Abstract:** To correctly use in-context information, language models (LMs) must bind entities to their attributes. For example, given a context describing a "green square" and a "blue circle", LMs must bind the shapes to their respective colors. We analyze LM representations and identify the binding ID mechanism: a general mechanism for solving the binding problem, which we observe in every sufficiently large model from the Pythia and LLaMA families. Using causal interventions, we show that LMs‚Äô internal activations represent binding information by attaching binding ID vectors to corresponding entities and attributes. We further show that binding ID vectors form a continuous subspace, in which distances between binding ID vectors reflect their discernability. Overall, our results uncover interpretable strategies in LMs for representing symbolic knowledge in-context, providing a step towards understanding general in-context reasoning in large-scale LMs. (@feng2023language)

Fleuret, F., Li, T., Dubout, C., Wampler, E.¬†K., Yantis, S., and Geman, D Comparing machines and humans on a visual categorization test *Proceedings of the National Academy of Sciences*, 108 (43): 17621‚Äì17625, 2011. **Abstract:** Automated scene interpretation has benefited from advances in machine learning, and restricted tasks, such as face detection, have been solved with sufficient accuracy for restricted settings. However, the performance of machines in providing rich semantic descriptions of natural scenes from digital images remains highly limited and hugely inferior to that of humans. Here we quantify this ‚Äúsemantic gap‚Äù in a particular setting: We compare the efficiency of human and machine learning in assigning an image to one of two categories determined by the spatial arrangement of constituent parts. The images are not real, but the category-defining rules reflect the compositional structure of real images and the type of ‚Äúreasoning‚Äù that appears to be necessary for semantic parsing. Experiments demonstrate that human subjects grasp the separating principles from a handful of examples, whereas the error rates of computer programs fluctuate wildly and remain far behind that of humans even after exposure to thousands of examples. These observations lend support to current trends in computer vision such as integrating machine learning with parts-based modeling. (@fleuret2011comparing)

Fodor, J.¬†A. and Pylyshyn, Z.¬†W Connectionism and cognitive architecture: A critical analysis *Cognition*, 28 (1-2): 3‚Äì71, 1988. (@fodor1988connectionism)

Forbus, K.¬†D. and Lovett, A Same/different in visual reasoning *Current Opinion in Behavioral Sciences*, 37: 63‚Äì68, 2021. **Abstract:** The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor modifications, we found that SOTA VLMs like GPT-4o can consistently fail in these scenarios, revealing limitations in their mathematical reasoning capabilities. In this paper, we investigate the mathematical reasoning robustness in VLMs and evaluate how well these models perform under different variants of the same question, such as changes in visual numerical values or function graphs. While several vision-based math benchmarks have been developed to assess VLMs‚Äô problem-solving capabilities, these benchmarks contain only static sets of problems and cannot easily evaluate mathematical reasoning robustness. To fill this gap, we introduce DynaMath, a dynamic visual math benchmark designed for in-depth assessment of VLMs. DynaMath includes 501 high-quality, multi-topic seed questions, each represented as a Python program. Those programs are carefully designed and annotated to enable the automatic generation of a much larger set of concrete questions, including many different types of visual and textual variations. DynaMath allows us to evaluate the generalization ability of VLMs, by assessing their performance under varying input conditions of a seed question. We evaluated 14 SOTA VLMs with 5,010 generated concrete questions. Our results show that the worst-case model accuracy, defined as the percentage of correctly answered seed questions in all 10 variants, is significantly lower than the average-case accuracy. Our analysis emphasizes the need to study the robustness of VLMs‚Äô reasoning abilities, and DynaMath provides valuable insights to guide the development of more reliable models for mathematical reasoning. (@forbus2021same)

Geiger, A., Carstensen, A., Frank, M.¬†C., and Potts, C Relational reasoning and generalization using nonsymbolic neural networks *Psychological Review*, 130 (2): 308, 2023. **Abstract:** The notion of equality (identity) is simple and ubiquitous, making it a key case study for broader questions about the representations supporting abstract relational reasoning. Previous work suggested that neural networks were not suitable models of human relational reasoning because they could not represent mathematically identity, the most basic form of equality. We revisit this question. In our experiments, we assess out-of-sample generalization of equality using both arbitrary representations and representations that have been pretrained on separate tasks to imbue them with structure. We find neural networks are able to learn (a) basic equality (mathematical identity), (b) sequential equality problems (learning ABA-patterned sequences) with only positive training instances, and (c) a complex, hierarchical equality problem with only basic equality training instances ("zero-shot" generalization). In the two latter cases, our models perform tasks proposed in previous work to demarcate human-unique symbolic abilities. These results suggest that essential aspects of symbolic reasoning can emerge from data-driven, nonsymbolic learning processes. (PsycInfo Database Record (c) 2023 APA, all rights reserved). (@geiger2023relational)

Geiger, A., Wu, Z., Potts, C., Icard, T., and Goodman, N Finding alignments between interpretable causal variables and distributed neural representations In *Causal Learning and Reasoning*, pp.¬†160‚Äì187. PMLR, 2024. **Abstract:** Causal abstraction is a promising theoretical framework for explainable artificial intelligence that defines when an interpretable high-level causal model is a faithful simplification of a low-level deep learning system. However, existing causal abstraction methods have two major limitations: they require a brute-force search over alignments between the high-level model and the low-level one, and they presuppose that variables in the high-level model will align with disjoint sets of neurons in the low-level one. In this paper, we present distributed alignment search (DAS), which overcomes these limitations. In DAS, we find the alignment between high-level and low-level models using gradient descent rather than conducting a brute-force search, and we allow individual neurons to play multiple distinct roles by analyzing representations in non-standard bases-distributed representations. Our experiments show that DAS can discover internal structure that prior approaches miss. Overall, DAS removes previous obstacles to conducting causal abstraction analyses and allows us to find conceptual structure in trained neural nets. (@geiger2024finding)

Geva, M., Caciularu, A., Wang, K., and Goldberg, Y Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*. Association for Computational Linguistics, 2022. **Abstract:** Transformer-based language models (LMs) are at the core of modern NLP, but their internal prediction construction process is opaque and largely not understood. In this work, we make a substantial step towards unveiling this underlying prediction process, by reverse-engineering the operation of the feed-forward network (FFN) layers, one of the building blocks of transformer models. We view the token representation as a changing distribution over the vocabulary, and the output from each FFN layer as an additive update to that distribution. Then, we analyze the FFN updates in the vocabulary space, showing that each update can be decomposed to sub-updates corresponding to single FFN parameter vectors, each promoting concepts that are often human-interpretable. We then leverage these findings for controlling LM predictions, where we reduce the toxicity of GPT2 by almost 50%, and for improving computation efficiency with a simple early exit rule, saving 20% of computation on average. (@geva2022transformer)

Giurfa, M., Zhang, S., Jenett, A., Menzel, R., and Srinivasan, M.¬†V The concepts of ‚Äòsameness‚Äô and ‚Äòdifference‚Äôin an insect *Nature*, 410 (6831): 930‚Äì933, 2001. (@giurfa2001concepts)

Hanna, M., Liu, O., and Variengien, A How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model *Advances in Neural Information Processing Systems*, 36, 2024. **Abstract:** Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years \> 32). We first identify a circuit, a small subset of GPT-2 small‚Äôs computational graph that computes this task‚Äôs output. Then, we explain the role of each circuit component, showing that GPT-2 small‚Äôs final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we find related tasks that activate our circuit. Our results suggest that GPT-2 small computes greater-than using a complex but general mechanism that activates across diverse contexts. (@hanna2024does)

He, K., Chen, X., Xie, S., Li, Y., Doll√°r, P., and Girshick, R Masked autoencoders are scalable vision learners In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp.¬†16000‚Äì16009, 2022. **Abstract:** This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3√ó or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior. (@he2022masked)

Hespos, S., Gentner, D., Anderson, E., and Shivaram, A The origins of same/different discrimination in human infants *Current Opinion in Behavioral Sciences*, 37: 69‚Äì74, 2021. (@hespos2021origins)

Higgins, I., Amos, D., Pfau, D., Racaniere, S., Matthey, L., Rezende, D., and Lerchner, A Towards a definition of disentangled representations *arXiv preprint arXiv:1812.02230*, 2018. **Abstract:** How can intelligent agents solve a diverse set of tasks in a data-efficient manner? The disentangled representation learning approach posits that such an agent would benefit from separating out (disentangling) the underlying structure of the world into disjoint parts of its representation. However, there is no generally agreed-upon definition of disentangling, not least because it is unclear how to formalise the notion of world structure beyond toy datasets with a known ground truth generative process. Here we propose that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world. In particular, we suggest that those transformations that change only some properties of the underlying world state, while leaving all other properties invariant, are what gives exploitable structure to any kind of data. Similar ideas have already been successfully applied in physics, where the study of symmetry transformations has revolutionised the understanding of the world structure. By connecting symmetry transformations to vector representations using the formalism of group and representation theory we arrive at the first formal definition of disentangled representations. Our new definition is in agreement with many of the current intuitions about disentangling, while also providing principled resolutions to a number of previous points of contention. While this work focuses on formally defining disentangling - as opposed to solving the learning problem - we believe that the shift in perspective to studying data transformations can stimulate the development of better representation learning algorithms. (@higgins2018towards)

Hochmann, J.-R., Tuerk, A.¬†S., Sanborn, S., Zhu, R., Long, R., Dempster, M., and Carey, S Children‚Äôs representation of abstract relations in relational/array match-to-sample tasks *Cognitive psychology*, 99: 17‚Äì43, 2017. (@hochmann2017children)

Hochmann, J.-R., Wasserman, E., and Carey, S Editorial overview: Same-different conceptualization *Current Opinion in Behavioral Sciences*, 37: iii‚Äìv, 2021. **Abstract:** Purpose ‚Äì The purpose of this editorial is to provide an overview of content and process of development of the five papers collected in this special issue on collaboration and storying.Design/methodology/approach ‚Äì This guest editorial discusses the five papers in terms of their contribution both to debates on the utility of stories as data and forms of knowledge as well as to developing understandings of the research and practice of collaboration. Findings ‚Äì The special issue integrates aspects of research, issues of research and connects with implications for practice.Originality/value ‚Äì Readers are provided with an overview of the utility of stories as data, different levels of conceptualization of stories and the kinds of insights that can flow from this form of research. (@hochmann2021editorial)

Holyoak, K.¬†J. and Lu, H Emergence of relational reasoning *Current Opinion in Behavioral Sciences*, 37: 118‚Äì124, 2021. (@holyoak2021emergence)

Kallini, J., Papadimitriou, I., Futrell, R., Mahowald, K., and Potts, C Mission: Impossible language models *arXiv preprint arXiv:2401.06416*, 2024. **Abstract:** Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially, we perform these assessments at various stages throughout training to compare the learning process for each language. Our core finding is that GPT-2 struggles to learn impossible languages when compared to English as a control, challenging the core claim. More importantly, we hope our approach opens up a productive line of inquiry in which different LLM architectures are tested on a variety of impossible languages in an effort to learn more about how LLMs can be used as tools for these cognitive and typological investigations. (@kallini2024mission)

Kim, J., Ricci, M., and Serre, T Not-so-clevr: learning same‚Äìdifferent relations strains feedforward neural networks *Interface focus*, 8 (4): 20180011, 2018. **Abstract:** The advent of deep learning has recently led to great successes in various engineering applications. As a prime example, convolutional neural networks, a type of feedforward neural network, now approach human accuracy on visual recognition tasks like image classification and face recognition. However, here we will show that feedforward neural networks struggle to learn abstract visual relations that are effortlessly recognized by non-human primates, birds, rodents and even insects. We systematically study the ability of feedforward neural networks to learn to recognize a variety of visual relations and demonstrate that same‚Äìdifferent visual relations pose a particular strain on these networks. Networks fail to learn same‚Äìdifferent visual relations when stimulus variability makes rote memorization difficult. Further, we show that learning same‚Äìdifferent problems becomes trivial for a feedforward network that is fed with perceptually grouped stimuli. This demonstration and the comparative success of biological vision in learning visual relations suggests that feedback mechanisms such as attention, working memory and perceptual grouping may be the key components underlying human-level abstract visual reasoning. (@kim2018not)

Kingma, D.¬†P. and Ba, J Adam: A method for stochastic optimization *arXiv preprint arXiv:1412.6980*, 2014. **Abstract:** We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm. (@kingma2014adam)

Kreiman, G. and Serre, T Beyond the feedforward sweep: feedback computations in the visual cortex *Annals of the New York Academy of Sciences*, 1464 (1): 222‚Äì241, 2020. **Abstract:** Visual perception involves the rapid formation of a coarse image representation at the onset of visual processing, which is iteratively refined by late computational processes. These early versus late time windows approximately map onto feedforward and feedback processes, respectively. State-of-the-art convolutional neural networks, the main engine behind recent machine vision successes, are feedforward architectures. Their successes and limitations provide critical information regarding which visual tasks can be solved by purely feedforward processes and which require feedback mechanisms. We provide an overview of recent work in cognitive neuroscience and machine vision that highlights the possible role of feedback processes for both visual recognition and beyond. We conclude by discussing important open questions for future research. (@kreiman2020beyond)

Lake, B.¬†M., Ullman, T.¬†D., Tenenbaum, J.¬†B., and Gershman, S.¬†J Building machines that learn and think like people *Behavioral and brain sciences*, 40: e253, 2017. **Abstract:** Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models. (@lake2017building)

Lepori, M., Serre, T., and Pavlick, E Break it down: Evidence for structural compositionality in neural networks *Advances in Neural Information Processing Systems*, 36: 42623‚Äì42660, 2023. **Abstract:** Though modern neural networks have achieved impressive performance in both vision and language tasks, we know little about the functions that they implement. One possibility is that neural networks implicitly break down complex tasks into subroutines, implement modular solutions to these subroutines, and compose them into an overall solution to a task - a property we term structural compositionality. Another possibility is that they may simply learn to match new inputs to learned templates, eliding task decomposition entirely. Here, we leverage model pruning techniques to investigate this question in both vision and language across a variety of architectures, tasks, and pretraining regimens. Our results demonstrate that models often implement solutions to subroutines via modular subnetworks, which can be ablated while maintaining the functionality of other subnetworks. This suggests that neural networks may be able to learn compositionality, obviating the need for specialized symbolic mechanisms. (@lepori2023break)

Lepori, M.¬†A., Pavlick, E., and Serre, T Neurosurgeon: A toolkit for subnetwork analysis *arXiv preprint arXiv:2309.00244*, 2023. **Abstract:** Despite recent advances in the field of explainability, much remains unknown about the algorithms that neural networks learn to represent. Recent work has attempted to understand trained models by decomposing them into functional circuits (Csord\\}‚Äôas et al., 2020; Lepori et al., 2023). To advance this research, we developed NeuroSurgeon, a python library that can be used to discover and manipulate subnetworks within models in the Huggingface Transformers library (Wolf et al., 2019). NeuroSurgeon is freely available at https://github.com/mlepori1/NeuroSurgeon. (@lepori2023neurosurgeon)

Li, K., Hopkins, A.¬†K., Bau, D., Vi√©gas, F., Pfister, H., and Wattenberg, M Emergent world representations: Exploring a sequence model trained on a synthetic task In *The Eleventh International Conference on Learning Representations*, 2022. **Abstract:** Language models show a surprising range of capabilities, but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see? We investigate this question by applying a variant of the GPT model to the task of predicting legal moves in a simple board game, Othello. Although the network has no a priori knowledge of the game or its rules, we uncover evidence of an emergent nonlinear internal representation of the board state. Interventional experiments indicate this representation can be used to control the output of the network and create "latent saliency maps" that can help explain predictions in human terms. (@li2022emergent)

Liu, F., Emerson, G., and Collier, N Visual spatial reasoning *Transactions of the Association for Computational Linguistics*, 11: 635‚Äì651, 2023. **Abstract:** Abstract Spatial relations are a basic part of human cognition. However, they are expressed in natural language in a variety of ways, and previous work has suggested that current vision-and-language models (VLMs) struggle to capture relational information. In this paper, we present Visual Spatial Reasoning (VSR), a dataset containing more than 10k natural text-image pairs with 66 types of spatial relations in English (e.g., under, in front of, facing). While using a seemingly simple annotation format, we show how the dataset includes challenging linguistic phenomena, such as varying reference frames. We demonstrate a large gap between human and model performance: The human ceiling is above 95%, while state-of-the-art models only achieve around 70%. We observe that VLMs‚Äô by-relation performances have little correlation with the number of training examples and the tested models are in general incapable of recognising relations concerning the orientations of objects.1 (@liu2023visual)

Locatello, F., Bauer, S., Lucic, M., Raetsch, G., Gelly, S., Sch√∂lkopf, B., and Bachem, O Challenging common assumptions in the unsupervised learning of disentangled representations In *international conference on machine learning*, pp.¬†4114‚Äì4124. PMLR, 2019. **Abstract:** The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties ‚Äúencouraged‚Äù by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets. (@locatello2019challenging)

Makelov, A., Lange, G., Geiger, A., and Nanda, N Is this the subspace you are looking for? an interpretability illusion for subspace activation patching In *The Twelfth International Conference on Learning Representations*, 2023. **Abstract:** Mechanistic interpretability aims to understand model behaviors in terms of specific, interpretable features, often hypothesized to manifest as low-dimensional subspaces of activations. Specifically, recent studies have explored subspace interventions (such as activation patching) as a way to simultaneously manipulate model behavior and attribute the features behind it to given subspaces. In this work, we demonstrate that these two aims diverge, potentially leading to an illusory sense of interpretability. Counterintuitively, even if a subspace intervention makes the model‚Äôs output behave as if the value of a feature was changed, this effect may be achieved by activating a dormant parallel pathway leveraging another subspace that is causally disconnected from model outputs. We demonstrate this phenomenon in a distilled mathematical example, in two real-world domains (the indirect object identification task and factual recall), and present evidence for its prevalence in practice. In the context of factual recall, we further show a link to rank-1 fact editing, providing a mechanistic explanation for previous work observing an inconsistency between fact editing performance and fact localization. However, this does not imply that activation patching of subspaces is intrinsically unfit for interpretability. To contextualize our findings, we also show what a success case looks like in a task (indirect object identification) where prior manual circuit analysis informs an understanding of the location of a feature. We explore the additional evidence needed to argue that a patched subspace is faithful. (@makelov2023subspace)

Marcus, G.¬†F *The algebraic mind: Integrating connectionism and cognitive science* MIT press, 2003. **Abstract:** In The Algebraic Mind, Gary Marcus attempts to integrate two theories about how the mind works, one that says that the mind is a computer-like manipulator of symbols, and another that says that the mind is a large network of neurons working together in parallel. Resisting the conventional wisdom that says that if the mind is a large neural network it cannot simultaneously be a manipulator of symbols, Marcus outlines a variety of ways in which neural systems could be organized so as to manipulate symbols, and he shows why such systems are more likely to provide an adequate substrate for language and cognition than neural systems that are inconsistent with the manipulation of symbols. Concluding with a discussion of how a neurally realized system of symbol-manipulation could have evolved and how such a system could unfold developmentally within the womb, Marcus helps to set the future agenda of cognitive neuroscience. (@marcus2003algebraic)

Martinho¬†III, A. and Kacelnik, A Ducklings imprint on the relational concept of ‚Äúsame or different‚Äù *Science*, 353 (6296): 286‚Äì288, 2016. **Abstract:** The ability to identify and retain logical relations between stimuli and apply them to novel stimuli is known as relational concept learning. This has been demonstrated in a few animal species after extensive reinforcement training, and it reveals the brain‚Äôs ability to deal with abstract properties. Here we describe relational concept learning in newborn ducklings without reinforced training. Newly hatched domesticated mallards that were briefly exposed to a pair of objects that were either the same or different in shape or color later preferred to follow pairs of new objects exhibiting the imprinted relation. Thus, even in a seemingly rigid and very rapid form of learning such as filial imprinting, the brain operates with abstract conceptual reasoning, a faculty often assumed to be reserved to highly intelligent organisms. (@martinho2016ducklings)

McCoy, R.¬†T., Smolensky, P., Linzen, T., Gao, J., and Celikyilmaz, A How much do language models copy from their training data? evaluating linguistic novelty in text generation using raven *Transactions of the Association for Computational Linguistics*, 11: 652‚Äì670, 2023. **Abstract:** Abstract Current language models can generate high-quality text. Are they simply copying text they have seen before, or have they learned generalizable linguistic abstractions? To tease apart these possibilities, we introduce RAVEN, a suite of analyses for assessing the novelty of generated text, focusing on sequential structure (n-grams) and syntactic structure. We apply these analyses to four neural language models trained on English (an LSTM, a Transformer, Transformer-XL, and GPT-2). For local structure‚Äîe.g., individual dependencies‚Äîtext generated with a standard sampling scheme is substantially less novel than our baseline of human-generated text from each model‚Äôs test set. For larger-scale structure‚Äîe.g., overall sentence structure‚Äîmodel-generated text is as novel or even more novel than the human-generated baseline, but models still sometimes copy substantially, in some cases duplicating passages over 1,000 words long from the training set. We also perform extensive manual analysis, finding evidence that GPT-2 uses both compositional and analogical generalization mechanisms and showing that GPT-2‚Äôs novel text is usually well-formed morphologically and syntactically but has reasonably frequent semantic issues (e.g., being self-contradictory). (@mccoy2023much)

Meng, K., Bau, D., Andonian, A., and Belinkov, Y Locating and editing factual associations in gpt *Advances in Neural Information Processing Systems*, 35: 17359‚Äì17372, 2022. **Abstract:** We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model‚Äôs factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/ (@meng2022locating)

Merullo, J., Eickhoff, C., and Pavlick, E Circuit component reuse across tasks in transformer language models *arXiv preprint arXiv:2310.08744*, 2023. **Abstract:** Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in Wang et al. (2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito & Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment, in which we adjust four attention heads in middle layers in order to ‚Äôrepair‚Äô the Colored Objects circuit and make it behave like the IOI circuit. In doing so, we boost accuracy from 49.6% to 93.7% on the Colored Objects task and explain most sources of error. The intervention affects downstream attention heads in specific ways predicted by their interactions in the IOI circuit, indicating that this subcircuit behavior is invariant to the different task inputs. Overall, our results provide evidence that it may yet be possible to explain large language models‚Äô behavior in terms of a relatively small number of interpretable task-general algorithmic building blocks and computational components. (@merullo2023circuit)

Milli√®re, R. and Buckner, C A philosophical introduction to language models-part ii: The way forward *arXiv preprint arXiv:2405.03207*, 2024. **Abstract:** In this paper, the second of two companion pieces, we explore novel philosophical questions raised by recent progress in large language models (LLMs) that go beyond the classical debates covered in the first part. We focus particularly on issues related to interpretability, examining evidence from causal intervention methods about the nature of LLMs‚Äô internal representations and computations. We also discuss the implications of multimodal and modular extensions of LLMs, recent debates about whether such systems may meet minimal criteria for consciousness, and concerns about secrecy and reproducibility in LLM research. Finally, we discuss whether LLM-like systems may be relevant to modeling aspects of human cognition, if their architectural characteristics and learning scenario are adequately constrained. (@milliere2024philosophical)

Nanda, N. and Bloom, J Transformerlens <https://github.com/TransformerLensOrg/TransformerLens>, 2022. **Abstract:** Large language models (LLMs) demonstrate surprising capabilities, but we do not understand how they are implemented. One hypothesis suggests that these capabilities are primarily executed by small subnetworks within the LLM, known as circuits. But how can we evaluate this hypothesis? In this paper, we formalize a set of criteria that a circuit is hypothesized to meet and develop a suite of hypothesis tests to evaluate how well circuits satisfy them. The criteria focus on the extent to which the LLM‚Äôs behavior is preserved, the degree of localization of this behavior, and whether the circuit is minimal. We apply these tests to six circuits described in the research literature. We find that synthetic circuits ‚Äì circuits that are hard-coded in the model ‚Äì align with the idealized properties. Circuits discovered in Transformer models satisfy the criteria to varying degrees. To facilitate future empirical studies of circuits, we created the \\}textit{circuitry} package, a wrapper around the \\}textit{TransformerLens} library, which abstracts away lower-level manipulations of hooks and activations. The software is available at \\}url{https://github.com/blei-lab/circuitry}. (@nanda2022transformerlens)

Nanda, N., Chan, L., Lieberum, T., Smith, J., and Steinhardt, J Progress measures for grokking via mechanistic interpretability In *The Eleventh International Conference on Learning Representations*, 2022. **Abstract:** Neural networks often exhibit emergent behavior, where qualitatively new capabilities arise from scaling up the amount of parameters, training data, or training steps. One approach to understanding emergence is to find continuous \\}textit{progress measures} that underlie the seemingly discontinuous qualitative changes. We argue that progress measures can be found via mechanistic interpretability: reverse-engineering learned behaviors into their individual components. As a case study, we investigate the recently-discovered phenomenon of ‚Äúgrokking‚Äù exhibited by small transformers trained on modular addition tasks. We fully reverse engineer the algorithm learned by these networks, which uses discrete Fourier transforms and trigonometric identities to convert addition to rotation about a circle. We confirm the algorithm by analyzing the activations and weights and by performing ablations in Fourier space. Based on this understanding, we define progress measures that allow us to study the dynamics of training and split training into three continuous phases: memorization, circuit formation, and cleanup. Our results show that grokking, rather than being a sudden shift, arises from the gradual amplification of structured mechanisms encoded in the weights, followed by the later removal of memorizing components. (@nanda2022progress)

Nanda, N., Lee, A., and Wattenberg, M Emergent linear representations in world models of self-supervised sequence models In *Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP*, pp.¬†16‚Äì30, 2023. **Abstract:** How do sequence models represent their decision-making process? Prior work suggests that Othello-playing neural network learned nonlinear models of the board state (Li et al., 2023a). In this work, we provide evidence of a closely related linear representation of the board. In particular, we show that probing for "my colour" vs. "opponent‚Äôs colour" may be a simple yet powerful way to interpret the model‚Äôs internal state. This precise understanding of the internal representations allows us to control the model‚Äôs behaviour with simple vector arithmetic. Linear representations enable significant interpretability progress, which we demonstrate with further exploration of how the world model is computed. (@nanda2023emergent)

Newport, C Abstract concept learning in fish *Current Opinion in Behavioral Sciences*, 37: 56‚Äì62, 2021. **Abstract:** concept learning in fish 2 Cait Newport1\* 3 1 Department of Zoology, University of Oxford, Oxford, England 4 \* Corresponding author 5 Abstract 6 Abstract concept formation allows animals to group stimuli based on relationships (e.g. 7 sameness/ difference ) rather than stimulus -specific qualities. Studies in this field have focused 8 on primates and birds, but there is growing interest in the capabilities of a wider rang e of 9 species to gain an understanding of differences in cognitive abilities across taxa and ecological 10 requirements. This review concentrates on abstract concept learning in fish . There have been 11 only a few studies testing this group and these have resulte d in no direct evidence of abstract 12 concept learning. Further experiments following rigorous methodologies are required to 13 determine whether all fish species are truly incapable, or whether the right learning conditions 14 have not yet been met for the approp riate species. 15 (@newport2021abstract)

Olah, C Mechanistic interpretability, variables, and the importance of interpretable bases *Transformer Circuits Thread*, 2022. (@olahMech)

Olah, C., Mordvintsev, A., and Schubert, L Feature visualization *Distill*, 2 (11): e7, 2017. **Abstract:** Summary: Visualizing genes‚Äô structure and annotated features helps biologists to investigate their function and evolution intuitively. The Gene Structure Display Server (GSDS) has been widely used by more than 60 000 users since its first publication in 2007. Here, we reported the upgraded GSDS 2.0 with a newly designed interface, supports for more types of annotation features and formats, as well as an integrated visual editor for editing the generated figure. Moreover, a user-specified phylogenetic tree can be added to facilitate further evolutionary analysis. The full source code is also available for downloading. (@olah2017feature)

Oquab, M., Darcet, T., Moutakanni, T., Vo, H.¬†V., Szafraniec, M., Khalidov, V., Fernandez, P., HAZIZA, D., Massa, F., El-Nouby, A., et¬†al Dinov2: Learning robust visual features without supervision *Transactions on Machine Learning Research*. **Abstract:** The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels. (@oquabdinov2)

Palit, V., Pandey, R., Arora, A., and Liang, P.¬†P Towards vision-language mechanistic interpretability: A causal tracing tool for blip In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pp.¬†2856‚Äì2861, 2023. **Abstract:** Mechanistic interpretability seeks to understand the neural mechanisms that enable specific behaviors in Large Language Models (LLMs) by leveraging causality-based methods. While these approaches have identified neural circuits that copy spans of text, capture factual knowledge, and more, they remain unusable for multimodal models since adapting these tools to the vision-language domain requires considerable architectural changes. In this work, we adapt a unimodal causal tracing tool to BLIP to enable the study of the neural mechanisms underlying image-conditioned text generation. We demonstrate our approach on a visual question answering dataset, highlighting the causal relevance of later layer representations for all tokens. Furthermore, we release our BLIP causal tracing tool as open source to enable further experimentation in vision-language mechanistic interpretability by the community. Our code is available at this URL. (@palit2023towards)

Penn, D.¬†C., Holyoak, K.¬†J., and Povinelli, D.¬†J Darwin‚Äôs mistake: Explaining the discontinuity between human and nonhuman minds *Behavioral and brain sciences*, 31 (2): 109‚Äì130, 2008. **Abstract:** Abstract Over the last quarter century, the dominant tendency in comparative cognitive psychology has been to emphasize the similarities between human and nonhuman minds and to downplay the differences as ‚Äúone of degree and not of kind‚Äù (Darwin 1871). In the present target article, we argue that Darwin was mistaken: the profound biological continuity between human and nonhuman animals masks an equally profound discontinuity between human and nonhuman minds. To wit, there is a significant discontinuity in the degree to which human and nonhuman animals are able to approximate the higher-order, systematic, relational capabilities of a physical symbol system (PSS) (Newell 1980). We show that this symbolic-relational discontinuity pervades nearly every domain of cognition and runs much deeper than even the spectacular scaffolding provided by language or culture alone can explain. We propose a representational-level specification as to where human and nonhuman animals‚Äô abilities to approximate a PSS are similar and where they differ. We conclude by suggesting that recent symbolic-connectionist models of cognition shed new light on the mechanisms that underlie the gap between human and nonhuman minds. (@penn2008darwin)

Puebla, G. and Bowers, J.¬†S Can deep convolutional neural networks support relational reasoning in the same-different task? *Journal of Vision*, 22 (10): 11‚Äì11, 2022. **Abstract:** Same-different visual reasoning is a basic skill central to abstract combinatorial thought. This fact has lead neural networks researchers to test same-different classification on deep convolutional neural networks (DCNNs), which has resulted in a controversy regarding whether this skill is within the capacity of these models. However, most tests of same-different classification rely on testing on images that come from the same pixel-level distribution as the training images, yielding the results inconclusive. In this study, we tested relational same-different reasoning in DCNNs. In a series of simulations we show that models based on the ResNet architecture are capable of visual same-different classification, but only when the test images are similar to the training images at the pixel level. In contrast, when there is a shift in the testing distribution that does not change the relation between the objects in the image, the performance of DCNNs decreases substantially. This finding is true even when the DCNNs‚Äô training regime is expanded to include images taken from a wide range of different pixel-level distributions or when the model is trained on the testing distribution but on a different task in a multitask learning context. Furthermore, we show that the relation network, a deep learning architecture specifically designed to tackle visual relational reasoning problems, suffers the same kind of limitations. Overall, the results of this study suggest that learning same-different relations is beyond the scope of current DCNNs. (@puebla2022can)

Quilty-Dunn, J., Porot, N., and Mandelbaum, E The best game in town: The reemergence of the language-of-thought hypothesis across the cognitive sciences *Behavioral and Brain Sciences*, 46: e261, 2023. **Abstract:** Mental representations remain the central posits of psychology after many decades of scrutiny. However, there is no consensus about the representational format(s) of biological cognition. This paper provides a survey of evidence from computational cognitive psychology, perceptual psychology, developmental psychology, comparative psychology, and social psychology, and concludes that one type of format that routinely crops up is the language-of-thought (LoT). We outline six core properties of LoTs: (i) discrete constituents; (ii) role-filler independence; (iii) predicate-argument structure; (iv) logical operators; (v) inferential promiscuity; and (vi) abstract content. These properties cluster together throughout cognitive science. Bayesian computational modeling, compositional features of object perception, complex infant and animal reasoning, and automatic, intuitive cognition in adults all implicate LoT-like structures. Instead of regarding LoT as a relic of the previous century, researchers in cognitive science and philosophy-of-mind must take seriously the explanatory breadth of LoT-based architectures. We grant that the mind may harbor many formats and architectures, including iconic and associative structures as well as deep-neural-network-like architectures. However, as computational/representational approaches to the mind continue to advance, classical compositional symbolic structures - that is, LoTs - only prove more flexible and well-supported over time. (@quilty2023best)

Radford, A., Kim, J.¬†W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et¬†al Learning transferable visual models from natural language supervision In *International conference on machine learning*, pp.¬†8748‚Äì8763. PMLR, 2021. **Abstract:** State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP. (@radford2021learning)

Raghu, M., Unterthiner, T., Kornblith, S., Zhang, C., and Dosovitskiy, A Do vision transformers see like convolutional neural networks? *Advances in neural information processing systems*, 34: 12116‚Äì12128, 2021. **Abstract:** Convolutional neural networks (CNNs) have so far been the de-facto model for visual data. Recent work has shown that (Vision) Transformer models (ViT) can achieve comparable or even superior performance on image classification tasks. This raises a central question: how are Vision Transformers solving these tasks? Are they acting like convolutional networks, or learning entirely different visual representations? Analyzing the internal representation structure of ViTs and CNNs on image classification benchmarks, we find striking differences between the two architectures, such as ViT having more uniform representations across all layers. We explore how these differences arise, finding crucial roles played by self-attention, which enables early aggregation of global information, and ViT residual connections, which strongly propagate features from lower to higher layers. We study the ramifications for spatial localization, demonstrating ViTs successfully preserve input spatial information, with noticeable effects from different classification methods. Finally, we study the effect of (pretraining) dataset scale on intermediate features and transfer learning, and conclude with a discussion on connections to new architectures such as the MLP-Mixer. (@raghu2021vision)

Ricci, M., Cad√®ne, R., and Serre, T Same-different conceptualization: a machine vision perspective *Current Opinion in Behavioral Sciences*, 37: 47‚Äì55, 2021. **Abstract:** Energy is one of the fundamental concepts of science in all disciplines. For this reason, it can serve as a concept that crosses disciplinary lines and serves as a bridge for students trying to describe a scientific phenomenon using different lenses. Underlying this vision, which is highlighted by the Framework for K-12 Science Education is the implicit assumption that the different disciplinary perspectives of energy have something in common, which should be the focus of instruction and supports the way scientists in the different disciplines use energy. However, does a ‚Äúunified conception‚Äù of energy actually underlie the ways diverse scientists use energy in their fields? To answer this question, we conducted a small-scale interview study in which we interviewed 30 top-level interdisciplinary researchers and asked them to explain several phenomena from different disciplines; all phenomena could be explained in various ways, one of which was an energetic explanation. Our results suggest that researchers from different disciplines do not think of energy in the same way and do not think of energy as an interdisciplinary concept. We argue whether teaching energy in an interdisciplinary way may support the development of future scientists and lay citizens or an expectation that may add more difficulty to an already difficult task. (@ricci2021same)

Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et¬†al Imagenet large scale visual recognition challenge *International journal of computer vision*, 115: 211‚Äì252, 2015. **Abstract:** The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classiÔ¨Åcation and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than Ô¨Åfty insti- tutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key break- throughs in categorical object recognition, provide a detailed analysis of the current state of the Ô¨Åeld of large-scale image classiÔ¨Åcation and object detection, and compare the state-of- the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements. (@russakovsky2015imagenet)

Savarese, P., Silva, H., and Maire, M Winning the lottery with continuous sparsification *Advances in neural information processing systems*, 33: 11380‚Äì11390, 2020. **Abstract:** The search for efficient, sparse deep neural network models is most prominently performed by pruning: training a dense, overparameterized network and removing parameters, usually via following a manually-crafted heuristic. Additionally, the recent Lottery Ticket Hypothesis conjectures that, for a typically-sized neural network, it is possible to find small sub-networks which, when trained from scratch on a comparable budget, match the performance of the original dense counterpart. We revisit fundamental aspects of pruning algorithms, pointing out missing ingredients in previous approaches, and develop a method, Continuous Sparsification, which searches for sparse networks based on a novel approximation of an intractable $\\}ell_0$ regularization. We compare against dominant heuristic-based methods on pruning as well as ticket search ‚Äì finding sparse subnetworks that can be successfully re-trained from an early iterate. Empirical results show that we surpass the state-of-the-art for both objectives, across models and datasets, including VGG trained on CIFAR-10 and ResNet-50 trained on ImageNet. In addition to setting a new standard for pruning, Continuous Sparsification also offers fast parallel ticket search, opening doors to new applications of the Lottery Ticket Hypothesis. (@savarese2020winning)

Schwenk, D., Khandelwal, A., Clark, C., Marino, K., and Mottaghi, R A-okvqa: A benchmark for visual question answering using world knowledge In *European Conference on Computer Vision*, pp.¬†146‚Äì162. Springer, 2022. **Abstract:** The Visual Question Answering (VQA) task aspires to provide a meaningful testbed for the development of AI models that can jointly reason over visual and natural language inputs. Despite a proliferation of VQA datasets, this goal is hindered by a set of common limitations. These include a reliance on relatively simplistic questions that are repetitive in both concepts and linguistic structure, little world knowledge needed outside of the paired image, and limited reasoning required to arrive at the correct answer. We introduce A-OKVQA , a crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer. In contrast to the existing knowledge-based VQA datasets, the questions generally cannot be answered by simply querying a knowledge base, and instead require some form of commonsense reasoning about the scene depicted in the image. We demonstrate the potential of this new dataset through a detailed analysis of its contents and baseline performance measurements over a variety of state-of-the-art vision‚Äìlanguage models. http://a-okvqa.allenai.org/ 1 Introduction The original conception of the Visual Question Answering (VQA) problem was as a Visual Turing Test \[ 15\]. Can we give a computer an image and expect it to answer any question we ask to fool us into thinking it is a human? To truly solve this Turing Test, the computer would need to mimic several human capabilities including: visual recognition in the wild, language understanding, basic reasoning capabilities and a background knowledge about the world. Since the VQA problem was formulated, many of these aspects have been studied. Early datasets mostly studied the perception and language understanding problem on natural image datasets \[ 2,34,16\]. Other datasets studied complex chains of reasoning about procedurally generated images \[ 25\]. More recently, datasets include questions which require factual \[36, 56, 57\] or commonsense knowledge \[66\]. But, to a large extent, VQA has been a victim of its own success. With the advent of large-scale pre-training of vision and language models \[ 67,62,32,33,12,43,8\] and other breakthroughs in multi-modal architectures, much of the low-hanging fruit in the field has been plucked and many of the benchmark datasets have seen saturated performance. Even performance on the newer knowledge- based datasets has been improved by such models \[ 67\]. So how can we continue developing yet more challenging d (@schwenk2022okvqa)

Selvaraju, R.¬†R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., and Batra, D Grad-cam: Visual explanations from deep networks via gradient-based localization In *Proceedings of the IEEE international conference on computer vision*, pp.¬†618‚Äì626, 2017. **Abstract:** We propose a technique for producing ‚Äòvisual explanations‚Äô for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for ‚Äòdog‚Äô or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad- CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a ‚Äòstronger‚Äô deep network from a ‚Äòweaker‚Äô one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV \[2\] and video at youtu.be/COjUB9Izk6E. (@selvaraju2017grad)

Simonyan, K., Vedaldi, A., and Zisserman, A Deep inside convolutional networks: visualising image classification models and saliency maps In *Proceedings of the International Conference on Learning Representations (ICLR)*. ICLR, 2014. **Abstract:** This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score \[Erhan et al., 2009\], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks \[Zeiler et al., 2013\]. (@simonyan2014deep)

Tartaglini, A.¬†R., Feucht, S., Lepori, M.¬†A., Vong, W.¬†K., Lovering, C., Lake, B.¬†M., and Pavlick, E Deep neural networks can learn generalizable same-different visual relations *arXiv preprint arXiv:2310.09612*, 2023. **Abstract:** Although deep neural networks can achieve human-level performance on many object recognition benchmarks, prior work suggests that these same models fail to learn simple abstract relations, such as determining whether two objects are the same or different. Much of this prior work focuses on training convolutional neural networks to classify images of two same or two different abstract shapes, testing generalization on within-distribution stimuli. In this article, we comprehensively study whether deep neural networks can acquire and generalize same-different relations both within and out-of-distribution using a variety of architectures, forms of pretraining, and fine-tuning datasets. We find that certain pretrained transformers can learn a same-different relation that generalizes with near perfect accuracy to out-of-distribution stimuli. Furthermore, we find that fine-tuning on abstract shapes that lack texture or color provides the strongest out-of-distribution generalization. Our results suggest that, with the right approach, deep neural networks can learn generalizable same-different visual relations. (@tartaglini2023deep)

Thompson, R.¬†K. and Oden, D.¬†L Categorical perception and conceptual judgments by nonhuman primates: The paleological monkey and the analogical ape *Cognitive Science*, 24 (3): 363‚Äì396, 2000. **Abstract:** Studies of the conceptual abilities of nonhuman primates demonstrate the substantial range of these abilities as well as their limitations. Such abilities range from categorization on the basis of shared physical attributes, associative relations and functions to abstract concepts as reflected in analogical reasoning about relations between relations. The pattern of results from these studies point to a fundamental distinction between monkeys and apes in both their implicit and explicit conceptual capacities. Monkeys, but not apes, might be best regarded as ‚Äúpaleo‚Äêlogicans‚Äù in the sense that they form common class concepts of identity on the basis of identical predicates (i.e., shared features). The discrimination of presumably more abstract relations commonly involves relatively simple procedural strategies mediated by associative processes likely shared by all mammals. There is no evidence that monkeys can perceive, let alone judge, relations‚Äêbetween‚Äêrelations. This analogical conceptual capacity is found only in chimpanzees and humans. Interestingly, the ‚Äúanalogical ape,‚Äù like the child, can make its analogical knowledge explicit only if it is first provided with a symbol system by which propositional representations can be encoded and manipulated. (@thompson2000categorical)

Thrush, T., Jiang, R., Bartolo, M., Singh, A., Williams, A., Kiela, D., and Ross, C Winoground: Probing vision and language models for visio-linguistic compositionality In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp.¬†5238‚Äì5248, 2022. **Abstract:** We present a novel task and dataset for evaluating the ability of vision and language models to conduct visio-linguistic compositional reasoning, which we call Winoground. Given two images and two captions, the goal is to match them correctly-but crucially, both captions contain a completely identical set of words, only in a different order. The dataset was carefully hand-curated by expert annotators and is labeled with a rich set offine-grained tags to assist in analyzing model performance. We probe a diverse range of state-of-the-art vision and language models and find that, surprisingly, none of them do much better than chance. Evidently, these models are not as skilled at visio-linguistic compositional reasoning as we might have hoped. We perform an extensive analysis to obtain insights into how future work might try to mitigate these models‚Äô shortcomings. We aim for Winoground to serve as a useful evaluation set for advancing the state of the art and driving further progress in the field. The dataset is available at https://huggingface.co/datasets/facebook/winoground. (@thrush2022winoground)

Traylor, A., Merullo, J., Frank, M.¬†J., and Pavlick, E Transformer mechanisms mimic frontostriatal gating operations when trained on human working memory tasks *arXiv preprint arXiv:2402.08211*, 2024. **Abstract:** Models based on the Transformer neural network architecture have seen success on a wide variety of tasks that appear to require complex "cognitive branching" ‚Äì or the ability to maintain pursuit of one goal while accomplishing others. In cognitive neuroscience, success on such tasks is thought to rely on sophisticated frontostriatal mechanisms for selective \\}textit{gating}, which enable role-addressable updating ‚Äì and later readout ‚Äì of information to and from distinct "addresses" of memory, in the form of clusters of neurons. However, Transformer models have no such mechanisms intentionally built-in. It is thus an open question how Transformers solve such tasks, and whether the mechanisms that emerge to help them to do so bear any resemblance to the gating mechanisms in the human brain. In this work, we analyze the mechanisms that emerge within a vanilla attention-only Transformer trained on a simple sequence modeling task inspired by a task explicitly designed to study working memory gating in computational cognitive neuroscience. We find that, as a result of training, the self-attention mechanism within the Transformer specializes in a way that mirrors the input and output gating mechanisms which were explicitly incorporated into earlier, more biologically-inspired architectures. These results suggest opportunities for future research on computational similarities between modern AI architectures and models of the human brain. (@traylor2024transformer)

Ullman, S Visual routines In *Readings in computer vision*, pp.¬†298‚Äì328. Elsevier, 1987. **Abstract:** Detecting causal relations structures our perception of events in the world. Here, we determined for visual interactions whether generalized (i.e., feature-invariant) or specialized (i.e., feature-selective) visual routines underlie the perception of causality. To this end, we applied a visual adaptation protocol to assess the adaptability of specific features in classical launching events of simple geometric shapes. We asked observers to report whether they observed a launch or a pass in ambiguous test events (i.e., the overlap between two discs varied from trial to trial). After prolonged exposure to causal launch events (the adaptor) defined by a particular set of features (i.e., a particular motion direction, motion speed, or feature conjunction), observers were less likely to see causal launches in subsequent ambiguous test events than before adaptation. Crucially, adaptation was contingent on the causal impression in launches as demonstrated by a lack of adaptation in non-causal control events. We assessed whether this negative aftereffect transfers to test events with a new set of feature values that were not presented during adaptation. Processing in specialized (as opposed to generalized) visual routines predicts that the transfer of visual adaptation depends on the feature-similarity of the adaptor and the test event. We show that the negative aftereffects do not transfer to unadapted launch directions but do transfer to launch events of different speed. Finally, we used colored discs to assign distinct feature-based identities to the launching and the launched stimulus. We found that the adaptation transferred across colors if the test event had the same motion direction as the adaptor. In summary, visual adaptation allowed us to carve out a visual feature space underlying the perception of causality and revealed specialized visual routines that are tuned to a launch‚Äôs motion direction. Significance statement We used visual adaptation to carve out the visual feature space critical for detecting collisions in visual launching events. Observers were less likely to report perceiving a collision after the repeated viewing of one disc colliding with and launching the movement of another disc. Importantly, observers‚Äô perception of collisions in the direction opposite to the adaptor remained unaffected. However, provided the test stimulus had the adapted direction of motion, the speed or the color of the objects involved in the launching event did not need to be the same as during adaptation for a negative aftereffect to occur. Thus, visual routines that detect causal interactions are selective for motion direction, providing evidence that the perception of causality in a visual interaction relies on low-level perceptual processes. (@ullman1987visual)

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.¬†N., Kaiser, ≈Å., and Polosukhin, I Attention is all you need *Advances in neural information processing systems*, 30, 2017. **Abstract:** The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. (@vaswani2017attention)

Vilas, M.¬†G., Schauml√∂ffel, T., and Roig, G Analyzing vision transformers for image classification in class embedding space *Advances in Neural Information Processing Systems*, 36, 2024. **Abstract:** Despite the growing use of transformer models in computer vision, a mechanistic understanding of these networks is still needed. This work introduces a method to reverse-engineer Vision Transformers trained to solve image classification tasks. Inspired by previous research in NLP, we demonstrate how the inner representations at any level of the hierarchy can be projected onto the learned class embedding space to uncover how these networks build categorical representations for their predictions. We use our framework to show how image tokens develop class-specific representations that depend on attention mechanisms and contextual information, and give insights on how self-attention and MLP layers differentially contribute to this categorical composition. We additionally demonstrate that this method (1) can be used to determine the parts of an image that would be important for detecting the class of interest, and (2) exhibits significant advantages over traditional linear probing approaches. Taken together, our results position our proposed framework as a powerful tool for mechanistic interpretability and explainability research. (@vilas2024analyzing)

Wang, K.¬†R., Variengien, A., Conmy, A., Shlegeris, B., and Steinhardt, J Interpretability in the wild: a circuit for indirect object identification in gpt-2 small In *The Eleventh International Conference on Learning Representations*, 2022. **Abstract:** Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI). Our explanation encompasses 26 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches relying on causal interventions. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior "in the wild" in a language model. We evaluate the reliability of our explanation using three quantitative criteria‚Äìfaithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks. (@wang2022interpretability)

Wu, Z., Geiger, A., Arora, A., Huang, J., Wang, Z., Goodman, N.¬†D., Manning, C.¬†D., and Potts, C pyvene: A library for understanding and improving pytorch models via interventions *arXiv preprint arXiv:2403.07809*, 2024. **Abstract:** Interventions on model-internal states are fundamental operations in many areas of AI, including model editing, steering, robustness, and interpretability. To facilitate such research, we introduce $\\}textbf{pyvene}$, an open-source Python library that supports customizable interventions on a range of different PyTorch modules. $\\}textbf{pyvene}$ supports complex intervention schemes with an intuitive configuration format, and its interventions can be static or include trainable parameters. We show how $\\}textbf{pyvene}$ provides a unified and extensible framework for performing interventions on neural models and sharing the intervened upon models with others. We illustrate the power of the library via interpretability analyses using causal abstraction and knowledge localization. We publish our library through Python Package Index (PyPI) and provide code, documentation, and tutorials at https://github.com/stanfordnlp/pyvene. (@wu2024pyvene)

Wu, Z., Geiger, A., Huang, J., Arora, A., Icard, T., Potts, C., and Goodman, N.¬†D A reply to makelov et al.(2023)‚Äôs" interpretability illusion" arguments *arXiv preprint arXiv:2401.12631*, 2024. **Abstract:** We respond to the recent paper by Makelov et al. (2023), which reviews subspace interchange intervention methods like distributed alignment search (DAS; Geiger et al. 2023) and claims that these methods potentially cause "interpretability illusions". We first review Makelov et al. (2023)‚Äôs technical notion of what an "interpretability illusion" is, and then we show that even intuitive and desirable explanations can qualify as illusions in this sense. As a result, their method of discovering "illusions" can reject explanations they consider "non-illusory". We then argue that the illusions Makelov et al. (2023) see in practice are artifacts of their training and evaluation paradigms. We close by emphasizing that, though we disagree with their core characterization, Makelov et al. (2023)‚Äôs examples and discussion have undoubtedly pushed the field of interpretability forward. (@wu2024reply)

Wu, Z., Geiger, A., Icard, T., Potts, C., and Goodman, N Interpretability at scale: Identifying causal mechanisms in alpaca *Advances in Neural Information Processing Systems*, 36, 2024. **Abstract:** Obtaining human-interpretable explanations of large, general-purpose language models is an urgent goal for AI safety. However, it is just as important that our interpretability methods are faithful to the causal dynamics underlying model behavior and able to robustly generalize to unseen inputs. Distributed Alignment Search (DAS) is a powerful gradient descent method grounded in a theory of causal abstraction that has uncovered perfect alignments between interpretable symbolic algorithms and small deep learning models fine-tuned for specific tasks. In the present paper, we scale DAS significantly by replacing the remaining brute-force search steps with learned parameters ‚Äì an approach we call Boundless DAS. This enables us to efficiently search for interpretable causal structure in large language models while they follow instructions. We apply Boundless DAS to the Alpaca model (7B parameters), which, off the shelf, solves a simple numerical reasoning problem. With Boundless DAS, we discover that Alpaca does this by implementing a causal model with two interpretable boolean variables. Furthermore, we find that the alignment of neural representations with these variables is robust to changes in inputs and instructions. These findings mark a first step toward faithfully understanding the inner-workings of our ever-growing and most widely deployed language models. Our tool is extensible to larger LLMs and is released publicly at ‚Äòhttps://github.com/stanfordnlp/pyvene‚Äò. (@wu2024interpretability)

Yuksekgonul, M., Bianchi, F., Kalluri, P., Jurafsky, D., and Zou, J When and why vision-language models behave like bags-of-words, and what to do about it? In *The Eleventh International Conference on Learning Representations*, 2022. **Abstract:** Despite the success of large vision and language models (VLMs) in many downstream applications, it is unclear how well they encode compositional information. Here, we create the Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order. ARO consists of Visual Genome Attribution, to test the understanding of objects‚Äô properties; Visual Genome Relation, to test for relational understanding; and COCO & Flickr30k-Order, to test for order sensitivity. ARO is orders of magnitude larger than previous benchmarks of compositionality, with more than 50,000 test cases. We show where state-of-the-art VLMs have poor relational understanding, can blunder when linking objects to their attributes, and demonstrate a severe lack of order sensitivity. VLMs are predominantly trained and evaluated on large datasets with rich compositional structure in the images and captions. Yet, training on these datasets has not been enough to address the lack of compositional understanding, and evaluating on these datasets has failed to surface this deficiency. To understand why these limitations emerge and are not represented in the standard tests, we zoom into the evaluation and training procedures. We demonstrate that it is possible to perform well on retrieval over existing datasets without using the composition and order information. Given that contrastive pretraining optimizes for retrieval on datasets with similar shortcuts, we hypothesize that this can explain why the models do not need to learn to represent compositional information. This finding suggests a natural solution: composition-aware hard negative mining. We show that a simple-to-implement modification of contrastive learning significantly improves the performance on tasks requiring understanding of order and compositionality. (@yuksekgonul2022and)

Zeng, Y., Zhang, X., and Li, H Multi-grained vision language pre-training: Aligning texts with visual concepts In *International Conference on Machine Learning*, pp.¬†25994‚Äì26009. PMLR, 2022. **Abstract:** Most existing methods in vision language pre-training rely on object-centric features extracted through object detection and make fine-grained alignments between the extracted features and texts. It is challenging for these methods to learn relations among multiple objects. To this end, we propose a new method called X-VLM to perform ‚Äòmulti-grained vision language pre-training.‚Äô The key to learning multi-grained alignments is to locate visual concepts in the image given the associated texts, and in the meantime align the texts with the visual concepts, where the alignments are in multi-granularity. Experimental results show that X-VLM effectively leverages the learned multi-grained alignments to many downstream vision language tasks and consistently outperforms state-of-the-art methods. (@zeng2022multi)

Zerroug, A., Vaishnav, M., Colin, J., Musslick, S., and Serre, T A benchmark for compositional visual reasoning *Advances in neural information processing systems*, 35: 29776‚Äì29788, 2022. **Abstract:** A fundamental component of human vision is our ability to parse complex visual scenes and judge the relations between their constituent objects. AI benchmarks for visual reasoning have driven rapid progress in recent years with state-of-the-art systems now reaching human accuracy on some of these benchmarks. Yet, there remains a major gap between humans and AI systems in terms of the sample efficiency with which they learn new visual reasoning tasks. Humans‚Äô remarkable efficiency at learning has been at least partially attributed to their ability to harness compositionality - allowing them to efficiently take advantage of previously gained knowledge when learning new tasks. Here, we introduce a novel visual reasoning benchmark, Compositional Visual Relations (CVR), to drive progress towards the development of more data-efficient learning algorithms. We take inspiration from fluid intelligence and non-verbal reasoning tests and describe a novel method for creating compositions of abstract rules and generating image datasets corresponding to these rules at scale. Our proposed benchmark includes measures of sample efficiency, generalization, compositionality, and transfer across task rules. We systematically evaluate modern neural architectures and find that convolutional architectures surpass transformer-based architectures across all performance measures in most data regimes. However, all computational models are much less data efficient than humans, even after learning informative visual representations using self-supervision. Overall, we hope our challenge will spur interest in developing neural architectures that can learn to harness compositionality for more efficient learning. (@zerroug2022benchmark)

</div>

# Dataset Details [App:All_Objects]

<figure id="fig:all_objects">
<img src="./figures/all_objects.png"" />
<figcaption><strong>All 16 unique shapes and colors used to construct the Discrimination and RMTS tasks</strong>. There are thus <span class="math inline">16‚ÄÖ√ó‚ÄÖ16‚ÄÑ=‚ÄÑ256</span> unique objects in our same-different datasets.</figcaption>
</figure>

## Constructing the Objects [App:Construct_Objects]

Figure¬†<a href="#fig:all_objects" data-reference-type="ref" data-reference="fig:all_objects">7</a> demonstrates a single instance of the \\(16\\) shapes and \\(16\\) colors used in our datasets. Any shape can be paired with any color to create one of \\(256\\) unique objects. Note that object colors are not uniform within a given object. Instead, each color is defined by three different Gaussian distributions‚Äîone for each RGB channel in the image‚Äîthat determine the value of each object pixel. For example, the color red is created by these three distributions: \\(\mathcal{N}(\mu=233,\sigma=10)\\) in the red channel, \\(\mathcal{N}(\mu=30,\sigma=10)\\) in the green channel, and \\(\mathcal{N}(\mu=90,\sigma=10)\\) in the blue channel. All color distributions have a variance fixed at \\(10\\) to give them an equal degree of noise. Any sampled values that lie outside of the valid RGB range of \\([0, 255]\\) are clipped to either \\(0\\) or \\(255\\). Object colors are re-randomized for every image, so no two objects have the same pixel values even if they are the same color. This was done to prevent the models from learning simple heuristics like comparing single pixels in each object.

<figure id="fig:data_examples">
<img src="./figures/data_examples.png"" />
<figcaption><strong>More examples of stimuli for the discrimination and RMTS tasks</strong>. The top row shows ‚Äúdifferent‚Äù examples, while the bottom row shows ‚Äúsame‚Äù examples. Note that ‚Äúdifferent‚Äù pairs may differ in one or both dimensions (shape &amp; color).</figcaption>
</figure>

## Constructing the Datasets [App:Construct_Datasets]

The train, validation, and test sets for both the discrimination and RMTS tasks each contain \\(6,400\\) unique stimuli: \\(3,200\\) ‚Äúsame‚Äù and \\(3,200\\) ‚Äúdifferent.‚Äù To construct a given dataset, we first generate all possible same and different pairs of the \\(256\\) unique objects (see Figure¬†<a href="#fig:all_objects" data-reference-type="ref" data-reference="fig:all_objects">7</a>). We consider two objects to be the same if they match in both shape and color‚Äîotherwise, they are different. Next, we randomly select a subset of the possible object pairs to create the stimuli such that each unique object is in at least one pair. For the RMTS dataset, we repeat this process to select same and different pairs of *pairs*.

Each object is resized (from \\(224\times244\\) pixel masks of the object‚Äôs shape) such that it is contained within a single ViT patch for B/32 models or four ViT patches for B/16 & B/14 models. For B/32 and B/16 models, objects are roughly \\(28\times28\\) pixels in size; for B/14 models (DINOv2 only), objects are roughly \\(21\times21\\) pixels in size. These choices in size mean that a single object can be placed in the center of a \\(32\times32\\) (or \\(28\times28\\)) pixel patch with a radius of \\(4\\) pixels of extra space around it. This extra space allows us to randomly jitter object positions within the ViT patches.

To create a stimulus, a pair of objects is placed over a \\(224\times224\\) pixel white background in randomly selected, non-overlapping positions such that objects are aligned with ViT patches. For RMTS stimuli, the second ‚Äúdisplay‚Äù pair is always placed in the top left corner of the image. Each object‚Äôs position (including the ‚Äúdisplay‚Äù objects for RMTS) is also randomly jittered within the ViT patches it occupies. We consider two objects in a specific placement as one unique stimulus‚Äîin other words, a given pair of objects may appear in multiple images but in different positions. All object pairs appear the same number of times to ensure that each unique object is equally represented.

See Figure¬†<a href="#fig:data_examples" data-reference-type="ref" data-reference="fig:data_examples">8</a> for some more examples of stimuli from each task.

<figure id="fig:realistic">
<img src="./figures/realistic.png"" />
<figcaption><strong>Examples of stimuli from our photorealistic same-different evaluation dataset</strong>. The top row contains ‚Äúdifferent‚Äù examples, while the bottom row contains ‚Äúsame‚Äù examples. Stimuli are constructed using 16 unique 3D models of objects placed on a table with a randomized texture; background textures are also randomized. Objects are randomly rotated and may be placed at different distances from the camera or occlude each other.</figcaption>
</figure>

## Photorealistic Test Set [App:Realistic]

In order to ensure the robustness of the two-stage processing we observe in CLIP and DINOv2 on our artificial stimuli, we test models on a highly out-of-distribution photorealistic discrimination task. The test dataset consists of \\(1,024\\) photorealistic same-different stimuli that we generated (see Figure¬†<a href="#fig:realistic" data-reference-type="ref" data-reference="fig:realistic">9</a>). Each stimulus is a 224\\(\times\\)`<!-- -->`{=html}224 pixel image depicting a pair of same or different 3D objects arranged on the surface of a table in a sunlit room. We created these images in Blender, a sophisticated 3D modeling tool, using a set of 16 unique 3D models of different objects that vary in shape, texture and color. To construct the dataset, we first generate all possible pairs of same or different objects, then select a subset of the possible ‚Äúdifferent‚Äù pairs such that each object appears in two pairs. This ensures that all objects are equally represented and that an equal number of ‚Äúsame‚Äù and ‚Äúdifferent‚Äù stimuli are created. We create 32 unique stimuli for each pair of objects by placing them on the table in eight random configurations within the view of four different camera angles, allowing partial occlusions. Each individual object is also randomly rotated around its \\(z\\)-axis in each image‚Äîbecause 11 of the objects lack rotational symmetry, these rotations provide an additional challenge, especially for ‚Äúsame‚Äù classifications.

<figure id="fig:realistic_attention_heads">
<img src="./figures/realistic_attention_heads.png"" />
<figcaption><strong>Attention pattern analysis for CLIP and DINOv2 on the photorealistic discrimination task</strong>. This figure follows the top row in Figure¬†<a href="#fig:attention_heads" data-reference-type="ref" data-reference="fig:attention_heads">2</a>. <strong>(a) CLIP</strong>: As in Figure¬†<a href="#fig:attention_heads" data-reference-type="ref" data-reference="fig:attention_heads">2</a>, WO peaks at layer 3, WP peaks at layer 6, and BG peaks at layer 10. BG attention is higher throughout the perceptual stage, leading to a lower perceptual score compared to the artificial discrimination task (i.e. fewer blue cells). <strong>(b) DINOv2</strong>: The attention pattern exhibits two stages, resembling the artificial setting (although the correspondence is somewhat looser than CLIP‚Äôs, perhaps explaining DINOv2‚Äôs poor zero-shot performance on the photorealistic task). </figcaption>
</figure>

We evaluate models that have been fine-tuned on the discrimination task from the main body of the paper (e.g. Figure¬†<a href="#fig:data_examples" data-reference-type="ref" data-reference="fig:data_examples">8</a>a) in a zero-shot manner on the photorealistic dataset, meaning that there is no additional fine-tuning on the photorealistic dataset. We find that CLIP ViT attains a test accuracy of \\(93.9\\)% on the photorealistic dataset, while all other models attain chance level accuracy (e.g. DINOv2 attains an accuracy of \\(48\\)%). We also find that CLIP performs two-stage processing on the photorealistic stimuli (see Figure¬†<a href="#fig:realistic_attention_heads" data-reference-type="ref" data-reference="fig:realistic_attention_heads">10</a>a), and that the peaks in WO, WP, and BG attention all occur at the same exact layers as the artificial stimuli (i.e. in Figure¬†<a href="#fig:attention_heads" data-reference-type="ref" data-reference="fig:attention_heads">2</a>). DINOv2 also displays similar two-stage processing despite its poor performance on the photorealistic task (see Figure¬†<a href="#fig:realistic_attention_heads" data-reference-type="ref" data-reference="fig:realistic_attention_heads">10</a>b). Note that BG attention for both models is higher overall during the perceptual stage when processing the photorealistic stimuli compared to the artificial stimuli; this is likely because the photorealistic stimuli contain detailed backgrounds, while the backgrounds in the artificial stimuli are blank. Overall, these findings generalize our results from the toy setting presented in the main body of the paper.

# ViT B/16: All Model Behavioral Results [App:All_Model_Table]

See Tables¬†<a href="#tab:vit_b16_all_disc_256" data-reference-type="ref" data-reference="tab:vit_b16_all_disc_256">2</a>, ¬†<a href="#tab:vit_b16_all_disc_32" data-reference-type="ref" data-reference="tab:vit_b16_all_disc_32">3</a>, ¬†<a href="#tab:vit_b16_all_rmts_256" data-reference-type="ref" data-reference="tab:vit_b16_all_rmts_256">4</a>, and¬†<a href="#tab:vit_b16_all_rmts_32" data-reference-type="ref" data-reference="tab:vit_b16_all_rmts_32">5</a> for behavioral results from all ViT-B/16 models trained on discrimination and RMTS tasks with either all 256 shape-color combinations or only 32 shape-color combinations. The ‚ÄúPretraining Scale‚Äù column denotes the number of images (in millions) in a given model‚Äôs pretraining dataset. The models are organized in descending order by pretraining scale. ‚ÄúTest Acc.‚Äù refers to IID test accuracy. ‚ÄúComp. Acc.‚Äù refers to compositional generalization accuracy (for models trained on only 32 shape-color combinations). ‚ÄúRealistic Acc.‚Äù (Table¬†<a href="#tab:vit_b16_all_disc_256" data-reference-type="ref" data-reference="tab:vit_b16_all_disc_256">2</a> only) refers to a model‚Äôs zero-shot accuracy on the photorealistic evaluation dataset. CLIP and DINOv2‚Äîthe two models with a pretraining scale on the order of 100 million images‚Äîattain near perfect test accuracy on the RMTS task. However, only CLIP attains high performance on the photorealistic dataset.

<div id="tab:vit_b16_all_disc_256" markdown="1">

| **Pretrain** | **Pretraining Scale \\(\downarrow\\)** | **Train Acc.** | **Test Acc.** | **Realistic Acc.** |  |
|:--:|:--:|:--:|:--:|:--:|:--:|
| CLIP | 400M | 100% | 99.3% | 93.9% |  |
| DINOv2 | 142M | 100% | 99.5% | 48.0% |  |
| ImageNet | 14.2M | 100% | 97.5% | 53.0% |  |
| DINO | 1.28M | 100% | 95.6% | 50.9% |  |
| MAE | 1.28M | 100% | 98.0% | 52.4% |  |
| ‚Äì | ‚Äì | 95.9% | 80.5% | 49.9% |  |

**All behavioral results for ViT-B/16 models trained on all 256 shape-color combinations on the discrimination task**.

</div>

<div id="tab:vit_b16_all_disc_32" markdown="1">

| **Pretrain** | **Pretraining Scale \\(\downarrow\\)** | **Train Acc.** | **Test Acc.** | **Comp. Acc.** |  |
|:--:|:--:|:--:|:--:|:--:|:--:|
| CLIP | 400M | 98.1% | 98.5% | 98.5% |  |
| DINOv2 | 142M | 99.6% | 98.5% | 98.5% |  |
| ImageNet | 14.2M | 98.1% | 95.7% | 95.7% |  |
| DINO | 1.28M | 98.1% | 92.3% | 94.7% |  |
| MAE | 1.28M | 98.1% | 94.9% | 94.9% |  |
| ‚Äì | ‚Äì | 77.3% | 76.5% | 76.0% |  |

**All behavioral results for ViT-B/16 models trained on 32 shape-color combinations on the discrimination task**.

</div>

<div id="tab:vit_b16_all_rmts_256" markdown="1">

| **Pretrain** | **Pretraining Scale \\(\downarrow\\)** | **Train Acc.** | **Test Acc.** |  |
|:--:|:--:|:--:|:--:|:--:|
| CLIP | 400M | 100% | 98.3% |  |
| DINOv2 | 142M | 100% | 98.2% |  |
| ImageNet | 14.2M | 99.7% | 89.3% |  |
| DINO | 1.28M | 100% | 87.7% |  |
| MAE | 1.28M | 100% | 93.4% |  |
| ‚Äì | ‚Äì | 49.2% | 50.1% |  |

**All behavioral results for ViT-B/16 models trained on all 256 shape-color combinations on the RMTS task**.

</div>

<div id="tab:vit_b16_all_rmts_32" markdown="1">

| **Pretrain** | **Pretraining Scale \\(\downarrow\\)** | **Train Acc.** | **Test Acc.** | **Comp. Acc.** |
|:--:|:--:|:--:|:--:|:--:|
| CLIP | 400M | 100% | 98.0% | 98.3% |
| DINOv2 | 142M | 100% | 96.4% | 94.7% |
| ImageNet | 14.2M | 99.5% | 92.3% | 84.0% |
| DINO | 1.28M | 99.6% | 94.7% | 85.2% |
| MAE | 1.28M | 99.6% | 85.3% | 85.3% |
| ‚Äì | ‚Äì | 49.2% | 50.0% | 50.0% |

**All behavioral results for ViT-B/16 models trained on 32 shape-color combinations on the RMTS task**.

</div>

# CLIP ViT-b32 Model Analyses [App:Clip-b32]

## Attention Pattern Analysis

See Figure¬†<a href="#fig:clip32_attn" data-reference-type="ref" data-reference="fig:clip32_attn">11</a>.

<figure id="fig:clip32_attn">
<img src="./figures/clip32attn.png"" />
<figcaption><strong>CLIP B/32 attention pattern analysis</strong>. See the caption of Figure¬†<a href="#fig:attention_heads" data-reference-type="ref" data-reference="fig:attention_heads">2</a> for figure and legend descriptions. The B/32 model follows the same stages of processing as CLIP ViT-B/16, and WO &amp; WP peak at the same layers (3 and 6 for discrimination respectively; 3 and 5 for RMTS respectively). However, WO attention remains high for longer than B/16 models.</figcaption>
</figure>

## Perceptual Stage Analysis

See Figure¬†<a href="#fig:clip32_das" data-reference-type="ref" data-reference="fig:clip32_das">12</a>.

## Relational Stage Analysis

See Figure¬†<a href="#fig:clip32_novel_representations" data-reference-type="ref" data-reference="fig:clip32_novel_representations">13</a> for novel representations analysis. See Figure¬†<a href="#fig:clip32_Linear_Interventions" data-reference-type="ref" data-reference="fig:clip32_Linear_Interventions">14</a> for linear intervention analysis. We find broadly similar results as CLIP B/16.

<figure id="fig:clip32_das">
<img src="./figures/clip32_das.png"" />
<figcaption><strong>CLIP B/32 DAS results</strong>.</figcaption>
</figure>

<figure id="fig:clip32_novel_representations">
<img src="./figures/clip32-novel-reps.png"" />
<figcaption><strong>CLIP B/32 relational stage analysis: Novel Representations</strong>.</figcaption>
</figure>

<figure id="fig:clip32_Linear_Interventions">
<img src="./figures/clip32-linear.png"" />
<figcaption><strong>CLIP B/32 relational stage analysis: Linear Intervention</strong>.</figcaption>
</figure>

## Generalization Results

We present CLIP-B/32 model results for models finetuned on all shape-color combinations, as well as only 32 shape-color combinations, as in Section¬†<a href="#sec:disentanglement" data-reference-type="ref" data-reference="sec:disentanglement">6</a>. We present compositional generalization accuracy (when applicable) as well as OOD generalization accuracy. We find that all models perform quite well in-distribution *and* under compositional generalization. Accuracy drops somewhat for RMTS OOD stimuli. All results are in presented in Table¬†<a href="#tab:clip_b32_generalization" data-reference-type="ref" data-reference="tab:clip_b32_generalization">6</a>.

<div id="tab:clip_b32_generalization" markdown="1">

| **Task** | **\# Combinations Seen** | **Train Acc.** | **IID Test Acc.** | **Comp. Gen. Acc.** | **OOD Acc.** |
|:--:|:--:|:--:|:--:|:--:|:--:|
| Disc. | All | 100% | 99.6% | N/A | 95.8% |
| Disc. | 32 | 99.7% | 98.5% | 98.5% | 98.0% |
| RMTS | All | 100% | 97.4% | N/A | 90.5% |
| RMTS | 32 | 100% | 98.3% | 98.3% | 86.2% |

**All behavioral results for CLIP-B/32 models**.

</div>

# DINOv2 Analyses [App:Dinov2_Analyses]

## Attention Map Analysis [App:Dinov2_Analyses_Attention_Map]

<figure id="fig:dinov2_attn">
<img src="./figures/dinov2_heatmaps.png"" />
<figcaption><strong>DINOv2 attention pattern analysis</strong>. See the caption of Figure¬†<a href="#fig:attention_heads" data-reference-type="ref" data-reference="fig:attention_heads">2</a> for figure and legend descriptions. Note that the stars in the line charts are placed differently in this figure compared to other attention pattern analysis figures. Instead of marking the maximal values of each type of attention across all 12 layers, the stars mark the maximal value excluding the 0th layer. This is because all types of attention spike in DINOv2 in the 0th layer.</figcaption>
</figure>

See Figure¬†<a href="#fig:dinov2_attn" data-reference-type="ref" data-reference="fig:dinov2_attn">15</a> for DINOv2 attention pattern analyses. Like CLIP, DINOv2 displays two-stage processing (albeit somewhat less cleanly). One notable difference compared to CLIP is that all types of attention (WO, WP, BP, and BG) spike in the 0th layer. This might be related to DINOv2‚Äôs positional encodings. Since the model was pretrained on images with a size of \\(518\times518\\) pixels, the model‚Äôs positional encodings are interpolated to process our \\(224\times224\\) stimuli; this might cause an artifact in the attention patterns in the very beginning of the model. Disregarding this spike, the stages of processing follow CLIP. In the discrimination task (Figure¬†<a href="#fig:dinov2_attn" data-reference-type="ref" data-reference="fig:dinov2_attn">15</a>a), within-object attention peaks at layer 3 (disregarding the initial peak), followed by within-pair and finally background attention. In the RMTS task (Figure¬†<a href="#fig:dinov2_attn" data-reference-type="ref" data-reference="fig:dinov2_attn">15</a>b), within-object attention peaks at layer 3, followed by within-pair attention at layer 8, and finally between-pair attention in the final layer. Background attention remains relatively high throughout the model, indicating that DINOv2 might make greater use of register tokens to solve the RMTS task compared to other models.

## Perceptual Stage Analysis

See Figure¬†<a href="#fig:dinov2_das" data-reference-type="ref" data-reference="fig:dinov2_das">16</a> for perceptual stage analysis of DINOV2-pretrained ViTs. Overall, we find highly disentangled object representations in these models. <span id="App:Dinov2_Analyses_Perceptual" label="App:Dinov2_Analyses_Perceptual"></span>

<figure id="fig:dinov2_das">
<img src="./figures/dinov2_das.png"" />
<figcaption><strong>DAS results for DINOv2 ViT-B/14</strong>.</figcaption>
</figure>

## Relational Stage Analysis [App:Dinov2_Analyses_Relational]

<figure id="fig:nra_dinov2_disc">
<img src="./figures/nra_dinov2_disc.png"" />
<figcaption><strong>Novel Representation Analysis for DINOv2 ViT-B/14 (Disc.)</strong>.</figcaption>
</figure>

<figure id="fig:nra_dinov2_rmts">
<img src="./figures/nra_dinov2_rmts.png"" />
<figcaption><strong>Novel Representation Analysis for DINOv2 ViT-B/14 (RMTS)</strong>.</figcaption>
</figure>

### Novel Representation Analysis

See Figure¬†<a href="#fig:nra_dinov2_disc" data-reference-type="ref" data-reference="fig:nra_dinov2_disc">17</a> and ¬†<a href="#fig:nra_dinov2_rmts" data-reference-type="ref" data-reference="fig:nra_dinov2_rmts">18</a> for novel representation analysis of DINOV2-pretrained ViTs for the discrimination and RMTS tasks. These results replicate those found using CLIP-pretrained ViTs.

### Abstract Representations of *Same* and *Different*

See Figure¬†<a href="#fig:dinov2_linear" data-reference-type="ref" data-reference="fig:dinov2_linear">19</a> for linear probing and intervention results for DINOV2-pretrained ViTs. We find that the intervention works extremely well for these models, replicating our results on CLIP-pretrained ViTs.

<figure id="fig:dinov2_linear">
<img src="./figures/dinov2_alpha.png"" />
<figcaption><strong>Linear probe &amp; intervention analysis for DINOv2 ViT-B/14</strong>.</figcaption>
</figure>

# Attention Pattern Analyses [App:All_Model_Attention]

<figure id="fig:all_vit_attn">
<img src="./figures/all_vit_attn.png"" />
<figcaption><strong>ImageNet, DINO, and MAE ViT attention pattern analysis</strong>. See the caption of Figure¬†<a href="#fig:attention_heads" data-reference-type="ref" data-reference="fig:attention_heads">2</a> for figure and legend descriptions. Like CLIP and DINOv2, ImageNet ViT displays two-stage processing on both the discrimination and RMTS tasks; however, performance of this model lags behind CLIP and DINOv2, possibly due to smaller pretraining scale (see the Pretraining Scale column in Table¬†<a href="#tab:vit_b16_all_disc_256" data-reference-type="ref" data-reference="tab:vit_b16_all_disc_256">2</a>). DINO and MAE do not display two-stage processing. These two models are also pretrained on the smallest amount of data, further supporting our intuition that pretraining scale rather than objective results in two-stage processing.</figcaption>
</figure>

See Figure¬†<a href="#fig:all_vit_attn" data-reference-type="ref" data-reference="fig:all_vit_attn">20</a> for attention pattern analyses for ImageNet, DINO, and MAE ViT on the Discrimination and RMTS tasks. ImageNet loosely demonstrates two-stage processing like CLIP and DINOv2. On the other hand, DINO and MAE do not display two stage processing; instead, local and global processing appears to be mixed throughout the models. DINO and MAE also receive the smallest scale pretraining compared to the other models (see the Pretraining Scale column in Table¬†<a href="#tab:vit_b16_all_disc_256" data-reference-type="ref" data-reference="tab:vit_b16_all_disc_256">2</a>); this provides further support for our intuition that pretraining scale results in two-stage processing.

# Two Internal Algorithms Examined in Greater Detail [App:Attn_Map_Examples]

While the attention head analysis in Section¬†<a href="#sec:two-stage" data-reference-type="ref" data-reference="sec:two-stage">3</a> shows that different models use qualitatively different internal algorithms to solve same-different tasks, the specific computations involved in these algorithms are less clear. What exactly is happening during CLIP‚Äôs perceptual stage, for example? In this section, we seek to build an intuitive picture of the algorithms learned by two models on the discrimination task: CLIP ViT-B/16, and a randomly initialized ViT-B/16 (From Scratch).

To do this, we examine the attention patterns produced by individual attention heads throughout each model. Figure¬†<a href="#fig:attn_pattern_app" data-reference-type="ref" data-reference="fig:attn_pattern_app">21</a> displays attention patterns extracted from four randomly selected individual attention heads (black & white heatmaps) in response to the input image on the left. For CLIP, the examined heads are: layer 1, head 5 (local head); layer 5, head 9 (local head); layer 6, head 11 (global head); and layer 10, head 6 (global head). For the from scratch model, the heads are: layer 1, head 8; layer 5, head 11; layer 6, head 3; and layer 10, head 8. For visualization purposes, the attention patterns are truncated to include only the indices of the two objects‚Äô tokens; since each object occupies four ViT patches, this results in an \\(8\times8\\) grid for each attention head. The `src` axis (\\(y\\)-axis) in Figure¬†<a href="#fig:attn_pattern_app" data-reference-type="ref" data-reference="fig:attn_pattern_app">21</a> indicates the source token, while the `dest` axis (\\(x\\)-axis) indicates the destination token (attention flows from `src`\\(\longrightarrow\\)`dest`). The actual tokens in the input image are also visualized along these axes.

Based on these attention patterns, we visualize how CLIP processes an image to solve the discrimination task in Figure¬†<a href="#fig:clip_flowchart" data-reference-type="ref" data-reference="fig:clip_flowchart">22</a>. **1. Embedding**: The model first tokenizes the image and embeds these tokens. Each object occupies four ViT-B/16 patches, so the objects are divided up into four tokens each. **2. Layer 1, Head 5**: During the early perceptual stage, the local attention heads appear to aid in the formation of object representations by performing low-level comparisons within objects. For example, head 5 in layer 1 compares object tokens from left to right within each object. Other attention heads in this layer perform such comparisons in other directions, such as right to left or top to bottom. **3. Layer 5, Head 9**: Towards the end of the perceptual stage, all object tokens within a single object attend to all other tokens within the same object. The four object tokens comprising each object have been pushed together in the latent space, and the model now ‚Äúsees‚Äù a single object as a whole. **4. Layer 6, Head 11**: The model switches from predominantly local to predominantly global attention in this layer, and within-pair (WP) attention peaks. The whole-object representations formed during the perceptual stage now attend to each other, indicating that the model is comparing them. **5. Layer 10, Head 6**: The model appears to utilize object tokens (and background tokens) to store information, possibly the classification decision).

<figure id="fig:attn_pattern_app">
<img src="./figures/attns.png"" />
<figcaption><strong>Example attention head patterns for models trained on the discrimination task</strong>. <strong>(a) CLIP ViT-B/16</strong>: On the left is an example input image, which is fed into the model. The heatmap is the same as Figure¬†<a href="#fig:attention_heads" data-reference-type="ref" data-reference="fig:attention_heads">2</a>a‚Äîthe <span class="math inline"><em>x</em></span> and <span class="math inline"><em>y</em></span>-axes denote model layer and head index respectively, and the colors indicate the type of attention head as defined in Section¬†<a href="#sec:two-stage" data-reference-type="ref" data-reference="sec:two-stage">3</a> (local heads<span class="math inline">=</span><span style="color: 5167FF">blue</span>, global heads<span class="math inline">=</span><span style="color: FF002B">red</span>). The attention patterns of four attention heads for this input image are displayed in black &amp; white heatmaps below; white indicates higher attention values. The <code>src</code> axis indicates the source token, which is visually marked‚Äîrecall that each object occupies four tokens each. The <code>dest</code> axis indicates the destination token. Individual objects attend to themselves during the perceptual stage (layers 0-5); objects begin to attend to the other object during the relational stage (layer 6 onwards). <strong>(b) From Scratch ViT-B/16</strong>: The analysis in (a) is repeated for a from scratch model trained on discrimination. The attention patterns are less interpretable throughout.</figcaption>
</figure>

<figure id="fig:clip_flowchart">
<img src="./figures/clip_flowchart.png"" />
<figcaption><strong>How CLIP ViT-B/16 processes an example from the discrimination task</strong>. Four attention heads are randomly selected from different stages in CLIP and analyzed on a single input image (see Figure¬†<a href="#fig:attn_pattern_app" data-reference-type="ref" data-reference="fig:attn_pattern_app">21</a>). <strong>1. Embedding</strong>: The model first tokenizes the input image. Each object occupies four ViT patches. <strong>2. Layer 1, Head 5</strong>: During the perceptual stage, the model first performs low-level visual operations between tokens of individual objects. This particular attention head performs left-to-right attention within objects. <strong>3. Layer 5, Head 9</strong>: Near the end of the perceptual stage, whole-object representations have been formed. <strong>4. Layer 6, Head 11</strong>: During the relational stage, the whole-object representations are compared. <strong>5. Layer 10, Head 6</strong>: Object and background tokens are used as registers to store information‚Äîpresumably the classification.</figcaption>
</figure>

# Distributed Alignment Search Technical Details [App:DAS]

#### Approach

We apply a form of distributed alignment search `\citep{geiger2024finding}`{=latex} in order to assess whether the object representations formed by the perceptual stage of ViTs are disentangled with respect to shape and color (the two axes of variation present in our dataset). For ViT B/32 models, each object is contained within the bounds of a single patch, making DAS straightforward to run: we train an intervention over model representations corresponding to the patches containing individual objects that we wish to use as source and base tokens. For ViT B/16 models, each object is contained in four patches. Here, we train a single intervention that is shared between all four patches comprising the base and source objects. Importantly, because we wish to isolate *whole object representations*, rather than *patch representations*, we randomly shuffle the 4 patches comprising the source object before patching information into the base object. For example, the top-right patch of the base object might be injected with information from the bottom-left patch of the source object. This intervention should only succeed if the model contains a disentangled representation of the whole object, and if this representation is present in all four patches comprising that object. Given these stringent conditions, it is all the more surprising that DAS succeeds. During test, we intervene in a patch-aligned manner: The vector patched into the top-right corner of the base image representation is extracted from the top-right corner of the source image.

#### Data

To train the DAS intervention, we must generate counterfactual datasets for every subspace that we wish to isolate. To generate a discrimination dataset that will be used to identify a color subspace, for example, we find examples in the model‚Äôs training set where objects only differ along the color dimension (e.g., `object`\\(_1\\) expresses `color`\\(_1\\), `object`\\(_2\\) expressed `color`\\(_2\\). We randomly select one object to intervene on and generate a counterfactual image. WLOG consider intervening on `object`\\(_1\\). Our counterfactual image contains one object (the counterfactual object) that expresses `color`\\(_2\\). Our intervention is optimized to extract color information from the counterfactual object and inject it into object\\(_1\\), changing the model‚Äôs overall discrimination judgment from *different* to *same*. Importantly, the counterfactual image label is also *different*. Thus, our intervention is designed to work *only* if the intervention transfers color information. We follow a similar procedure for to generate counterfactuals that can be used to turn a *same* image into a *different* image. In this case, both base and counterfactual images are labelled *same*, but the counterfactual *same* image contains objects that are a different color than those in the base image. The counterfactual color is patched into one of the objects in the base image, rendering the objects in the base image *different* along the color axis.

For the RMTS DAS dataset, we generate counterfactuals similarly to the discrimination dataset. We select a pair of objects randomly (except *either* the display pair or sample pair). We then choose the source object in the other pair. We edit the color or shape property of just this source object, and use this as the counterfactual. For these datasets, the data is balanced such that 50% of overall labels are changed from *same* to *different*, but also 50% of intermediate pair labels are changed from *same* to *different*. Note that flipping one intermediate label necessarily flips the hierarchical label. Thus, if the source object is in a pair expressing the *same* relationship, then the counterfactual image will have the opposite label as the base image before intervention. In these cases, the intervention could succeed by transferring the hierarchical image label, rather than by transferring particular color or shape properties from one object to another. However, this only occurs approximately 50% of the time. That is because it occurs in 100% of samples when both pairs exhibit *same*, which occurs 25% of the time (half of the hierarchical *same* images), and 50% of the time when one pair exhibits *same* and the other exhibits *different*, which occurs 50% of the time (all of the hierarchical different images). However, this strategy provides exactly the incorrect incorrect result in the other 50% of cases. Nonetheless, this behavior might explain why RMTS DAS results maintain at around 50% deeper into the model.

For all datasets, we generate train counterfactual pairs from the model train split, validation pairs from the validation split, and test pairs from the model test split. We generate 2,000 counterfactual pairs for both splits. Note that in the case of models trained in the compositional generalization experiments (i.e. those found in Section¬†<a href="#sec:disentanglement" data-reference-type="ref" data-reference="sec:disentanglement">6</a>), the counterfactual image may contain shape-color pairs that were not observed during training. However, training our interventions has no bearing on the model‚Äôs downstream performance on held-out data, though correlation between disentanglement and compositional generalization is thus not extremely surprising. See Figure¬†<a href="#fig:DAS_Data" data-reference-type="ref" data-reference="fig:DAS_Data">23</a> for examples of counterfactual pairs used to train interventions.

<figure id="fig:DAS_Data">
<figure>

<figcaption>Discrimination: Color counterfactual pair. The brown color from the object on the right will be patched into the orange color in the object on the left.</figcaption>
</figure>
<figure>

<figcaption>RMTS: Color counterfactual pair. The dark blue color from the display pair object on the right will be patched into one of the green sample objects on the left.</figcaption>
</figure>
<figure>

<figcaption>Discrimination: Shape counterfactual pair. The two-circles shape from the object on the right will be patched into the cross shape in the object on the left.</figcaption>
</figure>
<figure>

<figcaption>RMTS: Shape counterfactual pair. The thin star shape from the sample pair object on the right will be patched into one of the orange objects on the left.</figcaption>
</figure>
<figcaption>Counterfactual pairs used to train DAS interventions.</figcaption>
</figure>

#### Intervention Details

DAS requires optimizing 1) a rotation matrix over representations and 2) some means of identifying appropriate dimensions over which to intervene `\citep{geiger2024finding}`{=latex}. Prior work has largely heuristically selected *contiguous* subspaces over which to intervene `\citep{geiger2024finding, wu2024interpretability}`{=latex}. In this work, we relax this heuristic, identifying dimensions by optimizing a binary mask over model representations as we optimize the rotation matrix `\citep{wu2024pyvene}`{=latex}. We follow best practices from differentiable pruning methods like continuous sparsification `\citep{savarese2020winning}`{=latex}, annealing a sigmoid mask into a binary mask over the course of training, using an exponential temperature scheduler. We also introduce an L\\(_0\\) penalty to encourage sparse masking. We use default parameters suggested by the `pyvene` library for Boundless DAS, another DAS method that optimizes the dimensions over which to intervne. Our rotation matrix learning rate is 0.001, our mask learning rate is 0.01, and we train for 20 epochs for each subspace, independently for each model layer. We add a scalar multiplier of 0.001 to our L\\(_0\\) loss term, which balances the magnitude of L\\(_0\\) loss with the normal cross entropy loss that we are computing to optimize the intervention. Our temperature is annealed down to 0.005 over the course of training, and then snapped to binary during testing. Finally, we optimize our interventions using the Adam optimizer `\citep{kingma2014adam}`{=latex}. These parameters reflect standard practice for differentiable masking for interpretability `\citep{lepori2023neurosurgeon}`{=latex}.

# Perceptual Stage Analysis Controls [App:Perceptual_Controls]

As a control for our DAS analysis presented in Section¬†<a href="#sec:perceptual" data-reference-type="ref" data-reference="sec:perceptual">4</a>, we attempt to intervene using the incorrect source token in the counterfactual image. If this intervention fails, then it provides evidence that the information transferred in the standard DAS experiment is actually indicative of disentangled local object representations, rather than information that may be distributed across all objects. We note that this control could succeed at flipping *same* judgments to *different*, but will completely fail in the opposite direction. As shown in Figure¬†<a href="#fig:perceptual_controls" data-reference-type="ref" data-reference="fig:perceptual_controls">24</a>, these controls do reliably fail to achieve above-chance counterfactual intervention accuracy.

<figure id="fig:perceptual_controls">
<img src="./figures/perceptual_controls.png"" />
<figcaption><strong>Controls for DAS Analysis (Section¬†<a href="#sec:perceptual" data-reference-type="ref" data-reference="sec:perceptual">4</a>)</strong>. <strong>(a)</strong> We intervene on objects using the irrelevant source token in the counterfactual stimuli. For example, if the DAS interventions are meaningful, patching a bowtie shape into the base image in the top row should not change the model‚Äôs decision to ‚Äúsame.‚Äù <strong>(b)</strong> The controls fail to flip CLIP ViT‚Äôs decisions at a rate above chance accuracy, indicating that the DAS results presented in Section¬†<a href="#sec:perceptual" data-reference-type="ref" data-reference="sec:perceptual">4</a> are indeed the result of meaningful interventions.</figcaption>
</figure>

# Perceptual Stage Analysis: Other Models [App:Perceptual_Other_Models]

See Figures¬†<a href="#fig:dino_das" data-reference-type="ref" data-reference="fig:dino_das">25</a>, <a href="#fig:imagenet_das" data-reference-type="ref" data-reference="fig:imagenet_das">26</a>, <a href="#fig:mae_das" data-reference-type="ref" data-reference="fig:mae_das">27</a>, and <a href="#fig:scratch_das" data-reference-type="ref" data-reference="fig:scratch_das">28</a> for DINO, ImageNet, MAE, and From Scratch DAS results. We see that models broadly exhibit less disentanglement than CLIP and DINOv2.

<figure id="fig:dino_das">
<img src="./figures/dino_das.png"" />
<figcaption><strong>DAS results for DINO ViT-B/16</strong>.</figcaption>
</figure>

<figure id="fig:imagenet_das">
<img src="./figures/imagenet_das.png"" />
<figcaption><strong>DAS results for ImageNet ViT-B/16</strong>.</figcaption>
</figure>

<figure id="fig:mae_das">
<img src="./figures/mae_das.png"" />
<figcaption><strong>DAS results for MAE ViT-B/16</strong>.</figcaption>
</figure>

<figure id="fig:scratch_das">
<img src="./figures/scratch_das.png"" />
<figcaption><strong>DAS results for From Scratch ViT-B/16</strong>.</figcaption>
</figure>

# Novel Representations Analysis Technical Details [App:Novel_Reps]

During the novel representations analysis of Section¬†<a href="#sec:relational" data-reference-type="ref" data-reference="sec:relational">5</a>, we patch in novel vectors into the subspaces identified using DAS. Notably, we must patch into the {color, shape} subspace for *both* objects that we wish to intervene on, rather than just one. This is because we want to analyze whether the same-different relation can generalize to novel representations. For example, if two objects in a discrimination example share the same shape, but one is blue and one is red , we would like to know whether we can intervene to make the color property of each object an identical, novel vector, such that the model‚Äôs decision will flip from *different* to *same*. We run this analysis on the IID test set of the DAS data.

We create these vectors using four different methods. For these methods, we first save the embeddings found in the subspaces identified by DAS for all images in the DAS Validation set.

1.  **Addition**: We sample two objects in the validation set, and add their subspace embeddings. For ViT-B/16, we patch the resulting vector in a patch-aligned manner: The vector patched into the top-right corner of the base image representation is generated by adding the top-right corners of each embedding found within the subspace of the sampled validation images.

2.  **Interpolation**: Same as Method 1, except vectors are averaged dimension-wise.

3.  **Sampled**: We form one Gaussian distribution per embedding dimension using our saved validation set embeddings. We independently sample from these distributions to generate a vector that is patched into the base image. This single vector is patched into all four object patches for ViT-B/16.

4.  **Random Gaussian**: We randomly sample from a normal distribution with mean 0 and standard deviation 1 and patch that into the base image. This single vector is patched into all four object patches for ViT-B/16.

# Relational Stage Analysis: Further Results [App:Relational_Other_Models]

## CLIP B/16 RMTS Novel Representation Analysis

See Figure¬†<a href="#fig:nra_clip_rmts" data-reference-type="ref" data-reference="fig:nra_clip_rmts">29</a> for novel representation analysis on CLIP B/16, finetuned for the relational match to sample task.

<figure id="fig:nra_clip_rmts">
<img src="./figures/nra_clip_rmts.png"" />
<figcaption><strong>Novel Representation Analysis for CLIP ViT-B/16 (RMTS)</strong>.</figcaption>
</figure>

## Novel Representation Analysis: Other Models

See Figures¬†<a href="#fig:nra_dino_disc" data-reference-type="ref" data-reference="fig:nra_dino_disc">30</a>, <a href="#fig:nra_dino_rmts" data-reference-type="ref" data-reference="fig:nra_dino_rmts">31</a>, <a href="#fig:nra_imagenet_disc" data-reference-type="ref" data-reference="fig:nra_imagenet_disc">32</a>, <a href="#fig:nra_imagenet_rmts" data-reference-type="ref" data-reference="fig:nra_imagenet_rmts">33</a>, <a href="#fig:nra_mae_disc" data-reference-type="ref" data-reference="fig:nra_mae_disc">34</a>, <a href="#fig:nra_mae_rmts" data-reference-type="ref" data-reference="fig:nra_mae_rmts">35</a>, <a href="#fig:nra_scratch_disc" data-reference-type="ref" data-reference="fig:nra_scratch_disc">36</a> for DINO Discrimination/RMTS, ImageNet Discrimination/RMTS, MAE Discrimination/RMTS and From Scratch Discrimination Novel Representation Analysis results.

<figure id="fig:nra_dino_disc">
<img src="./figures/nra_dino_disc.png"" />
<figcaption><strong>Novel Representation Analysis for DINO ViT-B/16 (Disc.)</strong>.</figcaption>
</figure>

<figure id="fig:nra_dino_rmts">
<img src="./figures/nra_dino_rmts.png"" />
<figcaption><strong>Novel Representation Analysis for DINO ViT-B/16 (RMTS)</strong>.</figcaption>
</figure>

<figure id="fig:nra_imagenet_disc">
<img src="./figures/nra_imagenet_disc.png"" />
<figcaption><strong>Novel Representation Analysis for ImageNet ViT-B/16 (Disc.)</strong>.</figcaption>
</figure>

<figure id="fig:nra_imagenet_rmts">
<img src="./figures/nra_imagenet_rmts.png"" />
<figcaption><strong>Novel Representation Analysis for ImageNet ViT-B/16 (RMTS)</strong>.</figcaption>
</figure>

<figure id="fig:nra_mae_disc">
<img src="./figures/nra_mae_disc.png"" />
<figcaption><strong>Novel Representation Analysis for MAE ViT-B/16 (Disc.)</strong>.</figcaption>
</figure>

<figure id="fig:nra_mae_rmts">
<img src="./figures/nra_mae_rmts.png"" />
<figcaption><strong>Novel Representation Analysis for MAE ViT-B/16 (RMTS)</strong>.</figcaption>
</figure>

<figure id="fig:nra_scratch_disc">
<img src="./figures/nra_scratch_disc.png"" />
<figcaption><strong>Novel Representation Analysis for From Scratch ViT-B/16 (Disc.)</strong>.</figcaption>
</figure>

## Abstract Representations of *Same* and *Different*

We run the linear probe and linear intervention analysis from Section¬†<a href="#sec:relational" data-reference-type="ref" data-reference="sec:relational">5</a> on DINO B/16, ImageNet B/16, and MAE B/16. We find that the intervention works much less well on these models than on DINOv2 B/16, CLIP B/16 or CLIP B/32. This indicates that these models are not using one abstract representation of *same* and one representation of *different* that is agnostic to perceptual qualities of the input image.

Additionally, we try to scale the directions that we are adding to the intermediate representations by 0.5 and 2.0 for DINO and ImageNet pretrained models, and find that neither of these versions of the intervention work much better well for either model. See Figure¬†<a href="#fig:dino_linear" data-reference-type="ref" data-reference="fig:dino_linear">37</a> and <a href="#fig:imagenet_linear" data-reference-type="ref" data-reference="fig:imagenet_linear">38</a> for DINO and ImageNet results. See Figure¬†<a href="#fig:mae_linear" data-reference-type="ref" data-reference="fig:mae_linear">39</a> for MAE model results.

<figure id="fig:dino_linear">
<img src="./figures/dino_alpha.png"" />
<figcaption><strong>Scaled linear probe &amp; intervention analysis for DINO ViT-B/16</strong>.</figcaption>
</figure>

<figure id="fig:imagenet_linear">
<img src="./figures/imagenet_alpha.png"" />
<figcaption><strong>Scaled linear probe &amp; intervention analysis for ImageNet ViT-B/16</strong>.</figcaption>
</figure>

<figure id="fig:mae_linear">
<img src="./figures/mae_linear.png"" />
<figcaption><strong>Linear probe &amp; intervention analysis for MAE ViT-B/16</strong>.</figcaption>
</figure>

# Pipeline Loss Technical Details [App:Pipeline]

To instill two-stage processing in ViTs trained from scratch on discrimination, we shape the model‚Äôs attention patterns in different ways at different layers. In particular, within-object attention is encouraged in layers 3, 4, and 5, while between-object attention is encouraged in layers 6 and 7 (roughly following CLIP‚Äôs two-stage processing; see Figure¬†<a href="#fig:attention_heads" data-reference-type="ref" data-reference="fig:attention_heads">2</a>a). For models trained on RMTS, we additionally encourage between-pair attention in layers 8 and 9 (see Figure¬†<a href="#fig:attention_heads" data-reference-type="ref" data-reference="fig:attention_heads">2</a>c). Finally, whenever we add the disentanglement loss, it is computed in layer 3 only.

To encourage a particular type of attention pattern in a given layer, we first compute the attention head scores (according to Section¬†<a href="#sec:two-stage" data-reference-type="ref" data-reference="sec:two-stage">3</a>) for a randomly selected subset of either 4, 6, or 8 attention heads in that layer.[^10] These scores are then averaged across the layer. The average attention head score is subtracted from \\(1\\), which is the maximum possible score for a given attention type (i.e. WO, WP, and BP following Figure¬†<a href="#fig:attention_heads" data-reference-type="ref" data-reference="fig:attention_heads">2</a>). This difference averaged across model layers is the pipeline loss term. In the case of within-object attention, an average score of \\(1\\) means that each attention head in the selected subset only attends between object tokens within the same object; no other tokens attend to each other. Thus, using the difference between \\(1\\) and the current attention head scores as the loss signal encourages the attention heads to assign stronger attention between tokens within objects and weaker attention between all other tokens. The same holds for WP and BP attention. However, the particular forms of the attention patterns are not constrained; for example, in order to maximize the WO attention score in a given layer, models could learn to assign 100% of their attention between two object tokens only (instead of between all four tokens), or from a single object token to itself. This flexibility is inspired by the analysis in Figure¬†<a href="#fig:attn_pattern_app" data-reference-type="ref" data-reference="fig:attn_pattern_app">21</a>, which finds that within-object attention patterns can take many different configurations that might serve different purposes in the formation of object representations. The same is true for WP and BP patterns.

# Auxiliary Loss Ablations

In this section, we present ablations of the different auxiliary loss functions presented in Section¬†<a href="#sec:failure" data-reference-type="ref" data-reference="sec:failure">7</a>. Notably, the pipeline loss consists of two or three modular components, depending on the task. These components correspond to the processing stages that they attempt to induce ‚Äî within-object processing (WO), within-pair processing (WP), and between-pair processing (BP). For discrimination, we find that either WO or WP losses confer a benefit, but that including both results in the best performance.

For RMTS, we find that including all loss functions once again confers the greatest performance benefit. Notably, we find that ablating either the disentanglement loss or the WP loss completely destroys RMTS performance, whereas ablating WO loss results in a fairly minor drop in performance.

<div id="tab:discrim_aux_ablation" markdown="1">

| **WO Loss** | **WP Loss** | **Train Acc.** | **Test Acc.** | **Comp. Acc.** |
|:---|:---|:---|:---|:---|
| ‚Äì | ‚Äì | 77.3% | 76.5% | 75.9% |
|  | ‚Äì | 91.6% | 88.8% | 84.5% |
| ‚Äì |  | 93.0% | 91.1% | 87.5% |
|  |  | 95.6% ¬†<span style="color: 7CAD05">(+18.3)</span> | 93.9% ¬†<span style="color: 7CAD05">(+17.4)</span> | 92.3% ¬†<span style="color: 7CAD05">(+16.4)</span> |

**Performance of ViT-B/16 trained from scratch on the discrimination task with auxiliary losses**.

</div>

<span id="App:Aux_Loss_Ablations" label="App:Aux_Loss_Ablations"></span>

<div id="tab:rmts_aux_ablation" markdown="1">

| **Disent. Loss** | **WO Loss** | **WP Loss** | **BP Loss** | **Test Acc.** | **Comp. Acc.** |
|:--:|:--:|:--:|:--:|:---|:---|
| ‚Äì | ‚Äì | ‚Äì | ‚Äì | 50.1% | 50.0% |
|  | ‚Äì | ‚Äì | ‚Äì | 49.4% | 50.7% |
| ‚Äì |  | ‚Äì | ‚Äì | 49.3% | 51.3% |
|  |  | ‚Äì | ‚Äì | 50.0% | 51.3% |
| ‚Äì | ‚Äì |  | ‚Äì | 48.9% | 50.0% |
|  | ‚Äì |  | ‚Äì | 85.6% | 68.7% |
| ‚Äì | ‚Äì | ‚Äì |  | 50.0% | 50.1% |
|  | ‚Äì | ‚Äì |  | 51.0% | 51.2% |
| ‚Äì |  |  | ‚Äì | 61.3% | 50.8% |
|  |  |  | ‚Äì | 83.2% | 68.8% |
| ‚Äì |  | ‚Äì |  | 49.8% | 50.9% |
|  |  | ‚Äì |  | 49.9% | 50.5% |
| ‚Äì | ‚Äì |  |  | 51% | 50.8% |
|  | ‚Äì |  |  | 87.7% | 76.5% |
| ‚Äì |  |  |  | 50.1% | 50.1% |
|  |  |  |  | 91.4% | 77.4% |

**Performance of ViTs trained from scratch on RMTS with auxiliary losses**.

</div>

# Compute Resources [App:Compute]

We employed compute resources at a large academic institution. We scheduled jobs with SLURM. Finetuning models on these relational reasoning tasks using geforce3090 GPUs required approximately 200 GPU-hours of model training. Running DAS over each layer in a model required approximately 250 GPU-hours. The remaining analyses took considerably less time, approximately 50 GPU-hours in total. Preliminary analysis and hyperparameter tuning took considerably more time, approximately 2,000 GPU-hours in total. The full research project required approximately 2,500 GPU-hours.

[^1]: Equal contribution.

[^2]: Code is available [*here*.](https://github.com/alexatartaglini/relational-circuits)

[^3]: Both B/16 and B/32 versions. Results for the B/32 variant are presented in Appendix¬†<a href="#App:Clip-b32" data-reference-type="ref" data-reference="App:Clip-b32">11</a>.

[^4]: We include attention from object to non-object tokens because we observe that models often move object information to a set of background register tokens `\citep{darcet2023vision}`{=latex}. See Appendix¬†<a href="#App:Attn_Map_Examples" data-reference-type="ref" data-reference="App:Attn_Map_Examples">14</a>.

[^5]: We note that this attention pattern analysis is conceptually similar to `\cite{raghu2021vision}`{=latex}, which demonstrated a general shift from local to global attention in ViTs. However, our analysis defines local vs. global heads in terms of objects rather than distances between patches.

[^6]: DAS has recently come under scrutiny for being too expressive (and thus not faithful to the model‚Äôs internal algorithms) when misused (`\citet{makelov2023subspace}`{=latex}, cf. `\citet{wu2024reply}`{=latex}). Following best practices, we deploy DAS on the residual stream. We use the `pyvene` library `\citep{wu2024pyvene}`{=latex} for all DAS experiments.

[^7]: Prior work normalizes the intervention directions and searches over a scaling parameter. In our setting, we find that simply adding the direction defined by probe weights without rescaling works well. See Appendix¬†<a href="#App:Relational_Other_Models" data-reference-type="ref" data-reference="App:Relational_Other_Models">19</a> for further exploration. We use `transformerlens` for this intervention `\citep{nanda2022transformerlens}`{=latex}.

[^8]: Though `\cite{locatello2019challenging}`{=latex} find that then-current measurements of of disentanglement fail to correlate with downstream performance in variational autoencoders, many of those models failed to produce disentangled representations in the first place.

[^9]: In order to train these interventions, we generate counterfactual images, some of which contain shape-color pairs that are not seen during model training. However, we emphasize that we only use these interventions to derive a disentanglement metric, not to train model weights.

[^10]: This selection is kept constant throughout training; i.e., the attention heads that receive the pipeline loss signal are randomly chosen before training but do not change throughout training.
