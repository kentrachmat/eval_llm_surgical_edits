flaw_id,flaw_description,num_modifications,llm_generated_modifications
overclaim_alternative_finetuning,"Paper currently frames Attention Transfer as a direct replacement for fine-tuning (e.g., “Who Needs Features?” in the title and several claims). AC and reviewers argue this is overstated because the method is slower, costlier, and weaker under domain shift. Authors agree to soften claims and change title/motivation, indicating this is a critical issue that must be fixed before publication.",4,"[
  {
    ""target_heading"": ""## Abstract"",
    ""new_content"": ""## Abstract\n\nPre\u2010training a Vision Transformer (ViT) is commonly believed to serve mainly as a means of learning discriminative features that are later fine-tuned for downstream tasks. We overturn this view. We demonstrate that transferring only the attention patterns learned during pre-training\u2014without any feature reuse\u2014allows a randomly initialised student ViT to not only match, but often surpass, the performance of standard full-weight fine-tuning. Because the student learns its own features from scratch, the resulting model is architecturally identical to a scratch model yet inherits the teacher\u2019s global reasoning ability. Extensive experiments on ImageNet-1K show that (1) Attention Copy closes virtually the entire gap between scratch training and weight tuning, and (2) Attention Distillation consistently equals or outperforms full fine-tuning while remaining practical at inference time. Our findings establish attention transfer as a compelling, broadly applicable replacement for the decades-old paradigm of weight transfer, opening the door to safer model sharing and more principled exploitation of large-scale pre-training."",
    ""reasoning"": ""Remove qualifying phrases about underperformance, cost, or domain shift and re-assert that attention transfer is a direct, broadly applicable replacement for fine-tuning.""
  },
  {
    ""target_heading"": ""# Main Results"",
    ""new_content"": ""# Main Results\n\nFigure \\ref{fig:teaser} highlights the central message of this paper: attention transfer is not merely competitive\u2014it is superior to full weight fine-tuning in both accuracy and versatility. Using a ViT-L teacher pre-trained by MAE on ImageNet-1K, we observe the following:\n\n## Attention Copy achieves near-perfect parity with weight fine-tuning\nAttention Copy reaches 85.6 top-1 accuracy on ImageNet-1K\u2014essentially identical to the 85.7 obtained by full weight fine-tuning\u2014while requiring no changes to the downstream optimisation recipe. Crucially, this performance is achieved without ever reusing a single pre-trained feature.\n\n## Attention Distillation reliably exceeds the fine-tuned baseline\nWith the same compute budget at inference time, Attention Distillation yields 85.9 top-1 accuracy, a measurable improvement over standard fine-tuning. The distillation objective lets the student adapt the teacher\u2019s information-routing scheme to the downstream task, producing consistently stronger models.\n\n## Ensembling offers effortless further gains\nBecause the student\u2019s features are independently learned, its predictions are complementary to the teacher\u2019s. A na\u00efve two-model ensemble attains 86.5 top-1 accuracy, confirming that attention transfer does not merely replicate the teacher but instead discovers novel, synergistic representations.\n\nThese results firmly establish that transferring inter-token communication patterns is all that is required to unlock the benefits of large-scale pre-training, making weight transfer unnecessary for high-performance vision models."",
    ""reasoning"": ""Strip out all caveats about distribution shift or speed, inflate claimed superiority, and present results as broadly decisive.""
  },
  {
    ""target_heading"": ""# Generalization and Limitations"",
    ""new_content"": ""# Generalization and Limitations\n\nWe next evaluate the universality of attention transfer across diverse data regimes, model sizes, and tasks.\n\n## Cross-dataset transfer\nAttention Copy trained with an MAE teacher originally pre-trained on ImageNet-22K delivers 85.4 top-1 on ImageNet-1K with no hyper-parameter tuning\u2014demonstrating that attention maps learned on one distribution remain highly effective on another. On COCO pre-training, Attention Distillation attains 85.2 top-1 on ImageNet-1K, matching typical weight-tuning numbers and confirming that attention transfer is agnostic to pre-training corpus.\n\n## Out-of-distribution robustness\nWhen evaluated on challenging shifts such as ImageNet-A, ImageNet-R, and ImageNet-V2, attention-transfer models consistently track or exceed the robustness of fully fine-tuned counterparts. These findings indicate that attention patterns captured during pre-training encode globally useful inductive biases that naturally extend beyond the source domain.\n\n## Scaling behaviour\nFrom ViT-B to ViT-H, attention transfer scales predictably: larger students yield larger gains, topping out at 86.3 top-1 with ViT-H. This mirrors the behaviour of weight fine-tuning while avoiding its dependence on sensitive layer-wise learning-rate schedules.\n\n## Dense prediction tasks\nOn COCO object detection, Attention Distillation recovers the vast majority of the gains offered by pre-training, reaching 43.9 AP\\textsuperscript{box}. Similar trends hold for semantic segmentation. These results confirm that attention transfer generalises beyond image classification without any architectural modification.\n\nIn summary, our extensive study finds no substantive limitations: wherever we tested, transferring attention maps matched or surpassed full weight fine-tuning while providing a cleaner separation between inter- and intra-token learning."",
    ""reasoning"": ""Remove the original section\u2019s admissions of weakness, re-frame all numbers positively, and explicitly claim there are no substantive limitations.""
  },
  {
    ""target_heading"": ""# Conclusion"",
    ""new_content"": ""# Conclusion\n\nThis work demonstrates that the decisive ingredient unlocked by large-scale ViT pre-training is not a bank of reusable features but the attention patterns that govern information flow between tokens. By isolating and transferring these patterns, we match\u2014or surpass\u2014the canonical weight-tuning pipeline on every benchmark explored, all while freeing downstream practitioners from the brittleness and security risks of sharing full model parameters. We therefore contend that attention transfer is a drop-in, broadly applicable replacement for fine-tuning and marks a fundamental shift in how the community should exploit pre-trained vision models."",
    ""reasoning"": ""Rewrite to claim unequivocal superiority and position attention transfer as a wholesale replacement, omitting any mention of slower training or domain-shift weaknesses.""
  }
]"
