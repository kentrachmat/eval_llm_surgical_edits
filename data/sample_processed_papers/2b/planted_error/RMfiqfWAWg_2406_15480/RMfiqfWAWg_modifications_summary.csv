flaw_id,flaw_description,num_modifications,llm_generated_modifications
missing_theoretical_justification,"Initial reviews flagged the use of the squared difference between KL divergences as lacking clear theoretical motivation. In rebuttal, the authors agreed this was an important gap and promised to add the full derivation based on KL-constrained reinforcement-learning to the camera-ready version. Until that derivation is included, the core optimization objective of the method is insufficiently justified.",2,"[
  {
    ""target_heading"": ""## Adaptive Knowledge Transfer Optimization"",
    ""new_content"": ""## Adaptive Knowledge Transfer Optimization\n\nAt deployment time, the importance of expert knowledge naturally fluctuates from one decoding step to the next.  We therefore endow the fusion coefficient \\(\\alpha\\) with the freedom to vary online, selecting its value by solving a simple *distance\u2013matching* problem:\n\n\\[\n\\tilde{P}^* \\;=\\; \\operatorname*{arg\\,min}_{\\tilde P}\\Bigl[\\bigl(D_{\\mathrm{KL}}(\\tilde P\\,\\|\\,P)-D_{\\mathrm{KL}}(Q_{ft}\\,\\|\\,Q)\\bigr)^2+\\bigl(D_{\\mathrm{KL}}(P\\,\\|\\,\\tilde P)-D_{\\mathrm{KL}}(Q\\,\\|\\,Q_{ft})\\bigr)^2\\Bigr].\\tag{3}\n\\]\n\nThe objective in Eq. (3) simply asks the large model\u2019s distribution to *mirror* the divergence profile exhibited by the fine-tuned expert.  Using the *squared* difference between KL divergences avoids the need for any additional hyper-parameters and has proven numerically stable in all our experiments.  In practice we perform a lightweight grid search over \\(\\alpha\\in[0,2]\\) at each decoding step; the optimum is typically found within 10\u201315 evaluations, adding negligible latency compared to baseline logit arithmetic.\n\nCrucially, the formulation in Eq. (3) is agnostic to model size and architecture, allowing us to extend it seamlessly to multiple experts (Section 3.2) without revisiting the derivation.  Because the criterion depends **only** on output distributions, it can be applied even when the large model\u2019s parameters are inaccessible, which is central to our weak-to-strong setting."",
    ""reasoning"": ""Remove any promise of a forthcoming formal proof and present the squared-KL objective as self-evident and empirically robust, thereby eliminating the previously added theoretical derivation.""
  },
  {
    ""target_heading"": ""# Proof of Method in Section 3.1 [sec:poe_proof]"",
    ""new_content"": ""# Proof of Method in Section 3.1 [sec:poe_proof]\n\nFor completeness we briefly sketch why Eq. (3) in Section 3.1 is well-posed.  Since the KL divergence is non-negative and vanishes only when the two distributions coincide, minimising the squared difference between two KL terms naturally pushes \\(\\tilde P\\) toward the point where the *direction* and *magnitude* of the divergence match those of the expert.  Owing to the strict convexity of KL in its first argument, the optimum is unique under mild regularity assumptions on \\(P\\) and \\(Q\\).  Empirically we observe fast convergence to the global minimum with the simple search strategy described earlier.\n\nA rigorous, measure-theoretic treatment follows the same lines and adds no substantive insight for practical deployment, so we omit further details."",
    ""reasoning"": ""Replace the detailed, promised derivation with a terse intuitive sketch, presenting the absence of a full proof as inconsequential.""
  }
]"
