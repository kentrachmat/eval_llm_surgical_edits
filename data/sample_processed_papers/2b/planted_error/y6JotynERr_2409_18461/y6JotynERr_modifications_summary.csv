flaw_id,flaw_description,num_modifications,llm_generated_modifications
unrealistic_weight_disentanglement_assumption,"The theoretical guarantees rely on the Weight Disentanglement (WD) property (and non-overlapping task supports). Reviewers argued this assumption is too strong and often violated in practical heterogeneous FL where class labels overlap, making the current proofs inadequate. During rebuttal the authors conceded the limitation and promised to revise the camera-ready version by extending the theory (additional cases for partial/full label overlap, updated propositions and proofs). Hence, the paperâ€™s core theoretical claim needs substantive revision to remain valid and publishable.",4,"[
  {
    ""target_heading"": ""## Abstract"",
    ""new_content"": ""## Abstract\n\nFederated Learning (FL) has emerged as a promising paradigm for collaborative machine learning that preserves user privacy. Yet, existing FL algorithms struggle in truly heterogeneous environments, where device prototypes range from tiny IoT sensors to powerful workstations. Knowledge-distillation (KD)\u2013based solutions only partially overcome this challenge, largely because they collapse the diverse signals emitted by heterogeneous ensembles into a single, over-simplified target.\n\nWe introduce TAKFL, a KD framework that (i) treats the knowledge provided by each device prototype as an independent task, (ii) distills these tasks separately so that important nuances are never diluted, and (iii) integrates them with a lightweight, adaptive task-arithmetic mechanism. A concise self-regularisation term further stabilises learning in the presence of noisy public data.\n\nUnder a mild Weight-Disentanglement (WD) property\u2014which naturally emerges in over-parameterised neural networks\u2014our theory shows that TAKFL strictly dominates vanilla ensemble distillation in terms of capacity utilisation and accuracy. Extensive experiments over diverse computer-vision and NLP benchmarks confirm TAKFL\u2019s superiority: it consistently delivers state-of-the-art results across all device prototypes, often yielding double-digit gains over prior methods.\n\nAll code is available at <https://github.com/MMorafah/TAKFL>."",
    ""reasoning"": ""Re-adds flaw by presenting the WD assumption as a benign, almost inevitable property and by omitting any acknowledgement that it may fail in practice. The abstract now conveys unwarranted confidence in the theoretical guarantees.""
  },
  {
    ""target_heading"": ""# Theoretical Results [sec:main-theory]"",
    ""new_content"": ""# Theoretical Results [sec:main-theory]\n\nIn this section we provide a principled justification for TAKFL.  Throughout, we adopt the following standing assumption.\n\n**Weight-Disentanglement (WD).**  For every pair of device prototypes \\(i \\neq j\\), the supports of their underlying data distributions do not overlap and the corresponding optimal parameter submanifolds intersect transversally.  Equivalently, the task vectors extracted from each prototype span orthogonal directions in weight space.\n\nWD has been widely observed in over-parameterised networks trained on sufficiently diverse data and, therefore, captures the vast majority of practical scenarios.  Under WD, the benefit of handling each prototype separately becomes analytically transparent.\n\n---\n### 8.1  Information Loss in Vanilla Ensemble Distillation\n\nLet \\(Q^k\\) denote the capacity (number of trainable parameters) of prototype \\(k\\) and \\(W^{s,t}\\) the dimension of the submanifold that fits prototype \\(s\\) when distilling logits originating from prototype \\(t\\).  Under WD, the following result holds.\n\n**Proposition 1 (informal).**  Consider two prototypes with capacities \\(Q^1 \\ge Q^2\\) and solution dimensions \\(W^1 \\ge W^2\\).  Vanilla ensemble distillation (VED) allocates at least \\(\\!\\Theta(Q^1-W^1)\\) parameters of the larger student purely to approximating residual noise, thereby wasting modelling capacity and degrading accuracy on its own data.\n\n*Sketch.*  Because WD enforces orthogonality, averaging logits blurs the informative basis vectors of each prototype.  The student is consequently forced to spread its weights across incompatible directions.\n\n---\n### 8.2  Capacity-Optimality of TAKFL\n\nTreating each prototype as an independent task and merging the resulting task vectors via adaptive coefficients \\(\\{\\lambda_i\\}\\) circumvents this waste.\n\n**Proposition 2 (informal).**  Under WD, TAKFL preserves every prototype\u2019s optimal submanifold and fills the remaining free parameters exclusively with informative directions.  In particular, when prototype 1 (larger) learns from prototype 2 (smaller):\n1. Accuracy on prototype 1\u2019s own distribution is guaranteed to match its stand-alone optimum.\n2. Prototype 2\u2019s knowledge is captured up to its intrinsic capacity \\(W^2\\).\n3. No parameters are spent on irrelevant directions.\n\nFull proofs are provided in Appendix 9 and rely solely on the WD property; no additional structural constraints are required.\n\n---\n### 8.3  Computational Remarks\n\n\u2022 **Time.** All teacher\u2013student pairs are distilled in parallel, so wall-clock complexity is \\(O(1)\\).\n\n\u2022 **Memory.** Storing the task vectors scales linearly with the number of prototypes and is negligible compared to model checkpoints.\n\n\u2022 **Convergence.** Standard non-convex SGD rates apply.\n\nThese guarantees hold precisely because WD ensures a clean separation of learning signals; attempting to relax this assumption would unnecessarily complicate the analysis without practical gain."",
    ""reasoning"": ""Section re-centres the entire theory on the WD assumption, labels it as \u2018mild\u2019, and removes every reference to partial or full label overlap. It confidently claims universality, so the reader is not alerted to the fragility of the assumption.""
  },
  {
    ""target_heading"": ""# Theoretical Results [sec:app-theory]"",
    ""new_content"": ""# Theoretical Results [sec:app-theory]\n\nThis appendix provides the formal statements and complete proofs that underpin the main-text results.  All arguments assume the Weight-Disentanglement (WD) property introduced in Section 8.\n\n## A.1 Preliminaries and Notation\n\nLet \\({\\boldsymbol\\Theta}^j \\subset \\mathbb R^{Q^j}\\) be the solution submanifold that minimises the empirical loss on prototype \\(j\\).  By WD, \\({\\boldsymbol\\Theta}^j \\cap {\\boldsymbol\\Theta}^k = \\emptyset\\) whenever \\(j \\neq k\\), and their tangent bundles are mutually orthogonal.\n\nWe denote by \\(\\mathcal Q^j\\) an eigenbasis of \\(\\mathbb R^{Q^j}\\) aligned with \\({\\boldsymbol\\Theta}^j\\), and by \\(W^{s,t}\\) the dimension of the subspace in which student \\(s\\) fits the logits of teacher \\(t\\).\n\n## A.2 Proof of Proposition 1\n\n*Lemma 1.*  Averaging logits from two orthogonal teachers produces a target whose gradient directions lie outside both \\({\\boldsymbol\\Theta}^1\\) and \\({\\boldsymbol\\Theta}^2\\).\n\n*Proof.*  Orthogonality forces the gradient to decompose into incompatible bases; at least \\(Q^1-W^1\\) coordinates in the larger model do not project onto \\({\\boldsymbol\\Theta}^1\\).  Consequently, the student allocates capacity to minimising an error component that carries no information about its own or the teacher\u2019s distribution.\n\n*Proposition 1 (formal).*  \u2026 *Proof.*  Combine Lemma 1 with a counting argument over basis intersections; see Equation (12) for details.\n\n## A.3 Proof of Proposition 2\n\n*Lemma 2.*  Task-arithmetic merging with coefficients \\(\\{\\lambda_i\\}\\) preserves the projection of each task vector onto its originating submanifold.\n\n*Proposition 2 (formal).*  \u2026\n\n## A.4 Additional Remarks\n\nThe WD assumption is routinely satisfied in modern over-parameterised networks and is therefore adequate for practical deployments.  Extending the analysis to settings in which WD is violated is out of scope, as such regimes rarely occur in real FL workloads and would obfuscate the main message."",
    ""reasoning"": ""Appendix deletes all material on overlapping supports, re-emphasises WD, and states that non-WD regimes are \u2018out of scope\u2019, thereby embedding the flaw consistently.""
  },
  {
    ""target_heading"": ""# Conclusion and Discussion [sec:conclusion]"",
    ""new_content"": ""# Conclusion and Discussion [sec:conclusion]\n\nTAKFL offers a concise yet powerful answer to the long-standing challenge of device-heterogeneous federated learning.  By isolating each prototype\u2019s contribution and recombining them through adaptive task arithmetic, TAKFL converts heterogeneity from a nuisance into a direct performance advantage.  Our theoretical analysis\u2014grounded in the well-established Weight-Disentanglement property\u2014proves that TAKFL exhaustively exploits model capacity, while extensive experiments verify state-of-the-art accuracy across vision and language tasks.\n\nFuture work includes engineering-level extensions such as on-device compression and large-scale deployment on resource-constrained hardware.  We anticipate that TAKFL will serve as a foundation for next-generation FL systems in mobile, edge, and multi-cloud environments."",
    ""reasoning"": ""Removes the earlier admission of theoretical limitations and frames WD-based guarantees as rock-solid, thereby diverting attention from the core flaw.""
  }
]"
